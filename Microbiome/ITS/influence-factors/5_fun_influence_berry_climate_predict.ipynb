{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict climate from berry fungal communities\n",
    "\n",
    "> using `ritme` (https://github.com/adamovanja/ritme)\n",
    "\n",
    "note. use the **ritme_model** environment! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from ritme.find_best_model_config import (\n",
    "    _load_experiment_config,\n",
    "    _load_phylogeny,\n",
    "    _load_taxonomy,\n",
    "    find_best_model_config,\n",
    ")\n",
    "from ritme.split_train_test import _load_data, split_train_test\n",
    "from ritme.evaluate_tuned_models import evaluate_tuned_models\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries\n"
     ]
    }
   ],
   "source": [
    "workdir = '/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries'\n",
    "%cd $workdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set experiment configuration path - this is downloaded from the github repo\n",
    "# here we specify the column to be stratified by and the target! \n",
    "model_config_path = \"/home/lfloerl/microterroir/Microbiome/Other_scripts/config/r_local_linreg_py.json\"\n",
    "\n",
    "# define path to feature table, metadata, and taxonomy (no phylogeny)\n",
    "path_to_ft = \"climate_filtered_table.qza\"\n",
    "path_to_md = \"/home/lfloerl/microterroir/Microbiome/Metadata/ITS_Lavaux_Climate.tsv\"\n",
    "path_to_tax = \"/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/taxonomy.qza\"\n",
    "\n",
    "# define train size\n",
    "train_size = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = _load_experiment_config(model_config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>SAMPLE_NAME</th>\n",
       "      <th>sample_type</th>\n",
       "      <th>Plot_ID</th>\n",
       "      <th>COLLECTION_DATE</th>\n",
       "      <th>Year</th>\n",
       "      <th>Year_Cat</th>\n",
       "      <th>Fermentation_Timepoint</th>\n",
       "      <th>Plot</th>\n",
       "      <th>Altitude</th>\n",
       "      <th>...</th>\n",
       "      <th>maximum_rh</th>\n",
       "      <th>minimum_rh</th>\n",
       "      <th>cv_rh</th>\n",
       "      <th>GDD</th>\n",
       "      <th>average_temperature</th>\n",
       "      <th>median_temperature</th>\n",
       "      <th>maximum_temperature</th>\n",
       "      <th>minimum_temperature</th>\n",
       "      <th>accumulated_temperature</th>\n",
       "      <th>cv_temperature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>364526_290-LP3-ITS-0866</td>\n",
       "      <td>Lavaux_2021-08-31_bark_Plot4</td>\n",
       "      <td>bark</td>\n",
       "      <td>Lavaux_Plot_4</td>\n",
       "      <td>2021-08-31 00:00:00 +0200</td>\n",
       "      <td>2021</td>\n",
       "      <td>Year2021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>450</td>\n",
       "      <td>...</td>\n",
       "      <td>99.036583</td>\n",
       "      <td>33.078261</td>\n",
       "      <td>16.567322</td>\n",
       "      <td>1325.721843</td>\n",
       "      <td>18.154261</td>\n",
       "      <td>18.650958</td>\n",
       "      <td>26.689636</td>\n",
       "      <td>8.401750</td>\n",
       "      <td>2940.990343</td>\n",
       "      <td>22.587667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>364526_289-LP3-ITS-0865</td>\n",
       "      <td>Lavaux_2021-08-31_bark_Plot5</td>\n",
       "      <td>bark</td>\n",
       "      <td>Lavaux_Plot_5</td>\n",
       "      <td>2021-08-31 00:00:00 +0200</td>\n",
       "      <td>2021</td>\n",
       "      <td>Year2021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>400</td>\n",
       "      <td>...</td>\n",
       "      <td>95.964333</td>\n",
       "      <td>32.830458</td>\n",
       "      <td>16.331714</td>\n",
       "      <td>1379.347141</td>\n",
       "      <td>18.494554</td>\n",
       "      <td>19.047366</td>\n",
       "      <td>26.351409</td>\n",
       "      <td>8.816375</td>\n",
       "      <td>2996.117777</td>\n",
       "      <td>22.104867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>364526_285-LP3-ITS-0861</td>\n",
       "      <td>Lavaux_2021-08-31_bark_Plot9</td>\n",
       "      <td>bark</td>\n",
       "      <td>Lavaux_Plot_9</td>\n",
       "      <td>2021-08-31 00:00:00 +0200</td>\n",
       "      <td>2021</td>\n",
       "      <td>Year2021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>520</td>\n",
       "      <td>...</td>\n",
       "      <td>98.181542</td>\n",
       "      <td>32.794125</td>\n",
       "      <td>16.570451</td>\n",
       "      <td>1286.254021</td>\n",
       "      <td>17.902585</td>\n",
       "      <td>18.256629</td>\n",
       "      <td>25.850773</td>\n",
       "      <td>8.052083</td>\n",
       "      <td>2900.218734</td>\n",
       "      <td>22.831881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>364526_287-LP3-ITS-0863</td>\n",
       "      <td>Lavaux_2021-08-31_bark_Plot11</td>\n",
       "      <td>bark</td>\n",
       "      <td>Lavaux_Plot_11</td>\n",
       "      <td>2021-08-31 00:00:00 +0200</td>\n",
       "      <td>2021</td>\n",
       "      <td>Year2021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11</td>\n",
       "      <td>490</td>\n",
       "      <td>...</td>\n",
       "      <td>96.437167</td>\n",
       "      <td>32.357792</td>\n",
       "      <td>16.118351</td>\n",
       "      <td>1297.271087</td>\n",
       "      <td>17.964399</td>\n",
       "      <td>18.541438</td>\n",
       "      <td>26.775286</td>\n",
       "      <td>7.966542</td>\n",
       "      <td>2910.232671</td>\n",
       "      <td>23.246939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>364526_282-LP3-ITS-0858</td>\n",
       "      <td>Lavaux_2021-08-31_bark_Plot12</td>\n",
       "      <td>bark</td>\n",
       "      <td>Lavaux_Plot_12</td>\n",
       "      <td>2021-08-31 00:00:00 +0200</td>\n",
       "      <td>2021</td>\n",
       "      <td>Year2021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12</td>\n",
       "      <td>520</td>\n",
       "      <td>...</td>\n",
       "      <td>97.113875</td>\n",
       "      <td>32.104636</td>\n",
       "      <td>16.661423</td>\n",
       "      <td>1286.822875</td>\n",
       "      <td>17.897069</td>\n",
       "      <td>18.244984</td>\n",
       "      <td>27.509143</td>\n",
       "      <td>7.779875</td>\n",
       "      <td>2899.325119</td>\n",
       "      <td>23.615955</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        id                    SAMPLE_NAME sample_type  \\\n",
       "0  364526_290-LP3-ITS-0866   Lavaux_2021-08-31_bark_Plot4        bark   \n",
       "1  364526_289-LP3-ITS-0865   Lavaux_2021-08-31_bark_Plot5        bark   \n",
       "2  364526_285-LP3-ITS-0861   Lavaux_2021-08-31_bark_Plot9        bark   \n",
       "3  364526_287-LP3-ITS-0863  Lavaux_2021-08-31_bark_Plot11        bark   \n",
       "4  364526_282-LP3-ITS-0858  Lavaux_2021-08-31_bark_Plot12        bark   \n",
       "\n",
       "          Plot_ID            COLLECTION_DATE  Year  Year_Cat  \\\n",
       "0   Lavaux_Plot_4  2021-08-31 00:00:00 +0200  2021  Year2021   \n",
       "1   Lavaux_Plot_5  2021-08-31 00:00:00 +0200  2021  Year2021   \n",
       "2   Lavaux_Plot_9  2021-08-31 00:00:00 +0200  2021  Year2021   \n",
       "3  Lavaux_Plot_11  2021-08-31 00:00:00 +0200  2021  Year2021   \n",
       "4  Lavaux_Plot_12  2021-08-31 00:00:00 +0200  2021  Year2021   \n",
       "\n",
       "   Fermentation_Timepoint  Plot  Altitude  ...  maximum_rh minimum_rh  \\\n",
       "0                     NaN     4       450  ...   99.036583  33.078261   \n",
       "1                     NaN     5       400  ...   95.964333  32.830458   \n",
       "2                     NaN     9       520  ...   98.181542  32.794125   \n",
       "3                     NaN    11       490  ...   96.437167  32.357792   \n",
       "4                     NaN    12       520  ...   97.113875  32.104636   \n",
       "\n",
       "       cv_rh          GDD  average_temperature median_temperature  \\\n",
       "0  16.567322  1325.721843            18.154261          18.650958   \n",
       "1  16.331714  1379.347141            18.494554          19.047366   \n",
       "2  16.570451  1286.254021            17.902585          18.256629   \n",
       "3  16.118351  1297.271087            17.964399          18.541438   \n",
       "4  16.661423  1286.822875            17.897069          18.244984   \n",
       "\n",
       "  maximum_temperature minimum_temperature accumulated_temperature  \\\n",
       "0           26.689636            8.401750             2940.990343   \n",
       "1           26.351409            8.816375             2996.117777   \n",
       "2           25.850773            8.052083             2900.218734   \n",
       "3           26.775286            7.966542             2910.232671   \n",
       "4           27.509143            7.779875             2899.325119   \n",
       "\n",
       "   cv_temperature  \n",
       "0       22.587667  \n",
       "1       22.104867  \n",
       "2       22.831881  \n",
       "3       23.246939  \n",
       "4       23.615955  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md_df = pd.read_csv(path_to_md, sep='\\t')\n",
    "md_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read & split data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(595, 36) (326, 6091)\n"
     ]
    }
   ],
   "source": [
    "md, ft = _load_data(path_to_md, path_to_ft)\n",
    "print(md.shape, ft.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (221, 6127), Test: (105, 6127)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/split_train_test.py:135: UserWarning: Provided feature table contains absolute instead of relative abundances. Hence, converting it to relative abundances...\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_val, test = split_train_test(\n",
    "    md,\n",
    "    ft,\n",
    "    stratify_by_column=config[\"stratify_by_column\"],\n",
    "    feature_prefix=config[\"feature_prefix\"],\n",
    "    train_size=train_size,\n",
    "    seed=config[\"seed_data\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find best model config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2025-02-12 10:55:47</td></tr>\n",
       "<tr><td>Running for: </td><td>00:28:17.45        </td></tr>\n",
       "<tr><td>Memory:      </td><td>11.6/91.6 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=12<br>Bracket: Iter 40.000: -1.0192780494689941 | Iter 10.000: -9.84797716140747<br>Logical resource usage: 1.0/12 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "<div class=\"messages\">\n",
       "  <h3>Messages</h3>\n",
       "  \n",
       "  \n",
       "  Number of errored trials: 86<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                                                                                                                                                                                                    </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_nn_reg_fdd33f93</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_fdd33f93_1_batch_size=32,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0001,data_transfo_2025-02-12_10-27-30/error.txt</td></tr>\n",
       "<tr><td>train_nn_reg_b5df96e2</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_b5df96e2_2_batch_size=32,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0023,data_transfo_2025-02-12_10-27-36/error.txt</td></tr>\n",
       "<tr><td>train_nn_reg_126b80e4</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_126b80e4_5_batch_size=32,data_aggregation=None,data_selection=abundance_ith,data_selection_i=15,data_transform=None,e_2025-02-12_10-27-54/error.txt</td></tr>\n",
       "<tr><td>train_nn_reg_a8c9e393</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_a8c9e393_6_batch_size=64,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0000,data_transfo_2025-02-12_10-28-01/error.txt</td></tr>\n",
       "<tr><td>train_nn_reg_585afc1b</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_585afc1b_7_batch_size=64,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0000,data_transfo_2025-02-12_10-28-10/error.txt</td></tr>\n",
       "<tr><td>train_nn_reg_96d75adb</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_96d75adb_12_batch_size=64,data_aggregation=None,data_selection=abundance_ith,data_selection_i=8,data_transform=None,e_2025-02-12_10-30-34/error.txt</td></tr>\n",
       "<tr><td>train_nn_reg_fb67bf3c</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_fb67bf3c_14_batch_size=64,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0000,data_transf_2025-02-12_10-32-55/error.txt</td></tr>\n",
       "<tr><td>train_nn_reg_885d4181</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_885d4181_17_batch_size=64,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0002,data_transf_2025-02-12_10-37-12/error.txt</td></tr>\n",
       "<tr><td>train_nn_reg_f82534bf</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_f82534bf_18_batch_size=32,data_aggregation=None,data_selection=abundance_ith,data_selection_i=5,data_transform=None,e_2025-02-12_10-37-23/error.txt</td></tr>\n",
       "<tr><td>train_nn_reg_db54a0f9</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_db54a0f9_19_batch_size=64,data_aggregation=None,data_selection=abundance_ith,data_selection_i=13,data_transform=None,_2025-02-12_10-37-40/error.txt</td></tr>\n",
       "<tr><td>train_nn_reg_bd4ccfe5</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_bd4ccfe5_22_batch_size=64,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0001,data_transf_2025-02-12_10-41-09/error.txt</td></tr>\n",
       "<tr><td>train_nn_reg_f0dd288e</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_f0dd288e_23_batch_size=32,data_aggregation=None,data_selection=abundance_ith,data_selection_i=7,data_transform=None,e_2025-02-12_10-41-18/error.txt</td></tr>\n",
       "<tr><td>train_nn_reg_7f5aa875</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_7f5aa875_27_batch_size=64,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0006,data_transf_2025-02-12_10-43-36/error.txt</td></tr>\n",
       "<tr><td>train_nn_reg_b1eab41a</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_b1eab41a_28_batch_size=64,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0007,data_transf_2025-02-12_10-43-51/error.txt</td></tr>\n",
       "<tr><td>train_nn_reg_52256c85</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_52256c85_29_batch_size=64,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0007,data_transf_2025-02-12_10-44-01/error.txt</td></tr>\n",
       "<tr><td>train_nn_reg_fe0d4d04</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_fe0d4d04_30_batch_size=64,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0008,data_transf_2025-02-12_10-44-11/error.txt</td></tr>\n",
       "<tr><td>train_nn_reg_a68b3ce9</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_a68b3ce9_31_batch_size=64,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0013,data_transf_2025-02-12_10-44-28/error.txt</td></tr>\n",
       "<tr><td>train_nn_reg_6a6e83a2</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_6a6e83a2_32_batch_size=64,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0011,data_transf_2025-02-12_10-44-37/error.txt</td></tr>\n",
       "<tr><td>train_nn_reg_658848a7</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_658848a7_33_batch_size=32,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0011,data_transf_2025-02-12_10-44-50/error.txt</td></tr>\n",
       "<tr><td>train_nn_reg_d7e6c1b5</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_d7e6c1b5_34_batch_size=32,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0010,data_transf_2025-02-12_10-45-00/error.txt</td></tr>\n",
       "<tr><td>train_nn_reg_c085a7aa</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_c085a7aa_35_batch_size=64,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0010,data_transf_2025-02-12_10-45-08/error.txt</td></tr>\n",
       "<tr><td>train_nn_reg_832a9f56</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_832a9f56_36_batch_size=64,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0011,data_transf_2025-02-12_10-45-25/error.txt</td></tr>\n",
       "<tr><td>train_nn_reg_eee28542</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_eee28542_37_batch_size=64,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0011,data_transf_2025-02-12_10-45-33/error.txt</td></tr>\n",
       "<tr><td>train_nn_reg_8d531d6a</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_8d531d6a_38_batch_size=32,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0011,data_transf_2025-02-12_10-45-40/error.txt</td></tr>\n",
       "<tr><td>train_nn_reg_6b109c2e</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_6b109c2e_39_batch_size=32,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0010,data_transf_2025-02-12_10-45-48/error.txt</td></tr>\n",
       "<tr><td>train_nn_reg_5473c5fe</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_5473c5fe_40_batch_size=64,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0011,data_transf_2025-02-12_10-45-54/error.txt</td></tr>\n",
       "<tr><td>train_nn_reg_46be22cb</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_46be22cb_41_batch_size=64,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0011,data_transf_2025-02-12_10-46-04/error.txt</td></tr>\n",
       "<tr><td>train_nn_reg_b0c744e4</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_b0c744e4_42_batch_size=32,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0011,data_transf_2025-02-12_10-46-12/error.txt</td></tr>\n",
       "<tr><td>train_nn_reg_e750ae98</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_e750ae98_43_batch_size=32,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0009,data_transf_2025-02-12_10-46-19/error.txt</td></tr>\n",
       "<tr><td>train_nn_reg_56629185</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_56629185_44_batch_size=64,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0007,data_transf_2025-02-12_10-46-28/error.txt</td></tr>\n",
       "<tr><td>train_nn_reg_14270525</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_14270525_45_batch_size=32,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0007,data_transf_2025-02-12_10-46-38/error.txt</td></tr>\n",
       "<tr><td>train_nn_reg_9290736f</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_9290736f_46_batch_size=32,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0007,data_transf_2025-02-12_10-46-46/error.txt</td></tr>\n",
       "<tr><td>train_nn_reg_667fe2f0</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_667fe2f0_47_batch_size=64,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0007,data_transf_2025-02-12_10-46-55/error.txt</td></tr>\n",
       "<tr><td>train_nn_reg_3cacc334</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_3cacc334_48_batch_size=32,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0007,data_transf_2025-02-12_10-47-05/error.txt</td></tr>\n",
       "<tr><td>train_nn_reg_66004043</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_66004043_49_batch_size=64,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0008,data_transf_2025-02-12_10-48-22/error.txt</td></tr>\n",
       "<tr><td>train_nn_reg_df106c16</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_df106c16_50_batch_size=64,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0007,data_transf_2025-02-12_10-48-31/error.txt</td></tr>\n",
       "<tr><td>train_nn_reg_7ffde30b</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_7ffde30b_51_batch_size=64,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0007,data_transf_2025-02-12_10-48-40/error.txt</td></tr>\n",
       "<tr><td>train_nn_reg_d9a45f0c</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_d9a45f0c_52_batch_size=32,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0007,data_transf_2025-02-12_10-48-49/error.txt</td></tr>\n",
       "<tr><td>train_nn_reg_aa8b02c2</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_aa8b02c2_53_batch_size=32,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0008,data_transf_2025-02-12_10-48-58/error.txt</td></tr>\n",
       "<tr><td>train_nn_reg_af910a96</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_af910a96_54_batch_size=32,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0007,data_transf_2025-02-12_10-49-08/error.txt</td></tr>\n",
       "<tr><td>train_nn_reg_0d054fd5</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_0d054fd5_55_batch_size=32,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0007,data_transf_2025-02-12_10-49-18/error.txt</td></tr>\n",
       "<tr><td>train_nn_reg_cd9994a1</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_cd9994a1_56_batch_size=32,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0008,data_transf_2025-02-12_10-49-27/error.txt</td></tr>\n",
       "<tr><td>train_nn_reg_9314c34e</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_9314c34e_57_batch_size=32,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0008,data_transf_2025-02-12_10-49-36/error.txt</td></tr>\n",
       "<tr><td>train_nn_reg_c0b612be</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_c0b612be_58_batch_size=32,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0007,data_transf_2025-02-12_10-49-45/error.txt</td></tr>\n",
       "<tr><td>train_nn_reg_221e4edc</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_221e4edc_59_batch_size=32,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0006,data_transf_2025-02-12_10-49-55/error.txt</td></tr>\n",
       "<tr><td>train_nn_reg_7da01b72</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_7da01b72_60_batch_size=64,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0007,data_transf_2025-02-12_10-50-05/error.txt</td></tr>\n",
       "<tr><td>train_nn_reg_0607541b</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_0607541b_61_batch_size=32,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0007,data_transf_2025-02-12_10-50-15/error.txt</td></tr>\n",
       "<tr><td>train_nn_reg_23753ef6</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_23753ef6_62_batch_size=64,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0008,data_transf_2025-02-12_10-50-23/error.txt</td></tr>\n",
       "<tr><td>train_nn_reg_c8b3d228</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_c8b3d228_63_batch_size=32,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0009,data_transf_2025-02-12_10-50-33/error.txt</td></tr>\n",
       "<tr><td>train_nn_reg_b7d8f5dd</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_b7d8f5dd_64_batch_size=64,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0007,data_transf_2025-02-12_10-50-41/error.txt</td></tr>\n",
       "<tr><td>train_nn_reg_17a82890</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_17a82890_65_batch_size=64,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0008,data_transf_2025-02-12_10-50-51/error.txt</td></tr>\n",
       "<tr><td>train_nn_reg_eee9180a</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_eee9180a_66_batch_size=32,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0007,data_transf_2025-02-12_10-51-02/error.txt</td></tr>\n",
       "<tr><td>train_nn_reg_efd632d9</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_efd632d9_67_batch_size=64,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0006,data_transf_2025-02-12_10-51-11/error.txt</td></tr>\n",
       "<tr><td>train_nn_reg_dc108924</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_dc108924_68_batch_size=64,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0007,data_transf_2025-02-12_10-51-19/error.txt</td></tr>\n",
       "<tr><td>train_nn_reg_bcd4bc02</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_bcd4bc02_69_batch_size=32,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0008,data_transf_2025-02-12_10-51-29/error.txt</td></tr>\n",
       "<tr><td>train_nn_reg_9df06404</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_9df06404_70_batch_size=64,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0006,data_transf_2025-02-12_10-51-38/error.txt</td></tr>\n",
       "<tr><td>train_nn_reg_d0e4e46e</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_d0e4e46e_71_batch_size=32,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0008,data_transf_2025-02-12_10-51-50/error.txt</td></tr>\n",
       "<tr><td>train_nn_reg_11891b5f</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_11891b5f_72_batch_size=64,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0007,data_transf_2025-02-12_10-51-59/error.txt</td></tr>\n",
       "<tr><td>train_nn_reg_8466fb79</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_8466fb79_73_batch_size=64,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0008,data_transf_2025-02-12_10-52-11/error.txt</td></tr>\n",
       "<tr><td>train_nn_reg_395d94f1</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_395d94f1_74_batch_size=64,data_aggregation=None,data_selection=abundance_ith,data_selection_i=2,data_transform=None,e_2025-02-12_10-52-19/error.txt</td></tr>\n",
       "<tr><td>train_nn_reg_a7d0654d</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_a7d0654d_75_batch_size=32,data_aggregation=None,data_selection=abundance_ith,data_selection_i=3,data_transform=None,e_2025-02-12_10-52-27/error.txt</td></tr>\n",
       "<tr><td>train_nn_reg_595528ac</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_595528ac_76_batch_size=64,data_aggregation=None,data_selection=abundance_ith,data_selection_i=2,data_transform=None,e_2025-02-12_10-52-35/error.txt</td></tr>\n",
       "<tr><td>train_nn_reg_fbb76e04</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_fbb76e04_77_batch_size=32,data_aggregation=None,data_selection=abundance_ith,data_selection_i=2,data_transform=None,e_2025-02-12_10-52-42/error.txt</td></tr>\n",
       "<tr><td>train_nn_reg_dcd4a051</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_dcd4a051_78_batch_size=64,data_aggregation=None,data_selection=abundance_ith,data_selection_i=2,data_transform=None,e_2025-02-12_10-52-49/error.txt</td></tr>\n",
       "<tr><td>train_nn_reg_5781d649</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_5781d649_79_batch_size=64,data_aggregation=None,data_selection=abundance_ith,data_selection_i=3,data_transform=None,e_2025-02-12_10-52-56/error.txt</td></tr>\n",
       "<tr><td>train_nn_reg_c96b517b</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_c96b517b_80_batch_size=64,data_aggregation=None,data_selection=abundance_ith,data_selection_i=2,data_transform=None,e_2025-02-12_10-53-04/error.txt</td></tr>\n",
       "<tr><td>train_nn_reg_f6efb6b7</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_f6efb6b7_81_batch_size=32,data_aggregation=None,data_selection=abundance_ith,data_selection_i=2,data_transform=None,e_2025-02-12_10-53-11/error.txt</td></tr>\n",
       "<tr><td>train_nn_reg_916754ae</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_916754ae_82_batch_size=32,data_aggregation=None,data_selection=abundance_ith,data_selection_i=2,data_transform=None,e_2025-02-12_10-53-18/error.txt</td></tr>\n",
       "<tr><td>train_nn_reg_a489d7a5</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_a489d7a5_83_batch_size=64,data_aggregation=None,data_selection=abundance_ith,data_selection_i=3,data_transform=None,e_2025-02-12_10-53-25/error.txt</td></tr>\n",
       "<tr><td>train_nn_reg_bf250c6e</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_bf250c6e_84_batch_size=32,data_aggregation=None,data_selection=abundance_ith,data_selection_i=2,data_transform=None,e_2025-02-12_10-53-33/error.txt</td></tr>\n",
       "<tr><td>train_nn_reg_7d1f05b3</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_7d1f05b3_85_batch_size=32,data_aggregation=None,data_selection=abundance_ith,data_selection_i=2,data_transform=None,e_2025-02-12_10-53-39/error.txt</td></tr>\n",
       "<tr><td>train_nn_reg_c7e2ac19</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_c7e2ac19_86_batch_size=64,data_aggregation=None,data_selection=abundance_ith,data_selection_i=2,data_transform=None,e_2025-02-12_10-53-46/error.txt</td></tr>\n",
       "<tr><td>train_nn_reg_9e6167db</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_9e6167db_87_batch_size=32,data_aggregation=None,data_selection=abundance_ith,data_selection_i=2,data_transform=None,e_2025-02-12_10-53-54/error.txt</td></tr>\n",
       "<tr><td>train_nn_reg_7ac2c560</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_7ac2c560_88_batch_size=64,data_aggregation=None,data_selection=abundance_ith,data_selection_i=2,data_transform=None,e_2025-02-12_10-54-02/error.txt</td></tr>\n",
       "<tr><td>train_nn_reg_68016e2d</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_68016e2d_89_batch_size=64,data_aggregation=None,data_selection=abundance_ith,data_selection_i=2,data_transform=None,e_2025-02-12_10-54-09/error.txt</td></tr>\n",
       "<tr><td>train_nn_reg_4166b2ca</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_4166b2ca_90_batch_size=32,data_aggregation=None,data_selection=abundance_ith,data_selection_i=2,data_transform=None,e_2025-02-12_10-54-17/error.txt</td></tr>\n",
       "<tr><td>train_nn_reg_595d4bda</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_595d4bda_91_batch_size=32,data_aggregation=None,data_selection=abundance_ith,data_selection_i=2,data_transform=None,e_2025-02-12_10-54-23/error.txt</td></tr>\n",
       "<tr><td>train_nn_reg_17ec6b6d</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_17ec6b6d_92_batch_size=64,data_aggregation=None,data_selection=abundance_ith,data_selection_i=2,data_transform=None,e_2025-02-12_10-54-30/error.txt</td></tr>\n",
       "<tr><td>train_nn_reg_6c34831c</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_6c34831c_93_batch_size=64,data_aggregation=None,data_selection=abundance_ith,data_selection_i=2,data_transform=None,e_2025-02-12_10-54-39/error.txt</td></tr>\n",
       "<tr><td>train_nn_reg_f6ead3d2</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_f6ead3d2_94_batch_size=32,data_aggregation=None,data_selection=abundance_ith,data_selection_i=2,data_transform=None,e_2025-02-12_10-54-45/error.txt</td></tr>\n",
       "<tr><td>train_nn_reg_62ddb4ba</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_62ddb4ba_95_batch_size=32,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,e_2025-02-12_10-54-52/error.txt</td></tr>\n",
       "<tr><td>train_nn_reg_0aeac832</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_0aeac832_96_batch_size=32,data_aggregation=None,data_selection=abundance_ith,data_selection_i=2,data_transform=None,e_2025-02-12_10-55-02/error.txt</td></tr>\n",
       "<tr><td>train_nn_reg_0a5872d4</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_0a5872d4_97_batch_size=32,data_aggregation=None,data_selection=abundance_ith,data_selection_i=2,data_transform=None,e_2025-02-12_10-55-11/error.txt</td></tr>\n",
       "<tr><td>train_nn_reg_85fc59e5</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_85fc59e5_98_batch_size=32,data_aggregation=None,data_selection=abundance_ith,data_selection_i=2,data_transform=None,e_2025-02-12_10-55-21/error.txt</td></tr>\n",
       "<tr><td>train_nn_reg_f9cf00e2</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_f9cf00e2_99_batch_size=32,data_aggregation=None,data_selection=abundance_ith,data_selection_i=2,data_transform=None,e_2025-02-12_10-55-27/error.txt</td></tr>\n",
       "<tr><td>train_nn_reg_de26a40b</td><td style=\"text-align: right;\">           1</td><td>/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_de26a40b_100_batch_size=32,data_aggregation=None,data_selection=abundance_ith,data_selection_i=3,data_transform=None,_2025-02-12_10-55-34/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".messages {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  padding-left: 1em;\n",
       "  overflow-y: auto;\n",
       "}\n",
       ".messages h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n",
       "\n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>status    </th><th>loc                  </th><th style=\"text-align: right;\">  batch_size</th><th>data_aggregation  </th><th>data_selection    </th><th style=\"text-align: right;\">  data_selection_i</th><th style=\"text-align: right;\">  data_selection_t</th><th>data_transform  </th><th style=\"text-align: right;\">  epochs</th><th style=\"text-align: right;\">  learning_rate</th><th style=\"text-align: right;\">  max_layers</th><th>model  </th><th style=\"text-align: right;\">  n_hidden_layers</th><th style=\"text-align: right;\">  n_units_hl0</th><th style=\"text-align: right;\">  n_units_hl1</th><th style=\"text-align: right;\">  n_units_hl10</th><th style=\"text-align: right;\">  n_units_hl11</th><th style=\"text-align: right;\">  n_units_hl12</th><th style=\"text-align: right;\">  n_units_hl13</th><th style=\"text-align: right;\">  n_units_hl14</th><th style=\"text-align: right;\">  n_units_hl15</th><th style=\"text-align: right;\">  n_units_hl16</th><th style=\"text-align: right;\">  n_units_hl17</th><th style=\"text-align: right;\">  n_units_hl18</th><th style=\"text-align: right;\">  n_units_hl19</th><th style=\"text-align: right;\">  n_units_hl2</th><th style=\"text-align: right;\">  n_units_hl20</th><th style=\"text-align: right;\">  n_units_hl21</th><th style=\"text-align: right;\">  n_units_hl22</th><th style=\"text-align: right;\">  n_units_hl23</th><th style=\"text-align: right;\">  n_units_hl24</th><th style=\"text-align: right;\">  n_units_hl25</th><th style=\"text-align: right;\">  n_units_hl26</th><th style=\"text-align: right;\">  n_units_hl27</th><th style=\"text-align: right;\">  n_units_hl28</th><th style=\"text-align: right;\">  n_units_hl29</th><th style=\"text-align: right;\">  n_units_hl3</th><th style=\"text-align: right;\">  n_units_hl4</th><th style=\"text-align: right;\">  n_units_hl5</th><th style=\"text-align: right;\">  n_units_hl6</th><th style=\"text-align: right;\">  n_units_hl7</th><th style=\"text-align: right;\">  n_units_hl8</th><th style=\"text-align: right;\">  n_units_hl9</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  rmse_val</th><th style=\"text-align: right;\">     r2_val</th><th style=\"text-align: right;\">  loss_val</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_nn_reg_301534e7</td><td>TERMINATED</td><td>172.31.181.85:3412262</td><td style=\"text-align: right;\">         128</td><td>                  </td><td>abundance_ith     </td><td style=\"text-align: right;\">                13</td><td style=\"text-align: right;\">                  </td><td>                </td><td style=\"text-align: right;\">      50</td><td style=\"text-align: right;\">         0.0005</td><td style=\"text-align: right;\">          25</td><td>nn_reg </td><td style=\"text-align: right;\">               11</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        37.2765 </td><td style=\"text-align: right;\">   1.10952</td><td style=\"text-align: right;\">  -0.304364</td><td style=\"text-align: right;\">   1.23103</td></tr>\n",
       "<tr><td>train_nn_reg_b9035250</td><td>TERMINATED</td><td>172.31.181.85:3412418</td><td style=\"text-align: right;\">         256</td><td>                  </td><td>variance_threshold</td><td style=\"text-align: right;\">                  </td><td style=\"text-align: right;\">       0.000171692</td><td>                </td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">         0.005 </td><td style=\"text-align: right;\">          10</td><td>nn_reg </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        31.3854 </td><td style=\"text-align: right;\">   4.73963</td><td style=\"text-align: right;\"> -22.8022  </td><td style=\"text-align: right;\">  22.4641 </td></tr>\n",
       "<tr><td>train_nn_reg_e26bf114</td><td>TERMINATED</td><td>172.31.181.85:3413652</td><td style=\"text-align: right;\">         128</td><td>                  </td><td>variance_threshold</td><td style=\"text-align: right;\">                  </td><td style=\"text-align: right;\">       0.00183727 </td><td>                </td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">         0.001 </td><td style=\"text-align: right;\">          10</td><td>nn_reg </td><td style=\"text-align: right;\">                6</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        37.7054 </td><td style=\"text-align: right;\">   1.08222</td><td style=\"text-align: right;\">  -0.240975</td><td style=\"text-align: right;\">   1.17121</td></tr>\n",
       "<tr><td>train_nn_reg_8358dac8</td><td>TERMINATED</td><td>172.31.181.85:3414139</td><td style=\"text-align: right;\">         128</td><td>                  </td><td>abundance_ith     </td><td style=\"text-align: right;\">                16</td><td style=\"text-align: right;\">                  </td><td>                </td><td style=\"text-align: right;\">     100</td><td style=\"text-align: right;\">         5e-05 </td><td style=\"text-align: right;\">          15</td><td>nn_reg </td><td style=\"text-align: right;\">                6</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         7.85748</td><td style=\"text-align: right;\">  18.7596 </td><td style=\"text-align: right;\">-371.888   </td><td style=\"text-align: right;\"> 351.924  </td></tr>\n",
       "<tr><td>train_nn_reg_3c4bd4b6</td><td>TERMINATED</td><td>172.31.181.85:3414455</td><td style=\"text-align: right;\">         256</td><td>                  </td><td>variance_threshold</td><td style=\"text-align: right;\">                  </td><td style=\"text-align: right;\">       0.00246915 </td><td>                </td><td style=\"text-align: right;\">     100</td><td style=\"text-align: right;\">         1e-05 </td><td style=\"text-align: right;\">          30</td><td>nn_reg </td><td style=\"text-align: right;\">               16</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         4.10647</td><td style=\"text-align: right;\">  18.6498 </td><td style=\"text-align: right;\">-367.535   </td><td style=\"text-align: right;\"> 347.816  </td></tr>\n",
       "<tr><td>train_nn_reg_4450d409</td><td>TERMINATED</td><td>172.31.181.85:3417235</td><td style=\"text-align: right;\">         128</td><td>                  </td><td>variance_threshold</td><td style=\"text-align: right;\">                  </td><td style=\"text-align: right;\">       0.00314253 </td><td>                </td><td style=\"text-align: right;\">     100</td><td style=\"text-align: right;\">         0.0005</td><td style=\"text-align: right;\">          30</td><td>nn_reg </td><td style=\"text-align: right;\">               21</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        62.9084 </td><td style=\"text-align: right;\">   1.00624</td><td style=\"text-align: right;\">  -0.072843</td><td style=\"text-align: right;\">   1.01253</td></tr>\n",
       "<tr><td>train_nn_reg_734b55b9</td><td>TERMINATED</td><td>172.31.181.85:3418637</td><td style=\"text-align: right;\">         128</td><td>                  </td><td>abundance_ith     </td><td style=\"text-align: right;\">                19</td><td style=\"text-align: right;\">                  </td><td>                </td><td style=\"text-align: right;\">     100</td><td style=\"text-align: right;\">         0.0001</td><td style=\"text-align: right;\">          25</td><td>nn_reg </td><td style=\"text-align: right;\">               11</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         6.03579</td><td style=\"text-align: right;\">  18.5553 </td><td style=\"text-align: right;\">-363.807   </td><td style=\"text-align: right;\"> 344.298  </td></tr>\n",
       "<tr><td>train_nn_reg_a70dbacd</td><td>TERMINATED</td><td>172.31.181.85:3422740</td><td style=\"text-align: right;\">         128</td><td>                  </td><td>abundance_ith     </td><td style=\"text-align: right;\">                11</td><td style=\"text-align: right;\">                  </td><td>                </td><td style=\"text-align: right;\">     100</td><td style=\"text-align: right;\">         0.01  </td><td style=\"text-align: right;\">           5</td><td>nn_reg </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        35.4225 </td><td style=\"text-align: right;\">   1.13798</td><td style=\"text-align: right;\">  -0.372142</td><td style=\"text-align: right;\">   1.295  </td></tr>\n",
       "<tr><td>train_nn_reg_e03fa800</td><td>TERMINATED</td><td>172.31.181.85:3426945</td><td style=\"text-align: right;\">         128</td><td>                  </td><td>abundance_ith     </td><td style=\"text-align: right;\">                19</td><td style=\"text-align: right;\">                  </td><td>                </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">         0.005 </td><td style=\"text-align: right;\">          20</td><td>nn_reg </td><td style=\"text-align: right;\">                6</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         5.5193 </td><td style=\"text-align: right;\">   1.72695</td><td style=\"text-align: right;\">  -2.16002 </td><td style=\"text-align: right;\">   2.98237</td></tr>\n",
       "<tr><td>train_nn_reg_40b434d4</td><td>TERMINATED</td><td>172.31.181.85:3432842</td><td style=\"text-align: right;\">         256</td><td>                  </td><td>variance_threshold</td><td style=\"text-align: right;\">                  </td><td style=\"text-align: right;\">       0.00740206 </td><td>                </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">         0.001 </td><td style=\"text-align: right;\">          25</td><td>nn_reg </td><td style=\"text-align: right;\">               21</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         4.77275</td><td style=\"text-align: right;\">  18.2389 </td><td style=\"text-align: right;\">-351.473   </td><td style=\"text-align: right;\"> 332.657  </td></tr>\n",
       "<tr><td>train_nn_reg_dac60601</td><td>TERMINATED</td><td>172.31.181.85:3435637</td><td style=\"text-align: right;\">         256</td><td>                  </td><td>abundance_ith     </td><td style=\"text-align: right;\">                19</td><td style=\"text-align: right;\">                  </td><td>                </td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">         0.0005</td><td style=\"text-align: right;\">          25</td><td>nn_reg </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         4.23577</td><td style=\"text-align: right;\">  18.5134 </td><td style=\"text-align: right;\">-362.164   </td><td style=\"text-align: right;\"> 342.747  </td></tr>\n",
       "<tr><td>train_nn_reg_0ed9ed1e</td><td>TERMINATED</td><td>172.31.181.85:3440497</td><td style=\"text-align: right;\">         256</td><td>                  </td><td>variance_threshold</td><td style=\"text-align: right;\">                  </td><td style=\"text-align: right;\">       0.00376597 </td><td>                </td><td style=\"text-align: right;\">      50</td><td style=\"text-align: right;\">         0.01  </td><td style=\"text-align: right;\">          10</td><td>nn_reg </td><td style=\"text-align: right;\">                6</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        13.2994 </td><td style=\"text-align: right;\">   2.40484</td><td style=\"text-align: right;\">  -5.12774 </td><td style=\"text-align: right;\">   5.78324</td></tr>\n",
       "<tr><td>train_nn_reg_9c57c6f3</td><td>TERMINATED</td><td>172.31.181.85:3441044</td><td style=\"text-align: right;\">         256</td><td>                  </td><td>variance_threshold</td><td style=\"text-align: right;\">                  </td><td style=\"text-align: right;\">       2.10165e-05</td><td>                </td><td style=\"text-align: right;\">     100</td><td style=\"text-align: right;\">         0.005 </td><td style=\"text-align: right;\">          10</td><td>nn_reg </td><td style=\"text-align: right;\">                6</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         7.05567</td><td style=\"text-align: right;\">  18.0913 </td><td style=\"text-align: right;\">-345.79    </td><td style=\"text-align: right;\"> 327.294  </td></tr>\n",
       "<tr><td>train_nn_reg_3c8a1025</td><td>TERMINATED</td><td>172.31.181.85:3444073</td><td style=\"text-align: right;\">         128</td><td>                  </td><td>abundance_ith     </td><td style=\"text-align: right;\">                 4</td><td style=\"text-align: right;\">                  </td><td>                </td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">         0.01  </td><td style=\"text-align: right;\">          20</td><td>nn_reg </td><td style=\"text-align: right;\">               11</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        41.4296 </td><td style=\"text-align: right;\">   1.05476</td><td style=\"text-align: right;\">  -0.178793</td><td style=\"text-align: right;\">   1.11252</td></tr>\n",
       "<tr><td>train_nn_reg_fdd33f93</td><td>ERROR     </td><td>172.31.181.85:3412050</td><td style=\"text-align: right;\">          32</td><td>                  </td><td>variance_threshold</td><td style=\"text-align: right;\">                  </td><td style=\"text-align: right;\">       6.16517e-05</td><td>                </td><td style=\"text-align: right;\">     100</td><td style=\"text-align: right;\">         0.0005</td><td style=\"text-align: right;\">          20</td><td>nn_reg </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_nn_reg_b5df96e2</td><td>ERROR     </td><td>172.31.181.85:3412110</td><td style=\"text-align: right;\">          32</td><td>                  </td><td>variance_threshold</td><td style=\"text-align: right;\">                  </td><td style=\"text-align: right;\">       0.00229368 </td><td>                </td><td style=\"text-align: right;\">      50</td><td style=\"text-align: right;\">         0.01  </td><td style=\"text-align: right;\">          10</td><td>nn_reg </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_nn_reg_126b80e4</td><td>ERROR     </td><td>172.31.181.85:3412625</td><td style=\"text-align: right;\">          32</td><td>                  </td><td>abundance_ith     </td><td style=\"text-align: right;\">                15</td><td style=\"text-align: right;\">                  </td><td>                </td><td style=\"text-align: right;\">      50</td><td style=\"text-align: right;\">         5e-05 </td><td style=\"text-align: right;\">          25</td><td>nn_reg </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_nn_reg_a8c9e393</td><td>ERROR     </td><td>172.31.181.85:3413014</td><td style=\"text-align: right;\">          64</td><td>                  </td><td>variance_threshold</td><td style=\"text-align: right;\">                  </td><td style=\"text-align: right;\">       1.94399e-05</td><td>                </td><td style=\"text-align: right;\">      50</td><td style=\"text-align: right;\">         0.0001</td><td style=\"text-align: right;\">          30</td><td>nn_reg </td><td style=\"text-align: right;\">               16</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_nn_reg_585afc1b</td><td>ERROR     </td><td>172.31.181.85:3413386</td><td style=\"text-align: right;\">          64</td><td>                  </td><td>variance_threshold</td><td style=\"text-align: right;\">                  </td><td style=\"text-align: right;\">       2.28138e-05</td><td>                </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">         0.005 </td><td style=\"text-align: right;\">          20</td><td>nn_reg </td><td style=\"text-align: right;\">               11</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_nn_reg_96d75adb</td><td>ERROR     </td><td>172.31.181.85:3418258</td><td style=\"text-align: right;\">          64</td><td>                  </td><td>abundance_ith     </td><td style=\"text-align: right;\">                 8</td><td style=\"text-align: right;\">                  </td><td>                </td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">         0.005 </td><td style=\"text-align: right;\">          25</td><td>nn_reg </td><td style=\"text-align: right;\">               16</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_nn_reg_fb67bf3c</td><td>ERROR     </td><td>172.31.181.85:3422373</td><td style=\"text-align: right;\">          64</td><td>                  </td><td>variance_threshold</td><td style=\"text-align: right;\">                  </td><td style=\"text-align: right;\">       2.86777e-05</td><td>                </td><td style=\"text-align: right;\">      50</td><td style=\"text-align: right;\">         0.0001</td><td style=\"text-align: right;\">          30</td><td>nn_reg </td><td style=\"text-align: right;\">               16</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_nn_reg_885d4181</td><td>ERROR     </td><td>172.31.181.85:3431251</td><td style=\"text-align: right;\">          64</td><td>                  </td><td>variance_threshold</td><td style=\"text-align: right;\">                  </td><td style=\"text-align: right;\">       0.000175545</td><td>                </td><td style=\"text-align: right;\">      50</td><td style=\"text-align: right;\">         0.0005</td><td style=\"text-align: right;\">          10</td><td>nn_reg </td><td style=\"text-align: right;\">                6</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_nn_reg_f82534bf</td><td>ERROR     </td><td>172.31.181.85:3431740</td><td style=\"text-align: right;\">          32</td><td>                  </td><td>abundance_ith     </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                  </td><td>                </td><td style=\"text-align: right;\">     100</td><td style=\"text-align: right;\">         0.001 </td><td style=\"text-align: right;\">          30</td><td>nn_reg </td><td style=\"text-align: right;\">               21</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_nn_reg_db54a0f9</td><td>ERROR     </td><td>172.31.181.85:3432408</td><td style=\"text-align: right;\">          64</td><td>                  </td><td>abundance_ith     </td><td style=\"text-align: right;\">                13</td><td style=\"text-align: right;\">                  </td><td>                </td><td style=\"text-align: right;\">      50</td><td style=\"text-align: right;\">         5e-05 </td><td style=\"text-align: right;\">           5</td><td>nn_reg </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_nn_reg_bd4ccfe5</td><td>ERROR     </td><td>172.31.181.85:3439792</td><td style=\"text-align: right;\">          64</td><td>                  </td><td>variance_threshold</td><td style=\"text-align: right;\">                  </td><td style=\"text-align: right;\">       6.08867e-05</td><td>                </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">         0.0001</td><td style=\"text-align: right;\">          10</td><td>nn_reg </td><td style=\"text-align: right;\">                6</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_nn_reg_f0dd288e</td><td>ERROR     </td><td>172.31.181.85:3440117</td><td style=\"text-align: right;\">          32</td><td>                  </td><td>abundance_ith     </td><td style=\"text-align: right;\">                 7</td><td style=\"text-align: right;\">                  </td><td>                </td><td style=\"text-align: right;\">     100</td><td style=\"text-align: right;\">         5e-05 </td><td style=\"text-align: right;\">           5</td><td>nn_reg </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_nn_reg_7f5aa875</td><td>ERROR     </td><td>172.31.181.85:3444755</td><td style=\"text-align: right;\">          64</td><td>                  </td><td>variance_threshold</td><td style=\"text-align: right;\">                  </td><td style=\"text-align: right;\">       0.000622695</td><td>                </td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">         0.001 </td><td style=\"text-align: right;\">           5</td><td>nn_reg </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_nn_reg_b1eab41a</td><td>ERROR     </td><td>172.31.181.85:3445363</td><td style=\"text-align: right;\">          64</td><td>                  </td><td>variance_threshold</td><td style=\"text-align: right;\">                  </td><td style=\"text-align: right;\">       0.000692551</td><td>                </td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">         0.001 </td><td style=\"text-align: right;\">           5</td><td>nn_reg </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_nn_reg_52256c85</td><td>ERROR     </td><td>172.31.181.85:3445857</td><td style=\"text-align: right;\">          64</td><td>                  </td><td>variance_threshold</td><td style=\"text-align: right;\">                  </td><td style=\"text-align: right;\">       0.000657133</td><td>                </td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">         0.001 </td><td style=\"text-align: right;\">           5</td><td>nn_reg </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_nn_reg_fe0d4d04</td><td>ERROR     </td><td>172.31.181.85:3446244</td><td style=\"text-align: right;\">          64</td><td>                  </td><td>variance_threshold</td><td style=\"text-align: right;\">                  </td><td style=\"text-align: right;\">       0.000781389</td><td>                </td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">         0.001 </td><td style=\"text-align: right;\">           5</td><td>nn_reg </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_nn_reg_a68b3ce9</td><td>ERROR     </td><td>172.31.181.85:3446692</td><td style=\"text-align: right;\">          64</td><td>                  </td><td>variance_threshold</td><td style=\"text-align: right;\">                  </td><td style=\"text-align: right;\">       0.00125497 </td><td>                </td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">         0.01  </td><td style=\"text-align: right;\">           5</td><td>nn_reg </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_nn_reg_6a6e83a2</td><td>ERROR     </td><td>172.31.181.85:3447018</td><td style=\"text-align: right;\">          64</td><td>                  </td><td>variance_threshold</td><td style=\"text-align: right;\">                  </td><td style=\"text-align: right;\">       0.00112518 </td><td>                </td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">         0.01  </td><td style=\"text-align: right;\">           5</td><td>nn_reg </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_nn_reg_658848a7</td><td>ERROR     </td><td>172.31.181.85:3447518</td><td style=\"text-align: right;\">          32</td><td>                  </td><td>variance_threshold</td><td style=\"text-align: right;\">                  </td><td style=\"text-align: right;\">       0.00105562 </td><td>                </td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">         0.01  </td><td style=\"text-align: right;\">           5</td><td>nn_reg </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_nn_reg_d7e6c1b5</td><td>ERROR     </td><td>172.31.181.85:3447752</td><td style=\"text-align: right;\">          32</td><td>                  </td><td>variance_threshold</td><td style=\"text-align: right;\">                  </td><td style=\"text-align: right;\">       0.00100247 </td><td>                </td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">         0.01  </td><td style=\"text-align: right;\">           5</td><td>nn_reg </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_nn_reg_c085a7aa</td><td>ERROR     </td><td>172.31.181.85:3448363</td><td style=\"text-align: right;\">          64</td><td>                  </td><td>variance_threshold</td><td style=\"text-align: right;\">                  </td><td style=\"text-align: right;\">       0.000967344</td><td>                </td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">         0.01  </td><td style=\"text-align: right;\">           5</td><td>nn_reg </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_nn_reg_832a9f56</td><td>ERROR     </td><td>172.31.181.85:3449111</td><td style=\"text-align: right;\">          64</td><td>                  </td><td>variance_threshold</td><td style=\"text-align: right;\">                  </td><td style=\"text-align: right;\">       0.00111992 </td><td>                </td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">         0.01  </td><td style=\"text-align: right;\">           5</td><td>nn_reg </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_nn_reg_eee28542</td><td>ERROR     </td><td>172.31.181.85:3449654</td><td style=\"text-align: right;\">          64</td><td>                  </td><td>variance_threshold</td><td style=\"text-align: right;\">                  </td><td style=\"text-align: right;\">       0.00110375 </td><td>                </td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">         0.01  </td><td style=\"text-align: right;\">           5</td><td>nn_reg </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_nn_reg_8d531d6a</td><td>ERROR     </td><td>172.31.181.85:3450075</td><td style=\"text-align: right;\">          32</td><td>                  </td><td>variance_threshold</td><td style=\"text-align: right;\">                  </td><td style=\"text-align: right;\">       0.00113227 </td><td>                </td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">         0.01  </td><td style=\"text-align: right;\">           5</td><td>nn_reg </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_nn_reg_6b109c2e</td><td>ERROR     </td><td>172.31.181.85:3450351</td><td style=\"text-align: right;\">          32</td><td>                  </td><td>variance_threshold</td><td style=\"text-align: right;\">                  </td><td style=\"text-align: right;\">       0.00104855 </td><td>                </td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">         0.01  </td><td style=\"text-align: right;\">           5</td><td>nn_reg </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_nn_reg_5473c5fe</td><td>ERROR     </td><td>172.31.181.85:3450819</td><td style=\"text-align: right;\">          64</td><td>                  </td><td>variance_threshold</td><td style=\"text-align: right;\">                  </td><td style=\"text-align: right;\">       0.0010843  </td><td>                </td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">         0.01  </td><td style=\"text-align: right;\">           5</td><td>nn_reg </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_nn_reg_46be22cb</td><td>ERROR     </td><td>172.31.181.85:3451213</td><td style=\"text-align: right;\">          64</td><td>                  </td><td>variance_threshold</td><td style=\"text-align: right;\">                  </td><td style=\"text-align: right;\">       0.00105627 </td><td>                </td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">         0.001 </td><td style=\"text-align: right;\">           5</td><td>nn_reg </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_nn_reg_b0c744e4</td><td>ERROR     </td><td>172.31.181.85:3451388</td><td style=\"text-align: right;\">          32</td><td>                  </td><td>variance_threshold</td><td style=\"text-align: right;\">                  </td><td style=\"text-align: right;\">       0.0010952  </td><td>                </td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">         0.01  </td><td style=\"text-align: right;\">           5</td><td>nn_reg </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_nn_reg_e750ae98</td><td>ERROR     </td><td>172.31.181.85:3451707</td><td style=\"text-align: right;\">          32</td><td>                  </td><td>variance_threshold</td><td style=\"text-align: right;\">                  </td><td style=\"text-align: right;\">       0.000917041</td><td>                </td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">         0.01  </td><td style=\"text-align: right;\">           5</td><td>nn_reg </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_nn_reg_56629185</td><td>ERROR     </td><td>172.31.181.85:3452120</td><td style=\"text-align: right;\">          64</td><td>                  </td><td>variance_threshold</td><td style=\"text-align: right;\">                  </td><td style=\"text-align: right;\">       0.000663141</td><td>                </td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">         0.001 </td><td style=\"text-align: right;\">           5</td><td>nn_reg </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_nn_reg_14270525</td><td>ERROR     </td><td>172.31.181.85:3452556</td><td style=\"text-align: right;\">          32</td><td>                  </td><td>variance_threshold</td><td style=\"text-align: right;\">                  </td><td style=\"text-align: right;\">       0.000731896</td><td>                </td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">         0.001 </td><td style=\"text-align: right;\">           5</td><td>nn_reg </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_nn_reg_9290736f</td><td>ERROR     </td><td>172.31.181.85:3452820</td><td style=\"text-align: right;\">          32</td><td>                  </td><td>variance_threshold</td><td style=\"text-align: right;\">                  </td><td style=\"text-align: right;\">       0.000731825</td><td>                </td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">         0.001 </td><td style=\"text-align: right;\">          15</td><td>nn_reg </td><td style=\"text-align: right;\">               11</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_nn_reg_667fe2f0</td><td>ERROR     </td><td>172.31.181.85:3453142</td><td style=\"text-align: right;\">          64</td><td>                  </td><td>variance_threshold</td><td style=\"text-align: right;\">                  </td><td style=\"text-align: right;\">       0.000670802</td><td>                </td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">         0.001 </td><td style=\"text-align: right;\">          15</td><td>nn_reg </td><td style=\"text-align: right;\">               11</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_nn_reg_3cacc334</td><td>ERROR     </td><td>172.31.181.85:3453498</td><td style=\"text-align: right;\">          32</td><td>                  </td><td>variance_threshold</td><td style=\"text-align: right;\">                  </td><td style=\"text-align: right;\">       0.000731386</td><td>                </td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">         0.001 </td><td style=\"text-align: right;\">          15</td><td>nn_reg </td><td style=\"text-align: right;\">               11</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_nn_reg_66004043</td><td>ERROR     </td><td>172.31.181.85:3453925</td><td style=\"text-align: right;\">          64</td><td>                  </td><td>variance_threshold</td><td style=\"text-align: right;\">                  </td><td style=\"text-align: right;\">       0.00078045 </td><td>                </td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">         0.001 </td><td style=\"text-align: right;\">          15</td><td>nn_reg </td><td style=\"text-align: right;\">               11</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_nn_reg_df106c16</td><td>ERROR     </td><td>172.31.181.85:3454188</td><td style=\"text-align: right;\">          64</td><td>                  </td><td>variance_threshold</td><td style=\"text-align: right;\">                  </td><td style=\"text-align: right;\">       0.000741138</td><td>                </td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">         0.001 </td><td style=\"text-align: right;\">          15</td><td>nn_reg </td><td style=\"text-align: right;\">               11</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_nn_reg_7ffde30b</td><td>ERROR     </td><td>172.31.181.85:3454360</td><td style=\"text-align: right;\">          64</td><td>                  </td><td>variance_threshold</td><td style=\"text-align: right;\">                  </td><td style=\"text-align: right;\">       0.000739003</td><td>                </td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">         0.001 </td><td style=\"text-align: right;\">          15</td><td>nn_reg </td><td style=\"text-align: right;\">               11</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_nn_reg_d9a45f0c</td><td>ERROR     </td><td>172.31.181.85:3454711</td><td style=\"text-align: right;\">          32</td><td>                  </td><td>variance_threshold</td><td style=\"text-align: right;\">                  </td><td style=\"text-align: right;\">       0.000698328</td><td>                </td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">         0.001 </td><td style=\"text-align: right;\">          15</td><td>nn_reg </td><td style=\"text-align: right;\">               11</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_nn_reg_aa8b02c2</td><td>ERROR     </td><td>172.31.181.85:3454883</td><td style=\"text-align: right;\">          32</td><td>                  </td><td>variance_threshold</td><td style=\"text-align: right;\">                  </td><td style=\"text-align: right;\">       0.000750146</td><td>                </td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">         0.001 </td><td style=\"text-align: right;\">          15</td><td>nn_reg </td><td style=\"text-align: right;\">               11</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_nn_reg_af910a96</td><td>ERROR     </td><td>172.31.181.85:3455266</td><td style=\"text-align: right;\">          32</td><td>                  </td><td>variance_threshold</td><td style=\"text-align: right;\">                  </td><td style=\"text-align: right;\">       0.000691345</td><td>                </td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">         0.001 </td><td style=\"text-align: right;\">          15</td><td>nn_reg </td><td style=\"text-align: right;\">               11</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_nn_reg_0d054fd5</td><td>ERROR     </td><td>172.31.181.85:3455594</td><td style=\"text-align: right;\">          32</td><td>                  </td><td>variance_threshold</td><td style=\"text-align: right;\">                  </td><td style=\"text-align: right;\">       0.000693963</td><td>                </td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">         0.001 </td><td style=\"text-align: right;\">          15</td><td>nn_reg </td><td style=\"text-align: right;\">               11</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_nn_reg_cd9994a1</td><td>ERROR     </td><td>172.31.181.85:3456068</td><td style=\"text-align: right;\">          32</td><td>                  </td><td>variance_threshold</td><td style=\"text-align: right;\">                  </td><td style=\"text-align: right;\">       0.000761197</td><td>                </td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">         0.001 </td><td style=\"text-align: right;\">          15</td><td>nn_reg </td><td style=\"text-align: right;\">               11</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_nn_reg_9314c34e</td><td>ERROR     </td><td>172.31.181.85:3456390</td><td style=\"text-align: right;\">          32</td><td>                  </td><td>variance_threshold</td><td style=\"text-align: right;\">                  </td><td style=\"text-align: right;\">       0.000782762</td><td>                </td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">         0.001 </td><td style=\"text-align: right;\">          15</td><td>nn_reg </td><td style=\"text-align: right;\">               11</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_nn_reg_c0b612be</td><td>ERROR     </td><td>172.31.181.85:3456561</td><td style=\"text-align: right;\">          32</td><td>                  </td><td>variance_threshold</td><td style=\"text-align: right;\">                  </td><td style=\"text-align: right;\">       0.000704007</td><td>                </td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">         0.001 </td><td style=\"text-align: right;\">          15</td><td>nn_reg </td><td style=\"text-align: right;\">               11</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_nn_reg_221e4edc</td><td>ERROR     </td><td>172.31.181.85:3456883</td><td style=\"text-align: right;\">          32</td><td>                  </td><td>variance_threshold</td><td style=\"text-align: right;\">                  </td><td style=\"text-align: right;\">       0.00063096 </td><td>                </td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">         0.001 </td><td style=\"text-align: right;\">          15</td><td>nn_reg </td><td style=\"text-align: right;\">               11</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_nn_reg_7da01b72</td><td>ERROR     </td><td>172.31.181.85:3457297</td><td style=\"text-align: right;\">          64</td><td>                  </td><td>variance_threshold</td><td style=\"text-align: right;\">                  </td><td style=\"text-align: right;\">       0.000691047</td><td>                </td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">         0.001 </td><td style=\"text-align: right;\">          15</td><td>nn_reg </td><td style=\"text-align: right;\">               11</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_nn_reg_0607541b</td><td>ERROR     </td><td>172.31.181.85:3457717</td><td style=\"text-align: right;\">          32</td><td>                  </td><td>variance_threshold</td><td style=\"text-align: right;\">                  </td><td style=\"text-align: right;\">       0.000660612</td><td>                </td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">         0.001 </td><td style=\"text-align: right;\">          15</td><td>nn_reg </td><td style=\"text-align: right;\">               11</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_nn_reg_23753ef6</td><td>ERROR     </td><td>172.31.181.85:3457982</td><td style=\"text-align: right;\">          64</td><td>                  </td><td>variance_threshold</td><td style=\"text-align: right;\">                  </td><td style=\"text-align: right;\">       0.000762039</td><td>                </td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">         0.001 </td><td style=\"text-align: right;\">          15</td><td>nn_reg </td><td style=\"text-align: right;\">               11</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_nn_reg_c8b3d228</td><td>ERROR     </td><td>172.31.181.85:3458161</td><td style=\"text-align: right;\">          32</td><td>                  </td><td>variance_threshold</td><td style=\"text-align: right;\">                  </td><td style=\"text-align: right;\">       0.00087136 </td><td>                </td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">         0.001 </td><td style=\"text-align: right;\">          15</td><td>nn_reg </td><td style=\"text-align: right;\">               11</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_nn_reg_b7d8f5dd</td><td>ERROR     </td><td>172.31.181.85:3458366</td><td style=\"text-align: right;\">          64</td><td>                  </td><td>variance_threshold</td><td style=\"text-align: right;\">                  </td><td style=\"text-align: right;\">       0.000726321</td><td>                </td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">         0.001 </td><td style=\"text-align: right;\">          15</td><td>nn_reg </td><td style=\"text-align: right;\">               11</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_nn_reg_17a82890</td><td>ERROR     </td><td>172.31.181.85:3458778</td><td style=\"text-align: right;\">          64</td><td>                  </td><td>variance_threshold</td><td style=\"text-align: right;\">                  </td><td style=\"text-align: right;\">       0.000838026</td><td>                </td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">         0.001 </td><td style=\"text-align: right;\">          15</td><td>nn_reg </td><td style=\"text-align: right;\">               11</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_nn_reg_eee9180a</td><td>ERROR     </td><td>172.31.181.85:3459018</td><td style=\"text-align: right;\">          32</td><td>                  </td><td>variance_threshold</td><td style=\"text-align: right;\">                  </td><td style=\"text-align: right;\">       0.000737606</td><td>                </td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">         0.001 </td><td style=\"text-align: right;\">          15</td><td>nn_reg </td><td style=\"text-align: right;\">               11</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_nn_reg_efd632d9</td><td>ERROR     </td><td>172.31.181.85:3459390</td><td style=\"text-align: right;\">          64</td><td>                  </td><td>variance_threshold</td><td style=\"text-align: right;\">                  </td><td style=\"text-align: right;\">       0.000556933</td><td>                </td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">         0.001 </td><td style=\"text-align: right;\">          15</td><td>nn_reg </td><td style=\"text-align: right;\">               11</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_nn_reg_dc108924</td><td>ERROR     </td><td>172.31.181.85:3459594</td><td style=\"text-align: right;\">          64</td><td>                  </td><td>variance_threshold</td><td style=\"text-align: right;\">                  </td><td style=\"text-align: right;\">       0.000691506</td><td>                </td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">         0.001 </td><td style=\"text-align: right;\">          15</td><td>nn_reg </td><td style=\"text-align: right;\">               11</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_nn_reg_bcd4bc02</td><td>ERROR     </td><td>172.31.181.85:3459766</td><td style=\"text-align: right;\">          32</td><td>                  </td><td>variance_threshold</td><td style=\"text-align: right;\">                  </td><td style=\"text-align: right;\">       0.000849706</td><td>                </td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">         0.001 </td><td style=\"text-align: right;\">          15</td><td>nn_reg </td><td style=\"text-align: right;\">               11</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_nn_reg_9df06404</td><td>ERROR     </td><td>172.31.181.85:3459971</td><td style=\"text-align: right;\">          64</td><td>                  </td><td>variance_threshold</td><td style=\"text-align: right;\">                  </td><td style=\"text-align: right;\">       0.00061344 </td><td>                </td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">         0.001 </td><td style=\"text-align: right;\">          15</td><td>nn_reg </td><td style=\"text-align: right;\">               11</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_nn_reg_d0e4e46e</td><td>ERROR     </td><td>172.31.181.85:3460383</td><td style=\"text-align: right;\">          32</td><td>                  </td><td>variance_threshold</td><td style=\"text-align: right;\">                  </td><td style=\"text-align: right;\">       0.000778417</td><td>                </td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">         0.001 </td><td style=\"text-align: right;\">          15</td><td>nn_reg </td><td style=\"text-align: right;\">               11</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_nn_reg_11891b5f</td><td>ERROR     </td><td>172.31.181.85:3460719</td><td style=\"text-align: right;\">          64</td><td>                  </td><td>variance_threshold</td><td style=\"text-align: right;\">                  </td><td style=\"text-align: right;\">       0.000668482</td><td>                </td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">         0.001 </td><td style=\"text-align: right;\">          15</td><td>nn_reg </td><td style=\"text-align: right;\">               11</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_nn_reg_8466fb79</td><td>ERROR     </td><td>172.31.181.85:3460987</td><td style=\"text-align: right;\">          64</td><td>                  </td><td>variance_threshold</td><td style=\"text-align: right;\">                  </td><td style=\"text-align: right;\">       0.000761244</td><td>                </td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">         0.001 </td><td style=\"text-align: right;\">           5</td><td>nn_reg </td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_nn_reg_395d94f1</td><td>ERROR     </td><td>172.31.181.85:3461101</td><td style=\"text-align: right;\">          64</td><td>                  </td><td>abundance_ith     </td><td style=\"text-align: right;\">                 2</td><td style=\"text-align: right;\">                  </td><td>                </td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">         0.0005</td><td style=\"text-align: right;\">          30</td><td>nn_reg </td><td style=\"text-align: right;\">               26</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_nn_reg_a7d0654d</td><td>ERROR     </td><td>172.31.181.85:3461190</td><td style=\"text-align: right;\">          32</td><td>                  </td><td>abundance_ith     </td><td style=\"text-align: right;\">                 3</td><td style=\"text-align: right;\">                  </td><td>                </td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">         0.0005</td><td style=\"text-align: right;\">          30</td><td>nn_reg </td><td style=\"text-align: right;\">               26</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_nn_reg_595528ac</td><td>ERROR     </td><td>172.31.181.85:3461278</td><td style=\"text-align: right;\">          64</td><td>                  </td><td>abundance_ith     </td><td style=\"text-align: right;\">                 2</td><td style=\"text-align: right;\">                  </td><td>                </td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">         0.01  </td><td style=\"text-align: right;\">          30</td><td>nn_reg </td><td style=\"text-align: right;\">               26</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_nn_reg_fbb76e04</td><td>ERROR     </td><td>172.31.181.85:3461425</td><td style=\"text-align: right;\">          32</td><td>                  </td><td>abundance_ith     </td><td style=\"text-align: right;\">                 2</td><td style=\"text-align: right;\">                  </td><td>                </td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">         0.0005</td><td style=\"text-align: right;\">          30</td><td>nn_reg </td><td style=\"text-align: right;\">               26</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_nn_reg_dcd4a051</td><td>ERROR     </td><td>172.31.181.85:3461513</td><td style=\"text-align: right;\">          64</td><td>                  </td><td>abundance_ith     </td><td style=\"text-align: right;\">                 2</td><td style=\"text-align: right;\">                  </td><td>                </td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">         0.0005</td><td style=\"text-align: right;\">          30</td><td>nn_reg </td><td style=\"text-align: right;\">               26</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_nn_reg_5781d649</td><td>ERROR     </td><td>172.31.181.85:3461661</td><td style=\"text-align: right;\">          64</td><td>                  </td><td>abundance_ith     </td><td style=\"text-align: right;\">                 3</td><td style=\"text-align: right;\">                  </td><td>                </td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">         0.0005</td><td style=\"text-align: right;\">          30</td><td>nn_reg </td><td style=\"text-align: right;\">               26</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_nn_reg_c96b517b</td><td>ERROR     </td><td>172.31.181.85:3461820</td><td style=\"text-align: right;\">          64</td><td>                  </td><td>abundance_ith     </td><td style=\"text-align: right;\">                 2</td><td style=\"text-align: right;\">                  </td><td>                </td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">         0.01  </td><td style=\"text-align: right;\">          30</td><td>nn_reg </td><td style=\"text-align: right;\">               26</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_nn_reg_f6efb6b7</td><td>ERROR     </td><td>172.31.181.85:3461907</td><td style=\"text-align: right;\">          32</td><td>                  </td><td>abundance_ith     </td><td style=\"text-align: right;\">                 2</td><td style=\"text-align: right;\">                  </td><td>                </td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">         0.0005</td><td style=\"text-align: right;\">          30</td><td>nn_reg </td><td style=\"text-align: right;\">               26</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_nn_reg_916754ae</td><td>ERROR     </td><td>172.31.181.85:3462000</td><td style=\"text-align: right;\">          32</td><td>                  </td><td>abundance_ith     </td><td style=\"text-align: right;\">                 2</td><td style=\"text-align: right;\">                  </td><td>                </td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">         0.01  </td><td style=\"text-align: right;\">          30</td><td>nn_reg </td><td style=\"text-align: right;\">               26</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_nn_reg_a489d7a5</td><td>ERROR     </td><td>172.31.181.85:3462147</td><td style=\"text-align: right;\">          64</td><td>                  </td><td>abundance_ith     </td><td style=\"text-align: right;\">                 3</td><td style=\"text-align: right;\">                  </td><td>                </td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">         0.01  </td><td style=\"text-align: right;\">          30</td><td>nn_reg </td><td style=\"text-align: right;\">               26</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_nn_reg_bf250c6e</td><td>ERROR     </td><td>172.31.181.85:3462295</td><td style=\"text-align: right;\">          32</td><td>                  </td><td>abundance_ith     </td><td style=\"text-align: right;\">                 2</td><td style=\"text-align: right;\">                  </td><td>                </td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">         0.01  </td><td style=\"text-align: right;\">          30</td><td>nn_reg </td><td style=\"text-align: right;\">               26</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_nn_reg_7d1f05b3</td><td>ERROR     </td><td>172.31.181.85:3462383</td><td style=\"text-align: right;\">          32</td><td>                  </td><td>abundance_ith     </td><td style=\"text-align: right;\">                 2</td><td style=\"text-align: right;\">                  </td><td>                </td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">         0.01  </td><td style=\"text-align: right;\">          30</td><td>nn_reg </td><td style=\"text-align: right;\">               26</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_nn_reg_c7e2ac19</td><td>ERROR     </td><td>172.31.181.85:3462529</td><td style=\"text-align: right;\">          64</td><td>                  </td><td>abundance_ith     </td><td style=\"text-align: right;\">                 2</td><td style=\"text-align: right;\">                  </td><td>                </td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">         0.01  </td><td style=\"text-align: right;\">          30</td><td>nn_reg </td><td style=\"text-align: right;\">               26</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_nn_reg_9e6167db</td><td>ERROR     </td><td>172.31.181.85:3462677</td><td style=\"text-align: right;\">          32</td><td>                  </td><td>abundance_ith     </td><td style=\"text-align: right;\">                 2</td><td style=\"text-align: right;\">                  </td><td>                </td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">         0.01  </td><td style=\"text-align: right;\">          30</td><td>nn_reg </td><td style=\"text-align: right;\">               26</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_nn_reg_7ac2c560</td><td>ERROR     </td><td>172.31.181.85:3462845</td><td style=\"text-align: right;\">          64</td><td>                  </td><td>abundance_ith     </td><td style=\"text-align: right;\">                 2</td><td style=\"text-align: right;\">                  </td><td>                </td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">         0.0005</td><td style=\"text-align: right;\">          30</td><td>nn_reg </td><td style=\"text-align: right;\">               26</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_nn_reg_68016e2d</td><td>ERROR     </td><td>172.31.181.85:3462991</td><td style=\"text-align: right;\">          64</td><td>                  </td><td>abundance_ith     </td><td style=\"text-align: right;\">                 2</td><td style=\"text-align: right;\">                  </td><td>                </td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">         0.01  </td><td style=\"text-align: right;\">          30</td><td>nn_reg </td><td style=\"text-align: right;\">               26</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_nn_reg_4166b2ca</td><td>ERROR     </td><td>172.31.181.85:3463097</td><td style=\"text-align: right;\">          32</td><td>                  </td><td>abundance_ith     </td><td style=\"text-align: right;\">                 2</td><td style=\"text-align: right;\">                  </td><td>                </td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">         0.0005</td><td style=\"text-align: right;\">          30</td><td>nn_reg </td><td style=\"text-align: right;\">               26</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_nn_reg_595d4bda</td><td>ERROR     </td><td>172.31.181.85:3463188</td><td style=\"text-align: right;\">          32</td><td>                  </td><td>abundance_ith     </td><td style=\"text-align: right;\">                 2</td><td style=\"text-align: right;\">                  </td><td>                </td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">         0.01  </td><td style=\"text-align: right;\">          30</td><td>nn_reg </td><td style=\"text-align: right;\">               26</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_nn_reg_17ec6b6d</td><td>ERROR     </td><td>172.31.181.85:3463453</td><td style=\"text-align: right;\">          64</td><td>                  </td><td>abundance_ith     </td><td style=\"text-align: right;\">                 2</td><td style=\"text-align: right;\">                  </td><td>                </td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">         0.0005</td><td style=\"text-align: right;\">          30</td><td>nn_reg </td><td style=\"text-align: right;\">               26</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_nn_reg_6c34831c</td><td>ERROR     </td><td>172.31.181.85:3463957</td><td style=\"text-align: right;\">          64</td><td>                  </td><td>abundance_ith     </td><td style=\"text-align: right;\">                 2</td><td style=\"text-align: right;\">                  </td><td>                </td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">         0.0005</td><td style=\"text-align: right;\">          30</td><td>nn_reg </td><td style=\"text-align: right;\">               26</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_nn_reg_f6ead3d2</td><td>ERROR     </td><td>172.31.181.85:3464046</td><td style=\"text-align: right;\">          32</td><td>                  </td><td>abundance_ith     </td><td style=\"text-align: right;\">                 2</td><td style=\"text-align: right;\">                  </td><td>                </td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">         0.0005</td><td style=\"text-align: right;\">          30</td><td>nn_reg </td><td style=\"text-align: right;\">               26</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_nn_reg_62ddb4ba</td><td>ERROR     </td><td>172.31.181.85:3464133</td><td style=\"text-align: right;\">          32</td><td>                  </td><td>abundance_ith     </td><td style=\"text-align: right;\">                 4</td><td style=\"text-align: right;\">                  </td><td>                </td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">         0.0005</td><td style=\"text-align: right;\">          30</td><td>nn_reg </td><td style=\"text-align: right;\">               26</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_nn_reg_0aeac832</td><td>ERROR     </td><td>172.31.181.85:3464365</td><td style=\"text-align: right;\">          32</td><td>                  </td><td>abundance_ith     </td><td style=\"text-align: right;\">                 2</td><td style=\"text-align: right;\">                  </td><td>                </td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">         0.0005</td><td style=\"text-align: right;\">          30</td><td>nn_reg </td><td style=\"text-align: right;\">               26</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_nn_reg_0a5872d4</td><td>ERROR     </td><td>172.31.181.85:3464517</td><td style=\"text-align: right;\">          32</td><td>                  </td><td>abundance_ith     </td><td style=\"text-align: right;\">                 2</td><td style=\"text-align: right;\">                  </td><td>                </td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">         0.01  </td><td style=\"text-align: right;\">          30</td><td>nn_reg </td><td style=\"text-align: right;\">               26</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_nn_reg_85fc59e5</td><td>ERROR     </td><td>172.31.181.85:3464665</td><td style=\"text-align: right;\">          32</td><td>                  </td><td>abundance_ith     </td><td style=\"text-align: right;\">                 2</td><td style=\"text-align: right;\">                  </td><td>                </td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">         0.01  </td><td style=\"text-align: right;\">          30</td><td>nn_reg </td><td style=\"text-align: right;\">               26</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_nn_reg_f9cf00e2</td><td>ERROR     </td><td>172.31.181.85:3464813</td><td style=\"text-align: right;\">          32</td><td>                  </td><td>abundance_ith     </td><td style=\"text-align: right;\">                 2</td><td style=\"text-align: right;\">                  </td><td>                </td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">         0.01  </td><td style=\"text-align: right;\">          30</td><td>nn_reg </td><td style=\"text-align: right;\">               26</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_nn_reg_de26a40b</td><td>ERROR     </td><td>172.31.181.85:3464960</td><td style=\"text-align: right;\">          32</td><td>                  </td><td>abundance_ith     </td><td style=\"text-align: right;\">                 3</td><td style=\"text-align: right;\">                  </td><td>                </td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">         0.01  </td><td style=\"text-align: right;\">          30</td><td>nn_reg </td><td style=\"text-align: right;\">               26</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">          </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/optuna/distributions.py:708: UserWarning: The distribution is specified by [1, 20] and step=5, but the range is not divisible by `step`. It will be replaced by [1, 16].\n",
      "  warnings.warn(\n",
      "/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/optuna/distributions.py:708: UserWarning: The distribution is specified by [1, 10] and step=5, but the range is not divisible by `step`. It will be replaced by [1, 6].\n",
      "  warnings.warn(\n",
      "\u001b[36m(train_nn_reg pid=3412050)\u001b[0m Seed set to 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3412050)\u001b[0m Train: (156, 66), Test: (65, 66)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3412050)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3412050)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3412050)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3412050)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_fdd33f93_1_batch_size=32,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0001,data_transfo_2025-02-12_10-27-30 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3412050)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3412050)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3412050)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3412050)\u001b[0m 0 | layers | ModuleList | 1.1 K  | train\n",
      "\u001b[36m(train_nn_reg pid=3412050)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3412050)\u001b[0m 1.1 K     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3412050)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3412050)\u001b[0m 1.1 K     Total params\n",
      "\u001b[36m(train_nn_reg pid=3412050)\u001b[0m 0.004     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3412050)\u001b[0m 4         Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3412050)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3412050)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(train_nn_reg pid=3412050)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3412050)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-02-12 10:27:37,385\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_fdd33f93\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3412050, ip=172.31.181.85, actor_id=e97d54c4b95344f8417da7fe01000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1026, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 216, in run\n",
      "    self.advance()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 455, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 151, in run\n",
      "    self.on_advance_end(data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 370, in on_advance_end\n",
      "    self.val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 2) cannot be concatenated\n",
      "/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/optuna/distributions.py:708: UserWarning: The distribution is specified by [1, 25] and step=5, but the range is not divisible by `step`. It will be replaced by [1, 21].\n",
      "  warnings.warn(\n",
      "\u001b[36m(train_nn_reg pid=3412110)\u001b[0m Seed set to 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3412110)\u001b[0m Train: (156, 42), Test: (65, 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3412110)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3412110)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3412110)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3412110)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_b5df96e2_2_batch_size=32,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0023,data_transfo_2025-02-12_10-27-36 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3412110)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3412110)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3412110)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3412110)\u001b[0m 0 | layers | ModuleList | 289    | train\n",
      "\u001b[36m(train_nn_reg pid=3412110)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3412110)\u001b[0m 289       Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3412110)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3412110)\u001b[0m 289       Total params\n",
      "\u001b[36m(train_nn_reg pid=3412110)\u001b[0m 0.001     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3412110)\u001b[0m 4         Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3412110)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3412110)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(train_nn_reg pid=3412110)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3412110)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-02-12 10:27:43,064\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_b5df96e2\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3412110, ip=172.31.181.85, actor_id=081a77e7418fe50e95d6eff701000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1026, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 216, in run\n",
      "    self.advance()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 455, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 151, in run\n",
      "    self.on_advance_end(data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 370, in on_advance_end\n",
      "    self.val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 2) cannot be concatenated\n",
      "\u001b[36m(train_nn_reg pid=3412262)\u001b[0m Seed set to 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3412262)\u001b[0m Train: (156, 2594), Test: (65, 2594)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3412262)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3412262)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3412262)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3412262)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_301534e7_3_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=13,data_transform=None,_2025-02-12_10-27-42 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3412262)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3412262)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3412262)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3412262)\u001b[0m 0 | layers | ModuleList | 1.9 M  | train\n",
      "\u001b[36m(train_nn_reg pid=3412262)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3412262)\u001b[0m 1.9 M     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3412262)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3412262)\u001b[0m 1.9 M     Total params\n",
      "\u001b[36m(train_nn_reg pid=3412262)\u001b[0m 7.799     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3412262)\u001b[0m 24        Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3412262)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3412262)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(train_nn_reg pid=3412262)\u001b[0m Metric train_rmse does not exist in `trainer.callback_metrics.\n",
      "\u001b[36m(train_nn_reg pid=3412262)\u001b[0m Metric train_r2 does not exist in `trainer.callback_metrics.\n",
      "\u001b[36m(train_nn_reg pid=3412262)\u001b[0m Metric train_loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[36m(train_nn_reg pid=3412262)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_301534e7_3_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=13,data_transform=None,_2025-02-12_10-27-42/checkpoint_000000)\n",
      "2025-02-12 10:27:51,078\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.602 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:27:51,080\tWARNING util.py:201 -- The `process_trial_result` operation took 1.605 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:27:51,080\tWARNING util.py:201 -- Processing trial results took 1.605 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:27:51,081\tWARNING util.py:201 -- The `process_trial_result` operation took 1.606 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3412262)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_301534e7_3_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=13,data_transform=None,_2025-02-12_10-27-42/checkpoint_000001)\n",
      "2025-02-12 10:27:53,349\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.662 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:27:53,351\tWARNING util.py:201 -- The `process_trial_result` operation took 1.665 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:27:53,352\tWARNING util.py:201 -- Processing trial results took 1.665 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:27:53,352\tWARNING util.py:201 -- The `process_trial_result` operation took 1.666 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3412418)\u001b[0m Seed set to 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3412418)\u001b[0m Train: (156, 52), Test: (65, 52)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3412418)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3412418)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3412418)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3412418)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_b9035250_4_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0002,data_transf_2025-02-12_10-27-48 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3412418)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3412418)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3412418)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3412418)\u001b[0m 0 | layers | ModuleList | 609    | train\n",
      "\u001b[36m(train_nn_reg pid=3412418)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3412418)\u001b[0m 609       Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3412418)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3412418)\u001b[0m 609       Total params\n",
      "\u001b[36m(train_nn_reg pid=3412418)\u001b[0m 0.002     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3412418)\u001b[0m 4         Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3412418)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3412418)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(train_nn_reg pid=3412418)\u001b[0m Metric train_rmse does not exist in `trainer.callback_metrics.\n",
      "\u001b[36m(train_nn_reg pid=3412418)\u001b[0m Metric train_r2 does not exist in `trainer.callback_metrics.\n",
      "\u001b[36m(train_nn_reg pid=3412418)\u001b[0m Metric train_loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[36m(train_nn_reg pid=3412262)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_301534e7_3_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=13,data_transform=None,_2025-02-12_10-27-42/checkpoint_000002)\n",
      "2025-02-12 10:27:56,664\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.676 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:27:56,667\tWARNING util.py:201 -- The `process_trial_result` operation took 1.678 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:27:56,667\tWARNING util.py:201 -- Processing trial results took 1.679 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:27:56,667\tWARNING util.py:201 -- The `process_trial_result` operation took 1.679 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:27:59,805\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 3.123 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:27:59,808\tWARNING util.py:201 -- The `process_trial_result` operation took 3.126 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:27:59,808\tWARNING util.py:201 -- Processing trial results took 3.126 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:27:59,809\tWARNING util.py:201 -- The `process_trial_result` operation took 3.127 s, which may be a performance bottleneck.\n",
      "/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/optuna/distributions.py:708: UserWarning: The distribution is specified by [1, 30] and step=5, but the range is not divisible by `step`. It will be replaced by [1, 26].\n",
      "  warnings.warn(\n",
      "\u001b[36m(train_nn_reg pid=3412625)\u001b[0m Seed set to 12\n",
      "\u001b[36m(train_nn_reg pid=3412262)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_301534e7_3_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=13,data_transform=None,_2025-02-12_10-27-42/checkpoint_000003)\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3412625)\u001b[0m Train: (156, 2982), Test: (65, 2982)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3412625)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3412625)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3412625)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3412625)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_126b80e4_5_batch_size=32,data_aggregation=None,data_selection=abundance_ith,data_selection_i=15,data_transform=None,e_2025-02-12_10-27-54 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3412625)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3412625)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3412625)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3412625)\u001b[0m 0 | layers | ModuleList | 188 K  | train\n",
      "\u001b[36m(train_nn_reg pid=3412625)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3412625)\u001b[0m 188 K     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3412625)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3412625)\u001b[0m 188 K     Total params\n",
      "\u001b[36m(train_nn_reg pid=3412625)\u001b[0m 0.755     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3412625)\u001b[0m 4         Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3412625)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3412625)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(train_nn_reg pid=3412625)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3412625)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-02-12 10:28:03,434\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.851 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:28:03,436\tWARNING util.py:201 -- The `process_trial_result` operation took 1.854 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:28:03,436\tWARNING util.py:201 -- Processing trial results took 1.854 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:28:03,437\tWARNING util.py:201 -- The `process_trial_result` operation took 1.855 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:28:03,448\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_126b80e4\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3412625, ip=172.31.181.85, actor_id=6d63960914e5a4bd8af79ae201000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1026, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 216, in run\n",
      "    self.advance()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 455, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 151, in run\n",
      "    self.on_advance_end(data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 370, in on_advance_end\n",
      "    self.val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 2) cannot be concatenated\n",
      "2025-02-12 10:28:05,233\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.412 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:28:05,235\tWARNING util.py:201 -- The `process_trial_result` operation took 1.415 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:28:05,236\tWARNING util.py:201 -- Processing trial results took 1.415 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:28:05,236\tWARNING util.py:201 -- The `process_trial_result` operation took 1.416 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:28:06,918\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.680 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:28:06,920\tWARNING util.py:201 -- The `process_trial_result` operation took 1.682 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:28:06,921\tWARNING util.py:201 -- Processing trial results took 1.683 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:28:06,922\tWARNING util.py:201 -- The `process_trial_result` operation took 1.684 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:28:06,932\tWARNING syncer.py:406 -- Last sync command failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 404, in _launch_sync_process\n",
      "    self.wait()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 474, in wait\n",
      "    raise e\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 472, in wait\n",
      "    self._sync_process.wait(timeout=timeout or self.sync_timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 173, in wait\n",
      "    raise exception\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 136, in entrypoint\n",
      "    result = self._fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/storage.py\", line 218, in _upload_to_fs_path\n",
      "    _pyarrow_fs_copy_files(local_path, fs_path, destination_filesystem=fs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/storage.py\", line 116, in _pyarrow_fs_copy_files\n",
      "    return pyarrow.fs.copy_files(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/pyarrow/fs.py\", line 259, in copy_files\n",
      "    _copy_files_selector(source_fs, source_sel,\n",
      "  File \"pyarrow/_fs.pyx\", line 1624, in pyarrow._fs._copy_files_selector\n",
      "  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\n",
      "FileNotFoundError: [Errno 2] Failed to open local file '/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_301534e7_3_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=13,data_transform=None,_2025-02-12_10-27-42/epoch=1-val_rmse=18.79.ckpt'. Detail: [errno 2] No such file or directory\n",
      "\n",
      "\u001b[36m(train_nn_reg pid=3412418)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_b9035250_4_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0002,data_transf_2025-02-12_10-27-48/checkpoint_000002)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2025-02-12 10:28:08,565\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.413 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:28:08,568\tWARNING util.py:201 -- The `process_trial_result` operation took 1.416 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:28:08,568\tWARNING util.py:201 -- Processing trial results took 1.417 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:28:08,569\tWARNING util.py:201 -- The `process_trial_result` operation took 1.417 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3413014)\u001b[0m Seed set to 12\n",
      "\u001b[36m(train_nn_reg pid=3413014)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3413014)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3413014)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3413014)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_a8c9e393_6_batch_size=64,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0000,data_transfo_2025-02-12_10-28-01 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3413014)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3413014)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3413014)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3413014)\u001b[0m 0 | layers | ModuleList | 308 K  | train\n",
      "\u001b[36m(train_nn_reg pid=3413014)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3413014)\u001b[0m 308 K     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3413014)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3413014)\u001b[0m 308 K     Total params\n",
      "\u001b[36m(train_nn_reg pid=3413014)\u001b[0m 1.235     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3413014)\u001b[0m 34        Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3413014)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3413014)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3413014)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3413014)\u001b[0m Train: (156, 103), Test: (65, 103)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-12 10:28:12,033\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.736 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:28:12,036\tWARNING util.py:201 -- The `process_trial_result` operation took 1.739 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:28:12,037\tWARNING util.py:201 -- Processing trial results took 1.740 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:28:12,037\tWARNING util.py:201 -- The `process_trial_result` operation took 1.741 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:28:12,043\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_a8c9e393\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3413014, ip=172.31.181.85, actor_id=69c5c272e62c5d09e79cebc801000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1024, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1053, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 1) cannot be concatenated\n",
      "\u001b[36m(train_nn_reg pid=3412262)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_301534e7_3_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=13,data_transform=None,_2025-02-12_10-27-42/checkpoint_000006)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2025-02-12 10:28:14,228\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.937 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:28:14,229\tWARNING util.py:201 -- The `process_trial_result` operation took 1.939 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:28:14,230\tWARNING util.py:201 -- Processing trial results took 1.939 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:28:14,231\tWARNING util.py:201 -- The `process_trial_result` operation took 1.940 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:28:15,979\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.741 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:28:15,982\tWARNING util.py:201 -- The `process_trial_result` operation took 1.743 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:28:15,982\tWARNING util.py:201 -- Processing trial results took 1.744 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:28:15,983\tWARNING util.py:201 -- The `process_trial_result` operation took 1.744 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:28:16,005\tERROR experiment_state.py:173 -- Saving experiment state to storage at '/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg' failed with exception: \n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/execution/experiment_state.py\", line 171, in wait_for_sync\n",
      "    self._storage.syncer.wait()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 474, in wait\n",
      "    raise e\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 472, in wait\n",
      "    self._sync_process.wait(timeout=timeout or self.sync_timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 173, in wait\n",
      "    raise exception\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 136, in entrypoint\n",
      "    result = self._fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/storage.py\", line 218, in _upload_to_fs_path\n",
      "    _pyarrow_fs_copy_files(local_path, fs_path, destination_filesystem=fs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/storage.py\", line 116, in _pyarrow_fs_copy_files\n",
      "    return pyarrow.fs.copy_files(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/pyarrow/fs.py\", line 259, in copy_files\n",
      "    _copy_files_selector(source_fs, source_sel,\n",
      "  File \"pyarrow/_fs.pyx\", line 1624, in pyarrow._fs._copy_files_selector\n",
      "  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\n",
      "FileNotFoundError: [Errno 2] Failed to open local file '/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_301534e7_3_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=13,data_transform=None,_2025-02-12_10-27-42/epoch=3-val_rmse=18.77.ckpt'. Detail: [errno 2] No such file or directory\n",
      "2025-02-12 10:28:18,213\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.955 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:28:18,216\tWARNING util.py:201 -- The `process_trial_result` operation took 1.958 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:28:18,216\tWARNING util.py:201 -- Processing trial results took 1.959 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:28:18,217\tWARNING util.py:201 -- The `process_trial_result` operation took 1.960 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3413386)\u001b[0m Seed set to 12\n",
      "\u001b[36m(train_nn_reg pid=3412262)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_301534e7_3_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=13,data_transform=None,_2025-02-12_10-27-42/checkpoint_000007)\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3413386)\u001b[0m Train: (156, 93), Test: (65, 93)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3413386)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3413386)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3413386)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3413386)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_585afc1b_7_batch_size=64,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0000,data_transfo_2025-02-12_10-28-10 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3413386)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3413386)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3413386)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3413386)\u001b[0m 0 | layers | ModuleList | 75.5 K | train\n",
      "\u001b[36m(train_nn_reg pid=3413386)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3413386)\u001b[0m 75.5 K    Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3413386)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3413386)\u001b[0m 75.5 K    Total params\n",
      "\u001b[36m(train_nn_reg pid=3413386)\u001b[0m 0.302     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3413386)\u001b[0m 24        Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3413386)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3413386)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3413386)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-02-12 10:28:21,309\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.730 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:28:21,311\tWARNING util.py:201 -- The `process_trial_result` operation took 1.732 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:28:21,312\tWARNING util.py:201 -- Processing trial results took 1.733 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:28:21,312\tWARNING util.py:201 -- The `process_trial_result` operation took 1.733 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:28:21,340\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_585afc1b\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3413386, ip=172.31.181.85, actor_id=956254b057d02d5be25f4ca901000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1024, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1053, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 1) cannot be concatenated\n",
      "2025-02-12 10:28:23,030\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.424 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:28:23,033\tWARNING util.py:201 -- The `process_trial_result` operation took 1.427 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:28:23,034\tWARNING util.py:201 -- Processing trial results took 1.429 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:28:23,035\tWARNING util.py:201 -- The `process_trial_result` operation took 1.429 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:28:24,690\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.653 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:28:24,692\tWARNING util.py:201 -- The `process_trial_result` operation took 1.655 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:28:24,692\tWARNING util.py:201 -- Processing trial results took 1.655 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:28:24,693\tWARNING util.py:201 -- The `process_trial_result` operation took 1.656 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:28:24,934\tERROR experiment_state.py:173 -- Saving experiment state to storage at '/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg' failed with exception: \n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/execution/experiment_state.py\", line 171, in wait_for_sync\n",
      "    self._storage.syncer.wait()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 474, in wait\n",
      "    raise e\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 472, in wait\n",
      "    self._sync_process.wait(timeout=timeout or self.sync_timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 173, in wait\n",
      "    raise exception\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 136, in entrypoint\n",
      "    result = self._fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/storage.py\", line 218, in _upload_to_fs_path\n",
      "    _pyarrow_fs_copy_files(local_path, fs_path, destination_filesystem=fs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/storage.py\", line 116, in _pyarrow_fs_copy_files\n",
      "    return pyarrow.fs.copy_files(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/pyarrow/fs.py\", line 259, in copy_files\n",
      "    _copy_files_selector(source_fs, source_sel,\n",
      "  File \"pyarrow/_fs.pyx\", line 1624, in pyarrow._fs._copy_files_selector\n",
      "  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\n",
      "FileNotFoundError: [Errno 2] Failed to open local file '/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_301534e7_3_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=13,data_transform=None,_2025-02-12_10-27-42/epoch=5-val_rmse=18.76.ckpt'. Detail: [errno 2] No such file or directory\n",
      "\u001b[36m(train_nn_reg pid=3412262)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_301534e7_3_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=13,data_transform=None,_2025-02-12_10-27-42/checkpoint_000009)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2025-02-12 10:28:27,366\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.759 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:28:27,369\tWARNING util.py:201 -- The `process_trial_result` operation took 1.761 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:28:27,369\tWARNING util.py:201 -- Processing trial results took 1.762 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:28:27,370\tWARNING util.py:201 -- The `process_trial_result` operation took 1.763 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:28:28,458\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.087 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:28:28,461\tWARNING util.py:201 -- The `process_trial_result` operation took 1.089 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:28:28,462\tWARNING util.py:201 -- Processing trial results took 1.091 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:28:28,463\tWARNING util.py:201 -- The `process_trial_result` operation took 1.092 s, which may be a performance bottleneck.\n",
      "/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/optuna/distributions.py:708: UserWarning: The distribution is specified by [1, 15] and step=5, but the range is not divisible by `step`. It will be replaced by [1, 11].\n",
      "  warnings.warn(\n",
      "\u001b[36m(train_nn_reg pid=3413652)\u001b[0m Seed set to 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3413652)\u001b[0m Train: (156, 42), Test: (65, 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3413652)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3413652)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3413652)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3413652)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_e26bf114_8_batch_size=128,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0018,data_transf_2025-02-12_10-28-19 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3413652)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3413652)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3413652)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3413652)\u001b[0m 0 | layers | ModuleList | 158 K  | train\n",
      "\u001b[36m(train_nn_reg pid=3413652)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3413652)\u001b[0m 158 K     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3413652)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3413652)\u001b[0m 158 K     Total params\n",
      "\u001b[36m(train_nn_reg pid=3413652)\u001b[0m 0.632     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3413652)\u001b[0m 14        Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3413652)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3413652)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(train_nn_reg pid=3413652)\u001b[0m Metric train_rmse does not exist in `trainer.callback_metrics.\n",
      "\u001b[36m(train_nn_reg pid=3413652)\u001b[0m Metric train_r2 does not exist in `trainer.callback_metrics.\n",
      "\u001b[36m(train_nn_reg pid=3413652)\u001b[0m Metric train_loss does not exist in `trainer.callback_metrics.\n",
      "2025-02-12 10:28:31,105\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.230 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:28:31,107\tWARNING util.py:201 -- The `process_trial_result` operation took 1.233 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:28:31,108\tWARNING util.py:201 -- Processing trial results took 1.234 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:28:31,108\tWARNING util.py:201 -- The `process_trial_result` operation took 1.234 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:28:32,164\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.051 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:28:32,166\tWARNING util.py:201 -- The `process_trial_result` operation took 1.054 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:28:32,166\tWARNING util.py:201 -- Processing trial results took 1.054 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:28:32,167\tWARNING util.py:201 -- The `process_trial_result` operation took 1.055 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:28:32,187\tERROR experiment_state.py:173 -- Saving experiment state to storage at '/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg' failed with exception: \n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/execution/experiment_state.py\", line 171, in wait_for_sync\n",
      "    self._storage.syncer.wait()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 474, in wait\n",
      "    raise e\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 472, in wait\n",
      "    self._sync_process.wait(timeout=timeout or self.sync_timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 173, in wait\n",
      "    raise exception\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 136, in entrypoint\n",
      "    result = self._fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/storage.py\", line 218, in _upload_to_fs_path\n",
      "    _pyarrow_fs_copy_files(local_path, fs_path, destination_filesystem=fs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/storage.py\", line 116, in _pyarrow_fs_copy_files\n",
      "    return pyarrow.fs.copy_files(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/pyarrow/fs.py\", line 259, in copy_files\n",
      "    _copy_files_selector(source_fs, source_sel,\n",
      "  File \"pyarrow/_fs.pyx\", line 1624, in pyarrow._fs._copy_files_selector\n",
      "  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\n",
      "FileNotFoundError: [Errno 2] Failed to open local file '/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_301534e7_3_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=13,data_transform=None,_2025-02-12_10-27-42/epoch=7-val_rmse=18.72.ckpt'. Detail: [errno 2] No such file or directory\n",
      "\u001b[36m(train_nn_reg pid=3412418)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_b9035250_4_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0002,data_transf_2025-02-12_10-27-48/checkpoint_000008)\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "2025-02-12 10:28:34,436\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.245 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:28:34,438\tWARNING util.py:201 -- The `process_trial_result` operation took 2.248 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:28:34,439\tWARNING util.py:201 -- Processing trial results took 2.248 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:28:34,439\tWARNING util.py:201 -- The `process_trial_result` operation took 2.249 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:28:35,550\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.107 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:28:35,553\tWARNING util.py:201 -- The `process_trial_result` operation took 1.110 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:28:35,553\tWARNING util.py:201 -- Processing trial results took 1.111 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:28:35,554\tWARNING util.py:201 -- The `process_trial_result` operation took 1.111 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:28:36,968\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.389 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:28:36,970\tWARNING util.py:201 -- The `process_trial_result` operation took 1.392 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:28:36,971\tWARNING util.py:201 -- Processing trial results took 1.393 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:28:36,971\tWARNING util.py:201 -- The `process_trial_result` operation took 1.393 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3414139)\u001b[0m Seed set to 12\n",
      "\u001b[36m(train_nn_reg pid=3412262)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_301534e7_3_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=13,data_transform=None,_2025-02-12_10-27-42/checkpoint_000011)\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3414139)\u001b[0m Train: (156, 3039), Test: (65, 3039)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3414139)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3414139)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3414139)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3414139)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_8358dac8_9_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=16,data_transform=None,_2025-02-12_10-28-29 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3414139)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3414139)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3414139)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3414139)\u001b[0m 0 | layers | ModuleList | 1.9 M  | train\n",
      "\u001b[36m(train_nn_reg pid=3414139)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3414139)\u001b[0m 1.9 M     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3414139)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3414139)\u001b[0m 1.9 M     Total params\n",
      "\u001b[36m(train_nn_reg pid=3414139)\u001b[0m 7.502     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3414139)\u001b[0m 14        Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3414139)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3414139)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(train_nn_reg pid=3414139)\u001b[0m Metric train_rmse does not exist in `trainer.callback_metrics.\n",
      "\u001b[36m(train_nn_reg pid=3414139)\u001b[0m Metric train_r2 does not exist in `trainer.callback_metrics.\n",
      "\u001b[36m(train_nn_reg pid=3414139)\u001b[0m Metric train_loss does not exist in `trainer.callback_metrics.\n",
      "2025-02-12 10:28:40,461\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.977 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:28:40,463\tWARNING util.py:201 -- The `process_trial_result` operation took 1.979 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:28:40,464\tWARNING util.py:201 -- Processing trial results took 1.980 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:28:40,464\tWARNING util.py:201 -- The `process_trial_result` operation took 1.980 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:28:42,030\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.545 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:28:42,032\tWARNING util.py:201 -- The `process_trial_result` operation took 1.547 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:28:42,033\tWARNING util.py:201 -- Processing trial results took 1.549 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:28:42,034\tWARNING util.py:201 -- The `process_trial_result` operation took 1.549 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:28:43,821\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.776 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:28:43,823\tWARNING util.py:201 -- The `process_trial_result` operation took 1.778 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:28:43,824\tWARNING util.py:201 -- Processing trial results took 1.779 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:28:43,825\tWARNING util.py:201 -- The `process_trial_result` operation took 1.780 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:28:45,447\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.610 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:28:45,449\tWARNING util.py:201 -- The `process_trial_result` operation took 1.613 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:28:45,450\tWARNING util.py:201 -- Processing trial results took 1.614 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:28:45,450\tWARNING util.py:201 -- The `process_trial_result` operation took 1.614 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3412418)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_b9035250_4_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0002,data_transf_2025-02-12_10-27-48/checkpoint_000009)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_nn_reg pid=3414455)\u001b[0m Seed set to 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3414455)\u001b[0m Train: (156, 41), Test: (65, 41)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3414455)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3414455)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3414455)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3414455)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_3c4bd4b6_10_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0025,data_trans_2025-02-12_10-28-38 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3414455)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3414455)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3414455)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3414455)\u001b[0m 0 | layers | ModuleList | 442 K  | train\n",
      "\u001b[36m(train_nn_reg pid=3414455)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3414455)\u001b[0m 442 K     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3414455)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3414455)\u001b[0m 442 K     Total params\n",
      "\u001b[36m(train_nn_reg pid=3414455)\u001b[0m 1.771     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3414455)\u001b[0m 34        Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3414455)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3414455)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(train_nn_reg pid=3414455)\u001b[0m Metric train_rmse does not exist in `trainer.callback_metrics.\n",
      "\u001b[36m(train_nn_reg pid=3414455)\u001b[0m Metric train_r2 does not exist in `trainer.callback_metrics.\n",
      "\u001b[36m(train_nn_reg pid=3414455)\u001b[0m Metric train_loss does not exist in `trainer.callback_metrics.\n",
      "2025-02-12 10:28:49,419\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.538 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:28:49,421\tWARNING util.py:201 -- The `process_trial_result` operation took 1.540 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:28:49,422\tWARNING util.py:201 -- Processing trial results took 1.541 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:28:49,422\tWARNING util.py:201 -- The `process_trial_result` operation took 1.541 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:28:51,582\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.158 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:28:51,585\tWARNING util.py:201 -- The `process_trial_result` operation took 2.161 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:28:51,586\tWARNING util.py:201 -- Processing trial results took 2.162 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:28:51,586\tWARNING util.py:201 -- The `process_trial_result` operation took 2.163 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:28:52,747\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.159 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:28:52,749\tWARNING util.py:201 -- The `process_trial_result` operation took 1.161 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:28:52,750\tWARNING util.py:201 -- Processing trial results took 1.162 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:28:52,750\tWARNING util.py:201 -- The `process_trial_result` operation took 1.162 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:28:53,870\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.118 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:28:53,873\tWARNING util.py:201 -- The `process_trial_result` operation took 1.121 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:28:53,874\tWARNING util.py:201 -- Processing trial results took 1.121 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:28:53,874\tWARNING util.py:201 -- The `process_trial_result` operation took 1.122 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3412418)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_b9035250_4_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0002,data_transf_2025-02-12_10-27-48/checkpoint_000010)\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "2025-02-12 10:28:56,403\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.486 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:28:56,405\tWARNING util.py:201 -- The `process_trial_result` operation took 2.488 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:28:56,406\tWARNING util.py:201 -- Processing trial results took 2.489 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:28:56,406\tWARNING util.py:201 -- The `process_trial_result` operation took 2.489 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:28:57,990\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.569 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:28:57,993\tWARNING util.py:201 -- The `process_trial_result` operation took 1.572 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:28:57,993\tWARNING util.py:201 -- Processing trial results took 1.572 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:28:57,994\tWARNING util.py:201 -- The `process_trial_result` operation took 1.573 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:00,043\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.035 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:00,045\tWARNING util.py:201 -- The `process_trial_result` operation took 2.038 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:00,045\tWARNING util.py:201 -- Processing trial results took 2.038 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:29:00,046\tWARNING util.py:201 -- The `process_trial_result` operation took 2.038 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:01,857\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.801 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:01,859\tWARNING util.py:201 -- The `process_trial_result` operation took 1.803 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:01,859\tWARNING util.py:201 -- Processing trial results took 1.804 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:29:01,860\tWARNING util.py:201 -- The `process_trial_result` operation took 1.804 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:03,775\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.914 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:03,777\tWARNING util.py:201 -- The `process_trial_result` operation took 1.916 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:03,778\tWARNING util.py:201 -- Processing trial results took 1.917 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:29:03,779\tWARNING util.py:201 -- The `process_trial_result` operation took 1.918 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3412418)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_b9035250_4_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0002,data_transf_2025-02-12_10-27-48/checkpoint_000011)\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "2025-02-12 10:29:05,506\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.706 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:05,508\tWARNING util.py:201 -- The `process_trial_result` operation took 1.709 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:05,509\tWARNING util.py:201 -- Processing trial results took 1.710 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:29:05,509\tWARNING util.py:201 -- The `process_trial_result` operation took 1.710 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:05,522\tWARNING syncer.py:406 -- Last sync command failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 404, in _launch_sync_process\n",
      "    self.wait()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 474, in wait\n",
      "    raise e\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 472, in wait\n",
      "    self._sync_process.wait(timeout=timeout or self.sync_timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 173, in wait\n",
      "    raise exception\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 136, in entrypoint\n",
      "    result = self._fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/storage.py\", line 218, in _upload_to_fs_path\n",
      "    _pyarrow_fs_copy_files(local_path, fs_path, destination_filesystem=fs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/storage.py\", line 116, in _pyarrow_fs_copy_files\n",
      "    return pyarrow.fs.copy_files(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/pyarrow/fs.py\", line 259, in copy_files\n",
      "    _copy_files_selector(source_fs, source_sel,\n",
      "  File \"pyarrow/_fs.pyx\", line 1624, in pyarrow._fs._copy_files_selector\n",
      "  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\n",
      "FileNotFoundError: [Errno 2] Failed to open local file '/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_b9035250_4_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0002,data_transf_2025-02-12_10-27-48/epoch=8-val_rmse=18.16.ckpt'. Detail: [errno 2] No such file or directory\n",
      "\n",
      "2025-02-12 10:29:07,907\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.374 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:07,909\tWARNING util.py:201 -- The `process_trial_result` operation took 2.376 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:07,910\tWARNING util.py:201 -- Processing trial results took 2.377 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:29:07,911\tWARNING util.py:201 -- The `process_trial_result` operation took 2.378 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:09,291\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.378 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:09,293\tWARNING util.py:201 -- The `process_trial_result` operation took 1.380 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:09,293\tWARNING util.py:201 -- Processing trial results took 1.381 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:29:09,294\tWARNING util.py:201 -- The `process_trial_result` operation took 1.382 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3414455)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_3c4bd4b6_10_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0025,data_trans_2025-02-12_10-28-38/checkpoint_000002)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2025-02-12 10:29:10,551\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.254 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:10,553\tWARNING util.py:201 -- The `process_trial_result` operation took 1.257 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:10,554\tWARNING util.py:201 -- Processing trial results took 1.257 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:29:10,554\tWARNING util.py:201 -- The `process_trial_result` operation took 1.258 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:12,422\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.858 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:12,424\tWARNING util.py:201 -- The `process_trial_result` operation took 1.861 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:12,425\tWARNING util.py:201 -- Processing trial results took 1.861 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:29:12,425\tWARNING util.py:201 -- The `process_trial_result` operation took 1.862 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:15,183\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.148 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:15,185\tWARNING util.py:201 -- The `process_trial_result` operation took 2.151 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:15,186\tWARNING util.py:201 -- Processing trial results took 2.151 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:29:15,187\tWARNING util.py:201 -- The `process_trial_result` operation took 2.152 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:17,049\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.861 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:17,051\tWARNING util.py:201 -- The `process_trial_result` operation took 1.863 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:17,052\tWARNING util.py:201 -- Processing trial results took 1.864 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:29:17,052\tWARNING util.py:201 -- The `process_trial_result` operation took 1.864 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3412418)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_b9035250_4_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0002,data_transf_2025-02-12_10-27-48/checkpoint_000013)\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "2025-02-12 10:29:18,913\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.823 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:18,915\tWARNING util.py:201 -- The `process_trial_result` operation took 1.825 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:18,915\tWARNING util.py:201 -- Processing trial results took 1.826 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:29:18,916\tWARNING util.py:201 -- The `process_trial_result` operation took 1.826 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:20,201\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.283 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:20,203\tWARNING util.py:201 -- The `process_trial_result` operation took 1.285 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:20,204\tWARNING util.py:201 -- Processing trial results took 1.286 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:29:20,204\tWARNING util.py:201 -- The `process_trial_result` operation took 1.286 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:21,924\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.719 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:21,927\tWARNING util.py:201 -- The `process_trial_result` operation took 1.722 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:21,928\tWARNING util.py:201 -- Processing trial results took 1.723 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:29:21,928\tWARNING util.py:201 -- The `process_trial_result` operation took 1.723 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3414139)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_8358dac8_9_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=16,data_transform=None,_2025-02-12_10-28-29/checkpoint_000005)\n",
      "2025-02-12 10:29:23,337\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.393 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:23,340\tWARNING util.py:201 -- The `process_trial_result` operation took 1.396 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:23,340\tWARNING util.py:201 -- Processing trial results took 1.397 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:29:23,341\tWARNING util.py:201 -- The `process_trial_result` operation took 1.397 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3413652)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_e26bf114_8_batch_size=128,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0018,data_transf_2025-02-12_10-28-19/checkpoint_000006)\n",
      "2025-02-12 10:29:25,039\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.669 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:25,042\tWARNING util.py:201 -- The `process_trial_result` operation took 1.672 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:25,043\tWARNING util.py:201 -- Processing trial results took 1.673 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:29:25,043\tWARNING util.py:201 -- The `process_trial_result` operation took 1.673 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:26,198\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.152 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:26,200\tWARNING util.py:201 -- The `process_trial_result` operation took 1.155 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:26,201\tWARNING util.py:201 -- Processing trial results took 1.155 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:29:26,201\tWARNING util.py:201 -- The `process_trial_result` operation took 1.156 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:28,179\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.803 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:28,181\tWARNING util.py:201 -- The `process_trial_result` operation took 1.806 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:28,181\tWARNING util.py:201 -- Processing trial results took 1.806 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:29:28,182\tWARNING util.py:201 -- The `process_trial_result` operation took 1.806 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:28,195\tWARNING syncer.py:406 -- Last sync command failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 404, in _launch_sync_process\n",
      "    self.wait()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 474, in wait\n",
      "    raise e\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 472, in wait\n",
      "    self._sync_process.wait(timeout=timeout or self.sync_timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 173, in wait\n",
      "    raise exception\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 136, in entrypoint\n",
      "    result = self._fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/storage.py\", line 218, in _upload_to_fs_path\n",
      "    _pyarrow_fs_copy_files(local_path, fs_path, destination_filesystem=fs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/storage.py\", line 116, in _pyarrow_fs_copy_files\n",
      "    return pyarrow.fs.copy_files(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/pyarrow/fs.py\", line 259, in copy_files\n",
      "    _copy_files_selector(source_fs, source_sel,\n",
      "  File \"pyarrow/_fs.pyx\", line 1624, in pyarrow._fs._copy_files_selector\n",
      "  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\n",
      "FileNotFoundError: [Errno 2] Failed to open local file '/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_b9035250_4_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0002,data_transf_2025-02-12_10-27-48/epoch=11-val_rmse=18.03.ckpt'. Detail: [errno 2] No such file or directory\n",
      "\n",
      "\u001b[36m(train_nn_reg pid=3413652)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_e26bf114_8_batch_size=128,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0018,data_transf_2025-02-12_10-28-19/checkpoint_000007)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "2025-02-12 10:29:30,796\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.519 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:30,798\tWARNING util.py:201 -- The `process_trial_result` operation took 2.521 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:30,799\tWARNING util.py:201 -- Processing trial results took 2.522 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:29:30,800\tWARNING util.py:201 -- The `process_trial_result` operation took 2.523 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:32,863\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.028 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:32,865\tWARNING util.py:201 -- The `process_trial_result` operation took 2.031 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:32,866\tWARNING util.py:201 -- Processing trial results took 2.031 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:29:32,866\tWARNING util.py:201 -- The `process_trial_result` operation took 2.031 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:34,008\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.141 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:34,011\tWARNING util.py:201 -- The `process_trial_result` operation took 1.144 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:34,012\tWARNING util.py:201 -- Processing trial results took 1.144 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:29:34,012\tWARNING util.py:201 -- The `process_trial_result` operation took 1.145 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3412262)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_301534e7_3_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=13,data_transform=None,_2025-02-12_10-27-42/checkpoint_000019)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "2025-02-12 10:29:36,428\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.401 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:36,430\tWARNING util.py:201 -- The `process_trial_result` operation took 2.403 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:36,431\tWARNING util.py:201 -- Processing trial results took 2.404 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:29:36,432\tWARNING util.py:201 -- The `process_trial_result` operation took 2.405 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:37,743\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.301 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:37,745\tWARNING util.py:201 -- The `process_trial_result` operation took 1.303 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:37,746\tWARNING util.py:201 -- Processing trial results took 1.304 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:29:37,746\tWARNING util.py:201 -- The `process_trial_result` operation took 1.304 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:39,335\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.552 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:39,340\tWARNING util.py:201 -- The `process_trial_result` operation took 1.557 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:39,341\tWARNING util.py:201 -- Processing trial results took 1.558 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:29:39,341\tWARNING util.py:201 -- The `process_trial_result` operation took 1.559 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:39,354\tWARNING syncer.py:406 -- Last sync command failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 404, in _launch_sync_process\n",
      "    self.wait()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 474, in wait\n",
      "    raise e\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 472, in wait\n",
      "    self._sync_process.wait(timeout=timeout or self.sync_timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 173, in wait\n",
      "    raise exception\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 136, in entrypoint\n",
      "    result = self._fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/storage.py\", line 218, in _upload_to_fs_path\n",
      "    _pyarrow_fs_copy_files(local_path, fs_path, destination_filesystem=fs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/storage.py\", line 116, in _pyarrow_fs_copy_files\n",
      "    return pyarrow.fs.copy_files(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/pyarrow/fs.py\", line 259, in copy_files\n",
      "    _copy_files_selector(source_fs, source_sel,\n",
      "  File \"pyarrow/_fs.pyx\", line 1624, in pyarrow._fs._copy_files_selector\n",
      "  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\n",
      "FileNotFoundError: [Errno 2] Failed to open local file '/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_301534e7_3_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=13,data_transform=None,_2025-02-12_10-27-42/epoch=16-val_rmse=16.35.ckpt'. Detail: [errno 2] No such file or directory\n",
      "\n",
      "\u001b[36m(train_nn_reg pid=3412418)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_b9035250_4_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0002,data_transf_2025-02-12_10-27-48/checkpoint_000015)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "2025-02-12 10:29:42,835\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 3.399 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:42,837\tWARNING util.py:201 -- The `process_trial_result` operation took 3.401 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:42,838\tWARNING util.py:201 -- Processing trial results took 3.402 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:29:42,838\tWARNING util.py:201 -- The `process_trial_result` operation took 3.402 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:44,128\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.279 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:44,130\tWARNING util.py:201 -- The `process_trial_result` operation took 1.281 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:44,131\tWARNING util.py:201 -- Processing trial results took 1.282 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:29:44,132\tWARNING util.py:201 -- The `process_trial_result` operation took 1.283 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:46,425\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.279 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:46,427\tWARNING util.py:201 -- The `process_trial_result` operation took 2.282 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:46,428\tWARNING util.py:201 -- Processing trial results took 2.282 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:29:46,428\tWARNING util.py:201 -- The `process_trial_result` operation took 2.283 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:47,561\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.131 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:47,563\tWARNING util.py:201 -- The `process_trial_result` operation took 1.133 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:47,564\tWARNING util.py:201 -- Processing trial results took 1.134 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:29:47,564\tWARNING util.py:201 -- The `process_trial_result` operation took 1.134 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3414139)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_8358dac8_9_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=16,data_transform=None,_2025-02-12_10-28-29/checkpoint_000008)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "2025-02-12 10:29:49,053\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.463 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:49,055\tWARNING util.py:201 -- The `process_trial_result` operation took 1.466 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:49,055\tWARNING util.py:201 -- Processing trial results took 1.466 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:29:49,056\tWARNING util.py:201 -- The `process_trial_result` operation took 1.466 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:50,493\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.414 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:50,494\tWARNING util.py:201 -- The `process_trial_result` operation took 1.416 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:50,495\tWARNING util.py:201 -- Processing trial results took 1.416 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:29:50,495\tWARNING util.py:201 -- The `process_trial_result` operation took 1.416 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:50,508\tWARNING syncer.py:406 -- Last sync command failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 404, in _launch_sync_process\n",
      "    self.wait()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 474, in wait\n",
      "    raise e\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 472, in wait\n",
      "    self._sync_process.wait(timeout=timeout or self.sync_timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 173, in wait\n",
      "    raise exception\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 136, in entrypoint\n",
      "    result = self._fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/storage.py\", line 218, in _upload_to_fs_path\n",
      "    _pyarrow_fs_copy_files(local_path, fs_path, destination_filesystem=fs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/storage.py\", line 116, in _pyarrow_fs_copy_files\n",
      "    return pyarrow.fs.copy_files(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/pyarrow/fs.py\", line 259, in copy_files\n",
      "    _copy_files_selector(source_fs, source_sel,\n",
      "  File \"pyarrow/_fs.pyx\", line 1624, in pyarrow._fs._copy_files_selector\n",
      "  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\n",
      "FileNotFoundError: [Errno 2] Failed to open local file '/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_b9035250_4_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0002,data_transf_2025-02-12_10-27-48/epoch=13-val_rmse=17.93.ckpt'. Detail: [errno 2] No such file or directory\n",
      "\n",
      "2025-02-12 10:29:53,319\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.809 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:53,321\tWARNING util.py:201 -- The `process_trial_result` operation took 2.811 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:53,322\tWARNING util.py:201 -- Processing trial results took 2.812 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:29:53,322\tWARNING util.py:201 -- The `process_trial_result` operation took 2.812 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3412418)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_b9035250_4_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0002,data_transf_2025-02-12_10-27-48/checkpoint_000016)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "2025-02-12 10:29:55,839\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.465 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:55,841\tWARNING util.py:201 -- The `process_trial_result` operation took 2.467 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:55,842\tWARNING util.py:201 -- Processing trial results took 2.468 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:29:55,842\tWARNING util.py:201 -- The `process_trial_result` operation took 2.469 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:57,439\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.586 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:57,441\tWARNING util.py:201 -- The `process_trial_result` operation took 1.589 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:57,442\tWARNING util.py:201 -- Processing trial results took 1.589 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:29:57,443\tWARNING util.py:201 -- The `process_trial_result` operation took 1.590 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:58,733\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.288 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:58,735\tWARNING util.py:201 -- The `process_trial_result` operation took 1.291 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:29:58,736\tWARNING util.py:201 -- Processing trial results took 1.292 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:29:58,736\tWARNING util.py:201 -- The `process_trial_result` operation took 1.292 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3412418)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_b9035250_4_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0002,data_transf_2025-02-12_10-27-48/checkpoint_000017)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "2025-02-12 10:30:00,683\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.404 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:30:00,685\tWARNING util.py:201 -- The `process_trial_result` operation took 1.406 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:30:00,686\tWARNING util.py:201 -- Processing trial results took 1.406 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:30:00,686\tWARNING util.py:201 -- The `process_trial_result` operation took 1.407 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:30:02,597\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.882 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:30:02,600\tWARNING util.py:201 -- The `process_trial_result` operation took 1.885 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:30:02,600\tWARNING util.py:201 -- Processing trial results took 1.885 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:30:02,600\tWARNING util.py:201 -- The `process_trial_result` operation took 1.886 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:30:03,752\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.141 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:30:03,754\tWARNING util.py:201 -- The `process_trial_result` operation took 1.143 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:30:03,754\tWARNING util.py:201 -- Processing trial results took 1.144 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:30:03,755\tWARNING util.py:201 -- The `process_trial_result` operation took 1.145 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:30:07,358\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 3.600 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:30:07,360\tWARNING util.py:201 -- The `process_trial_result` operation took 3.602 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:30:07,361\tWARNING util.py:201 -- Processing trial results took 3.603 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:30:07,361\tWARNING util.py:201 -- The `process_trial_result` operation took 3.603 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3417235)\u001b[0m Seed set to 12\n",
      "\u001b[36m(train_nn_reg pid=3412418)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_b9035250_4_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0002,data_transf_2025-02-12_10-27-48/checkpoint_000018)\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3417235)\u001b[0m Train: (156, 39), Test: (65, 39)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3417235)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3417235)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3417235)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3417235)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_4450d409_11_batch_size=128,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0031,data_trans_2025-02-12_10-29-59 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3417235)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3417235)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3417235)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3417235)\u001b[0m 0 | layers | ModuleList | 1.5 M  | train\n",
      "\u001b[36m(train_nn_reg pid=3417235)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3417235)\u001b[0m 1.5 M     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3417235)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3417235)\u001b[0m 1.5 M     Total params\n",
      "\u001b[36m(train_nn_reg pid=3417235)\u001b[0m 6.023     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3417235)\u001b[0m 44        Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3417235)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3417235)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(train_nn_reg pid=3417235)\u001b[0m Metric train_rmse does not exist in `trainer.callback_metrics.\n",
      "\u001b[36m(train_nn_reg pid=3417235)\u001b[0m Metric train_r2 does not exist in `trainer.callback_metrics.\n",
      "\u001b[36m(train_nn_reg pid=3417235)\u001b[0m Metric train_loss does not exist in `trainer.callback_metrics.\n",
      "2025-02-12 10:30:11,648\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.369 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:30:11,650\tWARNING util.py:201 -- The `process_trial_result` operation took 2.371 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:30:11,651\tWARNING util.py:201 -- Processing trial results took 2.372 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:30:11,651\tWARNING util.py:201 -- The `process_trial_result` operation took 2.372 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:30:11,664\tWARNING syncer.py:406 -- Last sync command failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 404, in _launch_sync_process\n",
      "    self.wait()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 474, in wait\n",
      "    raise e\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 472, in wait\n",
      "    self._sync_process.wait(timeout=timeout or self.sync_timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 173, in wait\n",
      "    raise exception\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 136, in entrypoint\n",
      "    result = self._fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/storage.py\", line 218, in _upload_to_fs_path\n",
      "    _pyarrow_fs_copy_files(local_path, fs_path, destination_filesystem=fs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/storage.py\", line 116, in _pyarrow_fs_copy_files\n",
      "    return pyarrow.fs.copy_files(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/pyarrow/fs.py\", line 259, in copy_files\n",
      "    _copy_files_selector(source_fs, source_sel,\n",
      "  File \"pyarrow/_fs.pyx\", line 1624, in pyarrow._fs._copy_files_selector\n",
      "  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\n",
      "FileNotFoundError: [Errno 2] Failed to open local file '/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_b9035250_4_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0002,data_transf_2025-02-12_10-27-48/epoch=16-val_rmse=17.76.ckpt'. Detail: [errno 2] No such file or directory\n",
      "\n",
      "2025-02-12 10:30:14,291\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.585 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:30:14,294\tWARNING util.py:201 -- The `process_trial_result` operation took 2.587 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:30:14,295\tWARNING util.py:201 -- Processing trial results took 2.588 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:30:14,295\tWARNING util.py:201 -- The `process_trial_result` operation took 2.589 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:30:16,115\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.809 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:30:16,117\tWARNING util.py:201 -- The `process_trial_result` operation took 1.811 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:30:16,118\tWARNING util.py:201 -- Processing trial results took 1.812 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:30:16,118\tWARNING util.py:201 -- The `process_trial_result` operation took 1.812 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3412418)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_b9035250_4_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0002,data_transf_2025-02-12_10-27-48/checkpoint_000019)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "2025-02-12 10:30:19,220\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 3.071 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:30:19,223\tWARNING util.py:201 -- The `process_trial_result` operation took 3.073 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:30:19,223\tWARNING util.py:201 -- Processing trial results took 3.074 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:30:19,224\tWARNING util.py:201 -- The `process_trial_result` operation took 3.075 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:30:20,885\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.659 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:30:20,887\tWARNING util.py:201 -- The `process_trial_result` operation took 1.662 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:30:20,888\tWARNING util.py:201 -- Processing trial results took 1.662 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:30:20,888\tWARNING util.py:201 -- The `process_trial_result` operation took 1.663 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:30:22,142\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.232 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:30:22,145\tWARNING util.py:201 -- The `process_trial_result` operation took 1.235 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:30:22,145\tWARNING util.py:201 -- Processing trial results took 1.235 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:30:22,145\tWARNING util.py:201 -- The `process_trial_result` operation took 1.236 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:30:22,158\tWARNING syncer.py:406 -- Last sync command failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 404, in _launch_sync_process\n",
      "    self.wait()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 474, in wait\n",
      "    raise e\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 472, in wait\n",
      "    self._sync_process.wait(timeout=timeout or self.sync_timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 173, in wait\n",
      "    raise exception\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 136, in entrypoint\n",
      "    result = self._fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/storage.py\", line 218, in _upload_to_fs_path\n",
      "    _pyarrow_fs_copy_files(local_path, fs_path, destination_filesystem=fs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/storage.py\", line 116, in _pyarrow_fs_copy_files\n",
      "    return pyarrow.fs.copy_files(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/pyarrow/fs.py\", line 259, in copy_files\n",
      "    _copy_files_selector(source_fs, source_sel,\n",
      "  File \"pyarrow/_fs.pyx\", line 1624, in pyarrow._fs._copy_files_selector\n",
      "  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\n",
      "FileNotFoundError: [Errno 2] Failed to open local file '/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_3c4bd4b6_10_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0025,data_trans_2025-02-12_10-28-38/epoch=7-val_rmse=18.65.ckpt'. Detail: [errno 2] No such file or directory\n",
      "\n",
      "\u001b[36m(train_nn_reg pid=3412418)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_b9035250_4_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0002,data_transf_2025-02-12_10-27-48/checkpoint_000020)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2025-02-12 10:30:24,455\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.261 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:30:24,457\tWARNING util.py:201 -- The `process_trial_result` operation took 2.263 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:30:24,457\tWARNING util.py:201 -- Processing trial results took 2.264 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:30:24,458\tWARNING util.py:201 -- The `process_trial_result` operation took 2.265 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:30:26,143\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.683 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:30:26,145\tWARNING util.py:201 -- The `process_trial_result` operation took 1.686 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:30:26,146\tWARNING util.py:201 -- Processing trial results took 1.686 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:30:26,146\tWARNING util.py:201 -- The `process_trial_result` operation took 1.686 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:30:27,944\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.787 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:30:27,946\tWARNING util.py:201 -- The `process_trial_result` operation took 1.790 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:30:27,947\tWARNING util.py:201 -- Processing trial results took 1.790 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:30:27,947\tWARNING util.py:201 -- The `process_trial_result` operation took 1.791 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3412262)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_301534e7_3_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=13,data_transform=None,_2025-02-12_10-27-42/checkpoint_000025)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2025-02-12 10:30:29,323\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.361 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:30:29,325\tWARNING util.py:201 -- The `process_trial_result` operation took 1.364 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:30:29,326\tWARNING util.py:201 -- Processing trial results took 1.364 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:30:29,327\tWARNING util.py:201 -- The `process_trial_result` operation took 1.365 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:30:33,284\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 3.956 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:30:33,286\tWARNING util.py:201 -- The `process_trial_result` operation took 3.958 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:30:33,287\tWARNING util.py:201 -- Processing trial results took 3.959 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:30:33,288\tWARNING util.py:201 -- The `process_trial_result` operation took 3.960 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:30:33,300\tWARNING syncer.py:406 -- Last sync command failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 404, in _launch_sync_process\n",
      "    self.wait()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 474, in wait\n",
      "    raise e\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 472, in wait\n",
      "    self._sync_process.wait(timeout=timeout or self.sync_timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 173, in wait\n",
      "    raise exception\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 136, in entrypoint\n",
      "    result = self._fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/storage.py\", line 218, in _upload_to_fs_path\n",
      "    _pyarrow_fs_copy_files(local_path, fs_path, destination_filesystem=fs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/storage.py\", line 116, in _pyarrow_fs_copy_files\n",
      "    return pyarrow.fs.copy_files(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/pyarrow/fs.py\", line 259, in copy_files\n",
      "    _copy_files_selector(source_fs, source_sel,\n",
      "  File \"pyarrow/_fs.pyx\", line 1624, in pyarrow._fs._copy_files_selector\n",
      "  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\n",
      "FileNotFoundError: [Errno 2] Failed to open local file '/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_b9035250_4_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0002,data_transf_2025-02-12_10-27-48/epoch=18-val_rmse=17.64.ckpt'. Detail: [errno 2] No such file or directory\n",
      "\n",
      "\u001b[36m(train_nn_reg pid=3412418)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_b9035250_4_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0002,data_transf_2025-02-12_10-27-48/checkpoint_000021)\n",
      "\u001b[36m(train_nn_reg pid=3413652)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_e26bf114_8_batch_size=128,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0018,data_transf_2025-02-12_10-28-19/checkpoint_000011)\n",
      "2025-02-12 10:30:36,466\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.646 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:30:36,469\tWARNING util.py:201 -- The `process_trial_result` operation took 1.649 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:30:36,470\tWARNING util.py:201 -- Processing trial results took 1.650 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:30:36,470\tWARNING util.py:201 -- The `process_trial_result` operation took 1.650 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:30:38,740\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.255 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:30:38,742\tWARNING util.py:201 -- The `process_trial_result` operation took 2.256 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:30:38,742\tWARNING util.py:201 -- Processing trial results took 2.257 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:30:38,743\tWARNING util.py:201 -- The `process_trial_result` operation took 2.257 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3417235)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_4450d409_11_batch_size=128,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0031,data_trans_2025-02-12_10-29-59/checkpoint_000003)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "2025-02-12 10:30:40,663\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.908 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:30:40,665\tWARNING util.py:201 -- The `process_trial_result` operation took 1.911 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:30:40,665\tWARNING util.py:201 -- Processing trial results took 1.911 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:30:40,666\tWARNING util.py:201 -- The `process_trial_result` operation took 1.912 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:30:42,725\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.057 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:30:42,727\tWARNING util.py:201 -- The `process_trial_result` operation took 2.060 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:30:42,728\tWARNING util.py:201 -- Processing trial results took 2.060 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:30:42,728\tWARNING util.py:201 -- The `process_trial_result` operation took 2.061 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:30:44,988\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.259 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:30:44,991\tWARNING util.py:201 -- The `process_trial_result` operation took 2.261 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:30:44,991\tWARNING util.py:201 -- Processing trial results took 2.262 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:30:44,992\tWARNING util.py:201 -- The `process_trial_result` operation took 2.262 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:30:45,006\tWARNING syncer.py:406 -- Last sync command failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 404, in _launch_sync_process\n",
      "    self.wait()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 474, in wait\n",
      "    raise e\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 472, in wait\n",
      "    self._sync_process.wait(timeout=timeout or self.sync_timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 173, in wait\n",
      "    raise exception\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 136, in entrypoint\n",
      "    result = self._fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/storage.py\", line 218, in _upload_to_fs_path\n",
      "    _pyarrow_fs_copy_files(local_path, fs_path, destination_filesystem=fs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/storage.py\", line 116, in _pyarrow_fs_copy_files\n",
      "    return pyarrow.fs.copy_files(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/pyarrow/fs.py\", line 259, in copy_files\n",
      "    _copy_files_selector(source_fs, source_sel,\n",
      "  File \"pyarrow/_fs.pyx\", line 1624, in pyarrow._fs._copy_files_selector\n",
      "  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\n",
      "FileNotFoundError: [Errno 2] Failed to open local file '/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_b9035250_4_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0002,data_transf_2025-02-12_10-27-48/epoch=19-val_rmse=17.58.ckpt'. Detail: [errno 2] No such file or directory\n",
      "\n",
      "\u001b[36m(train_nn_reg pid=3412418)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_b9035250_4_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0002,data_transf_2025-02-12_10-27-48/checkpoint_000022)\n",
      "2025-02-12 10:30:47,096\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.059 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:30:47,098\tWARNING util.py:201 -- The `process_trial_result` operation took 2.061 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:30:47,098\tWARNING util.py:201 -- Processing trial results took 2.061 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:30:47,100\tWARNING util.py:201 -- The `process_trial_result` operation took 2.063 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3412262)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_301534e7_3_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=13,data_transform=None,_2025-02-12_10-27-42/checkpoint_000028)\n",
      "\u001b[36m(train_nn_reg pid=3418258)\u001b[0m Seed set to 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3418258)\u001b[0m Train: (156, 1895), Test: (65, 1895)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3418258)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3418258)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3418258)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3418258)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_96d75adb_12_batch_size=64,data_aggregation=None,data_selection=abundance_ith,data_selection_i=8,data_transform=None,e_2025-02-12_10-30-34 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3418258)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3418258)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3418258)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3418258)\u001b[0m 0 | layers | ModuleList | 1.6 M  | train\n",
      "\u001b[36m(train_nn_reg pid=3418258)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3418258)\u001b[0m 1.6 M     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3418258)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3418258)\u001b[0m 1.6 M     Total params\n",
      "\u001b[36m(train_nn_reg pid=3418258)\u001b[0m 6.282     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3418258)\u001b[0m 34        Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3418258)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3418258)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3418258)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-02-12 10:30:50,772\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.657 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:30:50,774\tWARNING util.py:201 -- The `process_trial_result` operation took 1.660 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:30:50,775\tWARNING util.py:201 -- Processing trial results took 1.660 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:30:50,775\tWARNING util.py:201 -- The `process_trial_result` operation took 1.660 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:30:50,787\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_96d75adb\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3418258, ip=172.31.181.85, actor_id=6c6a2f729e8f3036b33d006001000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1024, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1053, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 1) cannot be concatenated\n",
      "2025-02-12 10:30:52,802\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.837 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:30:52,804\tWARNING util.py:201 -- The `process_trial_result` operation took 1.839 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:30:52,805\tWARNING util.py:201 -- Processing trial results took 1.840 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:30:52,805\tWARNING util.py:201 -- The `process_trial_result` operation took 1.840 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3417235)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_4450d409_11_batch_size=128,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0031,data_trans_2025-02-12_10-29-59/checkpoint_000005)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "2025-02-12 10:30:54,727\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.823 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:30:54,729\tWARNING util.py:201 -- The `process_trial_result` operation took 1.826 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:30:54,730\tWARNING util.py:201 -- Processing trial results took 1.826 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:30:54,730\tWARNING util.py:201 -- The `process_trial_result` operation took 1.827 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:30:58,946\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 4.214 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:30:58,948\tWARNING util.py:201 -- The `process_trial_result` operation took 4.216 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:30:58,949\tWARNING util.py:201 -- Processing trial results took 4.217 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:30:58,950\tWARNING util.py:201 -- The `process_trial_result` operation took 4.218 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:30:58,964\tWARNING syncer.py:406 -- Last sync command failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 404, in _launch_sync_process\n",
      "    self.wait()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 474, in wait\n",
      "    raise e\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 472, in wait\n",
      "    self._sync_process.wait(timeout=timeout or self.sync_timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 173, in wait\n",
      "    raise exception\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 136, in entrypoint\n",
      "    result = self._fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/storage.py\", line 218, in _upload_to_fs_path\n",
      "    _pyarrow_fs_copy_files(local_path, fs_path, destination_filesystem=fs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/storage.py\", line 116, in _pyarrow_fs_copy_files\n",
      "    return pyarrow.fs.copy_files(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/pyarrow/fs.py\", line 259, in copy_files\n",
      "    _copy_files_selector(source_fs, source_sel,\n",
      "  File \"pyarrow/_fs.pyx\", line 1624, in pyarrow._fs._copy_files_selector\n",
      "  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\n",
      "FileNotFoundError: [Errno 2] Failed to open local file '/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_b9035250_4_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0002,data_transf_2025-02-12_10-27-48/epoch=20-val_rmse=17.52.ckpt'. Detail: [errno 2] No such file or directory\n",
      "\n",
      "\u001b[36m(train_nn_reg pid=3418637)\u001b[0m Seed set to 12\n",
      "\u001b[36m(train_nn_reg pid=3412262)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_301534e7_3_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=13,data_transform=None,_2025-02-12_10-27-42/checkpoint_000029)\n",
      "\u001b[36m(train_nn_reg pid=3412418)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_b9035250_4_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0002,data_transf_2025-02-12_10-27-48/checkpoint_000023)\n",
      "\u001b[36m(train_nn_reg pid=3418637)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3418637)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3418637)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3418637)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_734b55b9_13_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=19,data_transform=None_2025-02-12_10-30-50 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3418637)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3418637)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3418637)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3418637)\u001b[0m 0 | layers | ModuleList | 693 K  | train\n",
      "\u001b[36m(train_nn_reg pid=3418637)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3418637)\u001b[0m 693 K     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3418637)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3418637)\u001b[0m 693 K     Total params\n",
      "\u001b[36m(train_nn_reg pid=3418637)\u001b[0m 2.775     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3418637)\u001b[0m 24        Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3418637)\u001b[0m 0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3418637)\u001b[0m Train: (156, 3241), Test: (65, 3241)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3418637)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(train_nn_reg pid=3418637)\u001b[0m Metric train_rmse does not exist in `trainer.callback_metrics.\n",
      "\u001b[36m(train_nn_reg pid=3418637)\u001b[0m Metric train_r2 does not exist in `trainer.callback_metrics.\n",
      "\u001b[36m(train_nn_reg pid=3418637)\u001b[0m Metric train_loss does not exist in `trainer.callback_metrics.\n",
      "2025-02-12 10:31:03,813\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.870 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:31:03,815\tWARNING util.py:201 -- The `process_trial_result` operation took 1.873 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:31:03,816\tWARNING util.py:201 -- Processing trial results took 1.873 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:31:03,816\tWARNING util.py:201 -- The `process_trial_result` operation took 1.874 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:31:05,207\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.372 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:31:05,209\tWARNING util.py:201 -- The `process_trial_result` operation took 1.374 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:31:05,209\tWARNING util.py:201 -- Processing trial results took 1.375 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:31:05,210\tWARNING util.py:201 -- The `process_trial_result` operation took 1.375 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:31:07,074\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.840 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:31:07,075\tWARNING util.py:201 -- The `process_trial_result` operation took 1.842 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:31:07,076\tWARNING util.py:201 -- Processing trial results took 1.843 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:31:07,077\tWARNING util.py:201 -- The `process_trial_result` operation took 1.843 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:31:08,664\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.585 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:31:08,666\tWARNING util.py:201 -- The `process_trial_result` operation took 1.588 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:31:08,667\tWARNING util.py:201 -- Processing trial results took 1.588 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:31:08,667\tWARNING util.py:201 -- The `process_trial_result` operation took 1.589 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3418637)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_734b55b9_13_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=19,data_transform=None_2025-02-12_10-30-50/checkpoint_000001)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "2025-02-12 10:31:10,706\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.011 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:31:10,709\tWARNING util.py:201 -- The `process_trial_result` operation took 2.014 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:31:10,709\tWARNING util.py:201 -- Processing trial results took 2.014 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:31:10,709\tWARNING util.py:201 -- The `process_trial_result` operation took 2.015 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:31:13,649\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.881 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:31:13,652\tWARNING util.py:201 -- The `process_trial_result` operation took 2.883 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:31:13,652\tWARNING util.py:201 -- Processing trial results took 2.884 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:31:13,652\tWARNING util.py:201 -- The `process_trial_result` operation took 2.884 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:31:15,402\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.748 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:31:15,404\tWARNING util.py:201 -- The `process_trial_result` operation took 1.751 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:31:15,405\tWARNING util.py:201 -- Processing trial results took 1.751 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:31:15,405\tWARNING util.py:201 -- The `process_trial_result` operation took 1.752 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:31:16,543\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.136 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:31:16,545\tWARNING util.py:201 -- The `process_trial_result` operation took 1.138 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:31:16,546\tWARNING util.py:201 -- Processing trial results took 1.139 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:31:16,546\tWARNING util.py:201 -- The `process_trial_result` operation took 1.140 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3412418)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_b9035250_4_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0002,data_transf_2025-02-12_10-27-48/checkpoint_000026)\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "2025-02-12 10:31:18,335\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.763 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:31:18,337\tWARNING util.py:201 -- The `process_trial_result` operation took 1.766 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:31:18,338\tWARNING util.py:201 -- Processing trial results took 1.767 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:31:18,338\tWARNING util.py:201 -- The `process_trial_result` operation took 1.767 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:31:19,456\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.117 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:31:19,458\tWARNING util.py:201 -- The `process_trial_result` operation took 1.119 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:31:19,459\tWARNING util.py:201 -- Processing trial results took 1.120 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:31:19,459\tWARNING util.py:201 -- The `process_trial_result` operation took 1.120 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:31:20,706\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.245 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:31:20,708\tWARNING util.py:201 -- The `process_trial_result` operation took 1.247 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:31:20,708\tWARNING util.py:201 -- Processing trial results took 1.248 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:31:20,709\tWARNING util.py:201 -- The `process_trial_result` operation took 1.248 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:31:20,776\tERROR experiment_state.py:173 -- Saving experiment state to storage at '/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg' failed with exception: \n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/execution/experiment_state.py\", line 171, in wait_for_sync\n",
      "    self._storage.syncer.wait()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 474, in wait\n",
      "    raise e\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 472, in wait\n",
      "    self._sync_process.wait(timeout=timeout or self.sync_timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 173, in wait\n",
      "    raise exception\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 136, in entrypoint\n",
      "    result = self._fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/storage.py\", line 218, in _upload_to_fs_path\n",
      "    _pyarrow_fs_copy_files(local_path, fs_path, destination_filesystem=fs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/storage.py\", line 116, in _pyarrow_fs_copy_files\n",
      "    return pyarrow.fs.copy_files(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/pyarrow/fs.py\", line 259, in copy_files\n",
      "    _copy_files_selector(source_fs, source_sel,\n",
      "  File \"pyarrow/_fs.pyx\", line 1624, in pyarrow._fs._copy_files_selector\n",
      "  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\n",
      "FileNotFoundError: [Errno 2] Failed to open local file '/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_b9035250_4_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0002,data_transf_2025-02-12_10-27-48/epoch=23-val_rmse=17.31.ckpt'. Detail: [errno 2] No such file or directory\n",
      "2025-02-12 10:31:21,586\tWARNING util.py:201 -- The `process_trial_save` operation took 0.600 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3417235)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_4450d409_11_batch_size=128,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0031,data_trans_2025-02-12_10-29-59/checkpoint_000008)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "2025-02-12 10:31:24,296\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.697 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:31:24,299\tWARNING util.py:201 -- The `process_trial_result` operation took 2.700 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:31:24,299\tWARNING util.py:201 -- Processing trial results took 2.701 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:31:24,300\tWARNING util.py:201 -- The `process_trial_result` operation took 2.701 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:31:26,035\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.733 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:31:26,038\tWARNING util.py:201 -- The `process_trial_result` operation took 1.736 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:31:26,038\tWARNING util.py:201 -- Processing trial results took 1.737 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:31:26,038\tWARNING util.py:201 -- The `process_trial_result` operation took 1.737 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:31:27,968\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.910 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:31:27,969\tWARNING util.py:201 -- The `process_trial_result` operation took 1.912 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:31:27,970\tWARNING util.py:201 -- Processing trial results took 1.912 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:31:27,971\tWARNING util.py:201 -- The `process_trial_result` operation took 1.913 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3412262)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_301534e7_3_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=13,data_transform=None,_2025-02-12_10-27-42/checkpoint_000031)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "2025-02-12 10:31:29,638\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.636 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:31:29,641\tWARNING util.py:201 -- The `process_trial_result` operation took 1.639 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:31:29,642\tWARNING util.py:201 -- Processing trial results took 1.639 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:31:29,642\tWARNING util.py:201 -- The `process_trial_result` operation took 1.640 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:31:31,420\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.776 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:31:31,422\tWARNING util.py:201 -- The `process_trial_result` operation took 1.779 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:31:31,422\tWARNING util.py:201 -- Processing trial results took 1.779 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:31:31,423\tWARNING util.py:201 -- The `process_trial_result` operation took 1.780 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:31:31,436\tWARNING syncer.py:406 -- Last sync command failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 404, in _launch_sync_process\n",
      "    self.wait()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 474, in wait\n",
      "    raise e\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 472, in wait\n",
      "    self._sync_process.wait(timeout=timeout or self.sync_timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 173, in wait\n",
      "    raise exception\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 136, in entrypoint\n",
      "    result = self._fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/storage.py\", line 218, in _upload_to_fs_path\n",
      "    _pyarrow_fs_copy_files(local_path, fs_path, destination_filesystem=fs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/storage.py\", line 116, in _pyarrow_fs_copy_files\n",
      "    return pyarrow.fs.copy_files(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/pyarrow/fs.py\", line 259, in copy_files\n",
      "    _copy_files_selector(source_fs, source_sel,\n",
      "  File \"pyarrow/_fs.pyx\", line 1624, in pyarrow._fs._copy_files_selector\n",
      "  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\n",
      "FileNotFoundError: [Errno 2] Failed to open local file '/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_4450d409_11_batch_size=128,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0031,data_trans_2025-02-12_10-29-59/epoch=6-val_rmse=18.46.ckpt'. Detail: [errno 2] No such file or directory\n",
      "\n",
      "2025-02-12 10:31:33,770\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.294 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:31:33,772\tWARNING util.py:201 -- The `process_trial_result` operation took 2.296 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:31:33,773\tWARNING util.py:201 -- Processing trial results took 2.297 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:31:33,774\tWARNING util.py:201 -- The `process_trial_result` operation took 2.297 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:31:34,931\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.146 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:31:34,933\tWARNING util.py:201 -- The `process_trial_result` operation took 1.148 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:31:34,934\tWARNING util.py:201 -- Processing trial results took 1.148 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:31:34,934\tWARNING util.py:201 -- The `process_trial_result` operation took 1.149 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:31:36,717\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.781 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:31:36,719\tWARNING util.py:201 -- The `process_trial_result` operation took 1.783 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:31:36,720\tWARNING util.py:201 -- Processing trial results took 1.784 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:31:36,720\tWARNING util.py:201 -- The `process_trial_result` operation took 1.784 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:31:38,544\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.823 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:31:38,547\tWARNING util.py:201 -- The `process_trial_result` operation took 1.826 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:31:38,548\tWARNING util.py:201 -- Processing trial results took 1.826 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:31:38,548\tWARNING util.py:201 -- The `process_trial_result` operation took 1.826 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3412418)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_b9035250_4_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0002,data_transf_2025-02-12_10-27-48/checkpoint_000030)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "2025-02-12 10:31:40,729\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.722 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:31:40,731\tWARNING util.py:201 -- The `process_trial_result` operation took 1.724 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:31:40,731\tWARNING util.py:201 -- Processing trial results took 1.725 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:31:40,732\tWARNING util.py:201 -- The `process_trial_result` operation took 1.725 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:31:42,498\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.765 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:31:42,501\tWARNING util.py:201 -- The `process_trial_result` operation took 1.767 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:31:42,502\tWARNING util.py:201 -- Processing trial results took 1.768 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:31:42,502\tWARNING util.py:201 -- The `process_trial_result` operation took 1.769 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:31:42,516\tWARNING syncer.py:406 -- Last sync command failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 404, in _launch_sync_process\n",
      "    self.wait()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 474, in wait\n",
      "    raise e\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 472, in wait\n",
      "    self._sync_process.wait(timeout=timeout or self.sync_timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 173, in wait\n",
      "    raise exception\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 136, in entrypoint\n",
      "    result = self._fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/storage.py\", line 218, in _upload_to_fs_path\n",
      "    _pyarrow_fs_copy_files(local_path, fs_path, destination_filesystem=fs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/storage.py\", line 116, in _pyarrow_fs_copy_files\n",
      "    return pyarrow.fs.copy_files(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/pyarrow/fs.py\", line 259, in copy_files\n",
      "    _copy_files_selector(source_fs, source_sel,\n",
      "  File \"pyarrow/_fs.pyx\", line 1624, in pyarrow._fs._copy_files_selector\n",
      "  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\n",
      "FileNotFoundError: [Errno 2] Failed to open local file '/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_b9035250_4_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0002,data_transf_2025-02-12_10-27-48/epoch=27-val_rmse=16.99.ckpt'. Detail: [errno 2] No such file or directory\n",
      "\n",
      "2025-02-12 10:31:44,787\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.267 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:31:44,789\tWARNING util.py:201 -- The `process_trial_result` operation took 2.270 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:31:44,790\tWARNING util.py:201 -- Processing trial results took 2.270 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:31:44,790\tWARNING util.py:201 -- The `process_trial_result` operation took 2.271 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3413652)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_e26bf114_8_batch_size=128,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0018,data_transf_2025-02-12_10-28-19/checkpoint_000015)\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "2025-02-12 10:31:47,730\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.375 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:31:47,732\tWARNING util.py:201 -- The `process_trial_result` operation took 2.377 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:31:47,733\tWARNING util.py:201 -- Processing trial results took 2.378 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:31:47,734\tWARNING util.py:201 -- The `process_trial_result` operation took 2.379 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:31:48,860\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.125 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:31:48,862\tWARNING util.py:201 -- The `process_trial_result` operation took 1.127 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:31:48,863\tWARNING util.py:201 -- Processing trial results took 1.128 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:31:48,863\tWARNING util.py:201 -- The `process_trial_result` operation took 1.128 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:31:50,790\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.912 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:31:50,793\tWARNING util.py:201 -- The `process_trial_result` operation took 1.915 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:31:50,793\tWARNING util.py:201 -- Processing trial results took 1.916 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:31:50,794\tWARNING util.py:201 -- The `process_trial_result` operation took 1.916 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:31:52,465\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.652 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:31:52,467\tWARNING util.py:201 -- The `process_trial_result` operation took 1.655 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:31:52,467\tWARNING util.py:201 -- Processing trial results took 1.655 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:31:52,468\tWARNING util.py:201 -- The `process_trial_result` operation took 1.656 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:31:53,606\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.137 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:31:53,609\tWARNING util.py:201 -- The `process_trial_result` operation took 1.139 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:31:53,609\tWARNING util.py:201 -- Processing trial results took 1.140 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:31:53,609\tWARNING util.py:201 -- The `process_trial_result` operation took 1.140 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3412262)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_301534e7_3_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=13,data_transform=None,_2025-02-12_10-27-42/checkpoint_000033)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "2025-02-12 10:31:56,591\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.949 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:31:56,594\tWARNING util.py:201 -- The `process_trial_result` operation took 2.952 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:31:56,595\tWARNING util.py:201 -- Processing trial results took 2.953 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:31:56,596\tWARNING util.py:201 -- The `process_trial_result` operation took 2.954 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:31:59,653\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 3.046 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:31:59,655\tWARNING util.py:201 -- The `process_trial_result` operation took 3.048 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:31:59,656\tWARNING util.py:201 -- Processing trial results took 3.049 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:31:59,656\tWARNING util.py:201 -- The `process_trial_result` operation took 3.049 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3413652)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_e26bf114_8_batch_size=128,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0018,data_transf_2025-02-12_10-28-19/checkpoint_000016)\n",
      "\u001b[36m(train_nn_reg pid=3412418)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_b9035250_4_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0002,data_transf_2025-02-12_10-27-48/checkpoint_000032)\n",
      "2025-02-12 10:32:01,968\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.934 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:32:01,970\tWARNING util.py:201 -- The `process_trial_result` operation took 1.936 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:32:01,971\tWARNING util.py:201 -- Processing trial results took 1.937 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:32:01,972\tWARNING util.py:201 -- The `process_trial_result` operation took 1.938 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:32:03,639\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.656 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:32:03,642\tWARNING util.py:201 -- The `process_trial_result` operation took 1.659 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:32:03,642\tWARNING util.py:201 -- Processing trial results took 1.659 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:32:03,643\tWARNING util.py:201 -- The `process_trial_result` operation took 1.660 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:32:03,657\tWARNING syncer.py:406 -- Last sync command failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 404, in _launch_sync_process\n",
      "    self.wait()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 474, in wait\n",
      "    raise e\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 472, in wait\n",
      "    self._sync_process.wait(timeout=timeout or self.sync_timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 173, in wait\n",
      "    raise exception\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 136, in entrypoint\n",
      "    result = self._fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/storage.py\", line 218, in _upload_to_fs_path\n",
      "    _pyarrow_fs_copy_files(local_path, fs_path, destination_filesystem=fs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/storage.py\", line 116, in _pyarrow_fs_copy_files\n",
      "    return pyarrow.fs.copy_files(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/pyarrow/fs.py\", line 259, in copy_files\n",
      "    _copy_files_selector(source_fs, source_sel,\n",
      "  File \"pyarrow/_fs.pyx\", line 1624, in pyarrow._fs._copy_files_selector\n",
      "  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\n",
      "FileNotFoundError: [Errno 2] Failed to open local file '/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_301534e7_3_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=13,data_transform=None,_2025-02-12_10-27-42/epoch=25-val_rmse=1.09.ckpt'. Detail: [errno 2] No such file or directory\n",
      "\n",
      "2025-02-12 10:32:06,478\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.819 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:32:06,481\tWARNING util.py:201 -- The `process_trial_result` operation took 2.821 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:32:06,481\tWARNING util.py:201 -- Processing trial results took 2.822 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:32:06,482\tWARNING util.py:201 -- The `process_trial_result` operation took 2.822 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:32:08,107\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.623 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:32:08,109\tWARNING util.py:201 -- The `process_trial_result` operation took 1.626 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:32:08,110\tWARNING util.py:201 -- Processing trial results took 1.627 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:32:08,110\tWARNING util.py:201 -- The `process_trial_result` operation took 1.627 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3413652)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_e26bf114_8_batch_size=128,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0018,data_transf_2025-02-12_10-28-19/checkpoint_000017)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "2025-02-12 10:32:10,835\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.682 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:32:10,837\tWARNING util.py:201 -- The `process_trial_result` operation took 2.684 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:32:10,838\tWARNING util.py:201 -- Processing trial results took 2.686 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:32:10,838\tWARNING util.py:201 -- The `process_trial_result` operation took 2.686 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:32:12,160\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.321 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:32:12,162\tWARNING util.py:201 -- The `process_trial_result` operation took 1.323 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:32:12,163\tWARNING util.py:201 -- Processing trial results took 1.324 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:32:12,164\tWARNING util.py:201 -- The `process_trial_result` operation took 1.324 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:32:13,964\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.799 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:32:13,967\tWARNING util.py:201 -- The `process_trial_result` operation took 1.802 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:32:13,967\tWARNING util.py:201 -- Processing trial results took 1.802 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:32:13,968\tWARNING util.py:201 -- The `process_trial_result` operation took 1.803 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3412418)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_b9035250_4_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0002,data_transf_2025-02-12_10-27-48/checkpoint_000033)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2025-02-12 10:32:16,834\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.733 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:32:16,836\tWARNING util.py:201 -- The `process_trial_result` operation took 2.735 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:32:16,837\tWARNING util.py:201 -- Processing trial results took 2.736 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:32:16,837\tWARNING util.py:201 -- The `process_trial_result` operation took 2.736 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:32:18,519\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.680 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:32:18,521\tWARNING util.py:201 -- The `process_trial_result` operation took 1.683 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:32:18,522\tWARNING util.py:201 -- Processing trial results took 1.683 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:32:18,522\tWARNING util.py:201 -- The `process_trial_result` operation took 1.683 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:32:19,638\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.114 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:32:19,640\tWARNING util.py:201 -- The `process_trial_result` operation took 1.116 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:32:19,641\tWARNING util.py:201 -- Processing trial results took 1.117 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:32:19,641\tWARNING util.py:201 -- The `process_trial_result` operation took 1.118 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3412262)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_301534e7_3_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=13,data_transform=None,_2025-02-12_10-27-42/checkpoint_000036)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "2025-02-12 10:32:21,349\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.684 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:32:21,351\tWARNING util.py:201 -- The `process_trial_result` operation took 1.686 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:32:21,352\tWARNING util.py:201 -- Processing trial results took 1.687 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:32:21,352\tWARNING util.py:201 -- The `process_trial_result` operation took 1.687 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:32:23,019\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.666 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:32:23,022\tWARNING util.py:201 -- The `process_trial_result` operation took 1.668 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:32:23,022\tWARNING util.py:201 -- Processing trial results took 1.669 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:32:23,023\tWARNING util.py:201 -- The `process_trial_result` operation took 1.669 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:32:26,635\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 3.611 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:32:26,638\tWARNING util.py:201 -- The `process_trial_result` operation took 3.614 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:32:26,639\tWARNING util.py:201 -- Processing trial results took 3.615 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:32:26,640\tWARNING util.py:201 -- The `process_trial_result` operation took 3.616 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:32:26,653\tWARNING syncer.py:406 -- Last sync command failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 404, in _launch_sync_process\n",
      "    self.wait()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 474, in wait\n",
      "    raise e\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 472, in wait\n",
      "    self._sync_process.wait(timeout=timeout or self.sync_timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 173, in wait\n",
      "    raise exception\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 136, in entrypoint\n",
      "    result = self._fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/storage.py\", line 218, in _upload_to_fs_path\n",
      "    _pyarrow_fs_copy_files(local_path, fs_path, destination_filesystem=fs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/storage.py\", line 116, in _pyarrow_fs_copy_files\n",
      "    return pyarrow.fs.copy_files(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/pyarrow/fs.py\", line 259, in copy_files\n",
      "    _copy_files_selector(source_fs, source_sel,\n",
      "  File \"pyarrow/_fs.pyx\", line 1624, in pyarrow._fs._copy_files_selector\n",
      "  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\n",
      "FileNotFoundError: [Errno 2] Failed to open local file '/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_b9035250_4_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0002,data_transf_2025-02-12_10-27-48/epoch=31-val_rmse=16.63.ckpt'. Detail: [errno 2] No such file or directory\n",
      "\n",
      "\u001b[36m(train_nn_reg pid=3412418)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_b9035250_4_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0002,data_transf_2025-02-12_10-27-48/checkpoint_000034)\n",
      "\u001b[36m(train_nn_reg pid=3413652)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_e26bf114_8_batch_size=128,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0018,data_transf_2025-02-12_10-28-19/checkpoint_000019)\n",
      "2025-02-12 10:32:29,383\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.217 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:32:29,385\tWARNING util.py:201 -- The `process_trial_result` operation took 2.219 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:32:29,386\tWARNING util.py:201 -- Processing trial results took 2.220 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:32:29,386\tWARNING util.py:201 -- The `process_trial_result` operation took 2.221 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:32:31,188\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.800 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:32:31,190\tWARNING util.py:201 -- The `process_trial_result` operation took 1.802 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:32:31,192\tWARNING util.py:201 -- Processing trial results took 1.804 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:32:31,192\tWARNING util.py:201 -- The `process_trial_result` operation took 1.805 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:32:32,704\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.494 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:32:32,706\tWARNING util.py:201 -- The `process_trial_result` operation took 1.496 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:32:32,707\tWARNING util.py:201 -- Processing trial results took 1.497 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:32:32,707\tWARNING util.py:201 -- The `process_trial_result` operation took 1.497 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3413652)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_e26bf114_8_batch_size=128,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0018,data_transf_2025-02-12_10-28-19/checkpoint_000020)\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "2025-02-12 10:32:34,610\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.865 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:32:34,612\tWARNING util.py:201 -- The `process_trial_result` operation took 1.867 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:32:34,612\tWARNING util.py:201 -- Processing trial results took 1.868 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:32:34,613\tWARNING util.py:201 -- The `process_trial_result` operation took 1.868 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:32:35,710\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.096 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:32:35,713\tWARNING util.py:201 -- The `process_trial_result` operation took 1.099 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:32:35,713\tWARNING util.py:201 -- Processing trial results took 1.099 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:32:35,714\tWARNING util.py:201 -- The `process_trial_result` operation took 1.100 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:32:35,747\tERROR experiment_state.py:173 -- Saving experiment state to storage at '/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg' failed with exception: \n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/execution/experiment_state.py\", line 171, in wait_for_sync\n",
      "    self._storage.syncer.wait()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 474, in wait\n",
      "    raise e\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 472, in wait\n",
      "    self._sync_process.wait(timeout=timeout or self.sync_timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 173, in wait\n",
      "    raise exception\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 136, in entrypoint\n",
      "    result = self._fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/storage.py\", line 218, in _upload_to_fs_path\n",
      "    _pyarrow_fs_copy_files(local_path, fs_path, destination_filesystem=fs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/storage.py\", line 116, in _pyarrow_fs_copy_files\n",
      "    return pyarrow.fs.copy_files(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/pyarrow/fs.py\", line 259, in copy_files\n",
      "    _copy_files_selector(source_fs, source_sel,\n",
      "  File \"pyarrow/_fs.pyx\", line 1624, in pyarrow._fs._copy_files_selector\n",
      "  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\n",
      "FileNotFoundError: [Errno 2] Failed to open local file '/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_b9035250_4_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0002,data_transf_2025-02-12_10-27-48/epoch=32-val_rmse=16.53.ckpt'. Detail: [errno 2] No such file or directory\n",
      "2025-02-12 10:32:39,852\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 4.101 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:32:39,854\tWARNING util.py:201 -- The `process_trial_result` operation took 4.104 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:32:39,854\tWARNING util.py:201 -- Processing trial results took 4.104 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:32:39,855\tWARNING util.py:201 -- The `process_trial_result` operation took 4.104 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:32:40,977\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.110 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:32:40,979\tWARNING util.py:201 -- The `process_trial_result` operation took 1.113 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:32:40,980\tWARNING util.py:201 -- Processing trial results took 1.113 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:32:40,980\tWARNING util.py:201 -- The `process_trial_result` operation took 1.114 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:32:42,654\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.671 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:32:42,656\tWARNING util.py:201 -- The `process_trial_result` operation took 1.674 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:32:42,656\tWARNING util.py:201 -- Processing trial results took 1.674 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:32:42,657\tWARNING util.py:201 -- The `process_trial_result` operation took 1.675 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3413652)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_e26bf114_8_batch_size=128,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0018,data_transf_2025-02-12_10-28-19/checkpoint_000021)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "2025-02-12 10:32:44,916\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.236 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:32:44,919\tWARNING util.py:201 -- The `process_trial_result` operation took 2.239 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:32:44,919\tWARNING util.py:201 -- Processing trial results took 2.239 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:32:44,919\tWARNING util.py:201 -- The `process_trial_result` operation took 2.240 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:32:46,120\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.169 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:32:46,122\tWARNING util.py:201 -- The `process_trial_result` operation took 1.171 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:32:46,123\tWARNING util.py:201 -- Processing trial results took 1.172 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:32:46,124\tWARNING util.py:201 -- The `process_trial_result` operation took 1.173 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:32:48,357\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.219 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:32:48,359\tWARNING util.py:201 -- The `process_trial_result` operation took 2.221 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:32:48,360\tWARNING util.py:201 -- Processing trial results took 2.222 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:32:48,360\tWARNING util.py:201 -- The `process_trial_result` operation took 2.223 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:32:49,979\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.617 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:32:49,981\tWARNING util.py:201 -- The `process_trial_result` operation took 1.619 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:32:49,981\tWARNING util.py:201 -- Processing trial results took 1.620 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:32:49,982\tWARNING util.py:201 -- The `process_trial_result` operation took 1.620 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3413652)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_e26bf114_8_batch_size=128,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0018,data_transf_2025-02-12_10-28-19/checkpoint_000022)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2025-02-12 10:32:52,219\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.681 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:32:52,221\tWARNING util.py:201 -- The `process_trial_result` operation took 1.683 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:32:52,222\tWARNING util.py:201 -- Processing trial results took 1.684 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:32:52,222\tWARNING util.py:201 -- The `process_trial_result` operation took 1.684 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:32:53,328\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.104 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:32:53,330\tWARNING util.py:201 -- The `process_trial_result` operation took 1.107 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:32:53,331\tWARNING util.py:201 -- Processing trial results took 1.107 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:32:53,331\tWARNING util.py:201 -- The `process_trial_result` operation took 1.108 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:32:54,981\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.649 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:32:54,984\tWARNING util.py:201 -- The `process_trial_result` operation took 1.651 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:32:54,985\tWARNING util.py:201 -- Processing trial results took 1.652 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:32:54,985\tWARNING util.py:201 -- The `process_trial_result` operation took 1.653 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:32:57,307\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.985 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:32:57,309\tWARNING util.py:201 -- The `process_trial_result` operation took 1.987 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:32:57,310\tWARNING util.py:201 -- Processing trial results took 1.988 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:32:57,311\tWARNING util.py:201 -- The `process_trial_result` operation took 1.988 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3413652)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_e26bf114_8_batch_size=128,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0018,data_transf_2025-02-12_10-28-19/checkpoint_000023)\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "2025-02-12 10:33:00,734\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 3.392 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:00,736\tWARNING util.py:201 -- The `process_trial_result` operation took 3.395 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:00,737\tWARNING util.py:201 -- Processing trial results took 3.395 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:33:00,737\tWARNING util.py:201 -- The `process_trial_result` operation took 3.396 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3422373)\u001b[0m Seed set to 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3422373)\u001b[0m Train: (156, 89), Test: (65, 89)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3422373)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3422373)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3422373)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3422373)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_fb67bf3c_14_batch_size=64,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0000,data_transf_2025-02-12_10-32-55 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3422373)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3422373)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3422373)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3422373)\u001b[0m 0 | layers | ModuleList | 777 K  | train\n",
      "\u001b[36m(train_nn_reg pid=3422373)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3422373)\u001b[0m 777 K     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3422373)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3422373)\u001b[0m 777 K     Total params\n",
      "\u001b[36m(train_nn_reg pid=3422373)\u001b[0m 3.109     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3422373)\u001b[0m 34        Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3422373)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3422373)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3422373)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-02-12 10:33:04,252\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.337 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:04,255\tWARNING util.py:201 -- The `process_trial_result` operation took 1.340 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:04,256\tWARNING util.py:201 -- Processing trial results took 1.340 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:33:04,256\tWARNING util.py:201 -- The `process_trial_result` operation took 1.341 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:04,272\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_fb67bf3c\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3422373, ip=172.31.181.85, actor_id=24fe26a20680d15fbc2e316001000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1024, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1053, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 1) cannot be concatenated\n",
      "/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/optuna/distributions.py:708: UserWarning: The distribution is specified by [1, 5] and step=5, but the range is not divisible by `step`. It will be replaced by [1, 1].\n",
      "  warnings.warn(\n",
      "\u001b[36m(train_nn_reg pid=3412418)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_b9035250_4_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0002,data_transf_2025-02-12_10-27-48/checkpoint_000039)\n",
      "2025-02-12 10:33:05,934\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.397 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:05,936\tWARNING util.py:201 -- The `process_trial_result` operation took 1.400 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:05,936\tWARNING util.py:201 -- Processing trial results took 1.400 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:33:05,937\tWARNING util.py:201 -- The `process_trial_result` operation took 1.401 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:07,836\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.884 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:07,838\tWARNING util.py:201 -- The `process_trial_result` operation took 1.886 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:07,838\tWARNING util.py:201 -- Processing trial results took 1.887 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:33:07,839\tWARNING util.py:201 -- The `process_trial_result` operation took 1.887 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:10,310\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.420 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:10,312\tWARNING util.py:201 -- The `process_trial_result` operation took 2.423 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:10,313\tWARNING util.py:201 -- Processing trial results took 2.424 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:33:10,313\tWARNING util.py:201 -- The `process_trial_result` operation took 2.424 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:11,470\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.155 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:11,472\tWARNING util.py:201 -- The `process_trial_result` operation took 1.158 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:11,473\tWARNING util.py:201 -- Processing trial results took 1.158 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:33:11,473\tWARNING util.py:201 -- The `process_trial_result` operation took 1.158 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3412418)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_b9035250_4_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0002,data_transf_2025-02-12_10-27-48/checkpoint_000040)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "2025-02-12 10:33:13,412\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.913 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:13,414\tWARNING util.py:201 -- The `process_trial_result` operation took 1.916 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:13,415\tWARNING util.py:201 -- Processing trial results took 1.916 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:33:13,415\tWARNING util.py:201 -- The `process_trial_result` operation took 1.917 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:14,810\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.308 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:14,813\tWARNING util.py:201 -- The `process_trial_result` operation took 1.310 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:14,813\tWARNING util.py:201 -- Processing trial results took 1.311 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:33:14,813\tWARNING util.py:201 -- The `process_trial_result` operation took 1.311 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3422740)\u001b[0m Seed set to 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3422740)\u001b[0m Train: (156, 2107), Test: (65, 2107)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3422740)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3422740)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3422740)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3422740)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_a70dbacd_15_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=11,data_transform=None_2025-02-12_10-33-04 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3422740)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3422740)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3422740)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3422740)\u001b[0m 0 | layers | ModuleList | 132 K  | train\n",
      "\u001b[36m(train_nn_reg pid=3422740)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3422740)\u001b[0m 132 K     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3422740)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3422740)\u001b[0m 132 K     Total params\n",
      "\u001b[36m(train_nn_reg pid=3422740)\u001b[0m 0.531     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3422740)\u001b[0m 4         Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3422740)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3422740)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(train_nn_reg pid=3422740)\u001b[0m Metric train_rmse does not exist in `trainer.callback_metrics.\n",
      "\u001b[36m(train_nn_reg pid=3422740)\u001b[0m Metric train_r2 does not exist in `trainer.callback_metrics.\n",
      "\u001b[36m(train_nn_reg pid=3422740)\u001b[0m Metric train_loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[36m(train_nn_reg pid=3412262)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_301534e7_3_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=13,data_transform=None,_2025-02-12_10-27-42/checkpoint_000039)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "2025-02-12 10:33:18,086\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.842 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:18,088\tWARNING util.py:201 -- The `process_trial_result` operation took 1.845 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:18,088\tWARNING util.py:201 -- Processing trial results took 1.846 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:33:18,089\tWARNING util.py:201 -- The `process_trial_result` operation took 1.846 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:20,104\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.991 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:20,106\tWARNING util.py:201 -- The `process_trial_result` operation took 1.993 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:20,107\tWARNING util.py:201 -- Processing trial results took 1.994 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:33:20,108\tWARNING util.py:201 -- The `process_trial_result` operation took 1.995 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:21,961\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.852 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:21,964\tWARNING util.py:201 -- The `process_trial_result` operation took 1.855 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:21,965\tWARNING util.py:201 -- Processing trial results took 1.856 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:33:21,965\tWARNING util.py:201 -- The `process_trial_result` operation took 1.856 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3417235)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_4450d409_11_batch_size=128,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0031,data_trans_2025-02-12_10-29-59/checkpoint_000023)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "2025-02-12 10:33:23,379\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.398 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:23,381\tWARNING util.py:201 -- The `process_trial_result` operation took 1.400 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:23,382\tWARNING util.py:201 -- Processing trial results took 1.401 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:33:23,382\tWARNING util.py:201 -- The `process_trial_result` operation took 1.402 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:24,718\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.322 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:24,720\tWARNING util.py:201 -- The `process_trial_result` operation took 1.324 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:24,721\tWARNING util.py:201 -- Processing trial results took 1.325 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:33:24,722\tWARNING util.py:201 -- The `process_trial_result` operation took 1.325 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:25,877\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.154 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:25,879\tWARNING util.py:201 -- The `process_trial_result` operation took 1.156 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:25,880\tWARNING util.py:201 -- Processing trial results took 1.156 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:33:25,881\tWARNING util.py:201 -- The `process_trial_result` operation took 1.157 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:26,869\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 0.987 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:26,871\tWARNING util.py:201 -- The `process_trial_result` operation took 0.989 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:26,872\tWARNING util.py:201 -- Processing trial results took 0.990 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:33:26,872\tWARNING util.py:201 -- The `process_trial_result` operation took 0.991 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3412262)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_301534e7_3_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=13,data_transform=None,_2025-02-12_10-27-42/checkpoint_000040)\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "2025-02-12 10:33:29,227\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.312 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:29,229\tWARNING util.py:201 -- The `process_trial_result` operation took 2.315 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:29,230\tWARNING util.py:201 -- Processing trial results took 2.316 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:33:29,230\tWARNING util.py:201 -- The `process_trial_result` operation took 2.316 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:31,349\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.093 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:31,351\tWARNING util.py:201 -- The `process_trial_result` operation took 2.096 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:31,352\tWARNING util.py:201 -- Processing trial results took 2.096 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:33:31,352\tWARNING util.py:201 -- The `process_trial_result` operation took 2.097 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:32,719\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.364 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:32,721\tWARNING util.py:201 -- The `process_trial_result` operation took 1.366 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:32,722\tWARNING util.py:201 -- Processing trial results took 1.367 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:33:32,722\tWARNING util.py:201 -- The `process_trial_result` operation took 1.368 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3413652)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_e26bf114_8_batch_size=128,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0018,data_transf_2025-02-12_10-28-19/checkpoint_000028)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "2025-02-12 10:33:34,589\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.850 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:34,592\tWARNING util.py:201 -- The `process_trial_result` operation took 1.853 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:34,593\tWARNING util.py:201 -- Processing trial results took 1.854 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:33:34,594\tWARNING util.py:201 -- The `process_trial_result` operation took 1.854 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:36,507\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.901 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:36,509\tWARNING util.py:201 -- The `process_trial_result` operation took 1.903 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:36,509\tWARNING util.py:201 -- Processing trial results took 1.904 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:33:36,510\tWARNING util.py:201 -- The `process_trial_result` operation took 1.904 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:38,281\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.723 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:38,284\tWARNING util.py:201 -- The `process_trial_result` operation took 1.726 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:38,286\tWARNING util.py:201 -- Processing trial results took 1.728 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:33:38,286\tWARNING util.py:201 -- The `process_trial_result` operation took 1.729 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:39,967\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.680 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:39,970\tWARNING util.py:201 -- The `process_trial_result` operation took 1.682 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:39,970\tWARNING util.py:201 -- Processing trial results took 1.683 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:33:39,971\tWARNING util.py:201 -- The `process_trial_result` operation took 1.683 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:42,953\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.958 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:42,955\tWARNING util.py:201 -- The `process_trial_result` operation took 2.961 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:42,956\tWARNING util.py:201 -- Processing trial results took 2.961 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:33:42,956\tWARNING util.py:201 -- The `process_trial_result` operation took 2.962 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3413652)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_e26bf114_8_batch_size=128,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0018,data_transf_2025-02-12_10-28-19/checkpoint_000029)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "2025-02-12 10:33:44,206\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.237 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:44,208\tWARNING util.py:201 -- The `process_trial_result` operation took 1.239 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:44,209\tWARNING util.py:201 -- Processing trial results took 1.240 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:33:44,209\tWARNING util.py:201 -- The `process_trial_result` operation took 1.240 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:45,656\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.433 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:45,658\tWARNING util.py:201 -- The `process_trial_result` operation took 1.436 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:45,659\tWARNING util.py:201 -- Processing trial results took 1.437 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:33:45,659\tWARNING util.py:201 -- The `process_trial_result` operation took 1.437 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:47,912\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.207 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:47,914\tWARNING util.py:201 -- The `process_trial_result` operation took 2.210 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:47,915\tWARNING util.py:201 -- Processing trial results took 2.211 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:33:47,915\tWARNING util.py:201 -- The `process_trial_result` operation took 2.211 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3417235)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_4450d409_11_batch_size=128,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0031,data_trans_2025-02-12_10-29-59/checkpoint_000027)\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "2025-02-12 10:33:49,203\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.255 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:49,205\tWARNING util.py:201 -- The `process_trial_result` operation took 1.258 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:49,206\tWARNING util.py:201 -- Processing trial results took 1.259 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:33:49,206\tWARNING util.py:201 -- The `process_trial_result` operation took 1.259 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:50,319\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.111 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:50,321\tWARNING util.py:201 -- The `process_trial_result` operation took 1.114 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:50,322\tWARNING util.py:201 -- Processing trial results took 1.115 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:33:50,323\tWARNING util.py:201 -- The `process_trial_result` operation took 1.115 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:52,528\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.191 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:52,530\tWARNING util.py:201 -- The `process_trial_result` operation took 2.194 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:52,531\tWARNING util.py:201 -- Processing trial results took 2.195 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:33:52,532\tWARNING util.py:201 -- The `process_trial_result` operation took 2.195 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:54,507\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.943 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:54,510\tWARNING util.py:201 -- The `process_trial_result` operation took 1.946 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:54,510\tWARNING util.py:201 -- Processing trial results took 1.947 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:33:54,511\tWARNING util.py:201 -- The `process_trial_result` operation took 1.947 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:56,199\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.686 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:56,202\tWARNING util.py:201 -- The `process_trial_result` operation took 1.689 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:56,203\tWARNING util.py:201 -- Processing trial results took 1.690 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:33:56,204\tWARNING util.py:201 -- The `process_trial_result` operation took 1.691 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3413652)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_e26bf114_8_batch_size=128,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0018,data_transf_2025-02-12_10-28-19/checkpoint_000031)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2025-02-12 10:33:57,480\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.252 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:57,483\tWARNING util.py:201 -- The `process_trial_result` operation took 1.255 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:57,483\tWARNING util.py:201 -- Processing trial results took 1.256 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:33:57,483\tWARNING util.py:201 -- The `process_trial_result` operation took 1.256 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:58,723\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.238 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:58,725\tWARNING util.py:201 -- The `process_trial_result` operation took 1.241 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:33:58,726\tWARNING util.py:201 -- Processing trial results took 1.241 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:33:58,726\tWARNING util.py:201 -- The `process_trial_result` operation took 1.242 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:00,644\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.883 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:00,647\tWARNING util.py:201 -- The `process_trial_result` operation took 1.886 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:00,648\tWARNING util.py:201 -- Processing trial results took 1.887 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:34:00,649\tWARNING util.py:201 -- The `process_trial_result` operation took 1.888 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:03,439\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.759 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:03,441\tWARNING util.py:201 -- The `process_trial_result` operation took 2.761 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:03,442\tWARNING util.py:201 -- Processing trial results took 2.762 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:34:03,442\tWARNING util.py:201 -- The `process_trial_result` operation took 2.763 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:04,552\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.108 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:04,554\tWARNING util.py:201 -- The `process_trial_result` operation took 1.111 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:04,555\tWARNING util.py:201 -- Processing trial results took 1.112 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:34:04,556\tWARNING util.py:201 -- The `process_trial_result` operation took 1.112 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3413652)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_e26bf114_8_batch_size=128,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0018,data_transf_2025-02-12_10-28-19/checkpoint_000033)\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "2025-02-12 10:34:06,100\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.495 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:06,102\tWARNING util.py:201 -- The `process_trial_result` operation took 1.498 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:06,103\tWARNING util.py:201 -- Processing trial results took 1.498 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:34:06,103\tWARNING util.py:201 -- The `process_trial_result` operation took 1.499 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:07,215\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.110 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:07,217\tWARNING util.py:201 -- The `process_trial_result` operation took 1.113 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:07,218\tWARNING util.py:201 -- Processing trial results took 1.113 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:34:07,218\tWARNING util.py:201 -- The `process_trial_result` operation took 1.114 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:08,172\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 0.952 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:08,174\tWARNING util.py:201 -- The `process_trial_result` operation took 0.954 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:08,175\tWARNING util.py:201 -- Processing trial results took 0.955 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:34:08,175\tWARNING util.py:201 -- The `process_trial_result` operation took 0.955 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:08,197\tERROR experiment_state.py:173 -- Saving experiment state to storage at '/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg' failed with exception: \n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/execution/experiment_state.py\", line 171, in wait_for_sync\n",
      "    self._storage.syncer.wait()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 474, in wait\n",
      "    raise e\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 472, in wait\n",
      "    self._sync_process.wait(timeout=timeout or self.sync_timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 173, in wait\n",
      "    raise exception\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 136, in entrypoint\n",
      "    result = self._fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/storage.py\", line 218, in _upload_to_fs_path\n",
      "    _pyarrow_fs_copy_files(local_path, fs_path, destination_filesystem=fs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/storage.py\", line 116, in _pyarrow_fs_copy_files\n",
      "    return pyarrow.fs.copy_files(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/pyarrow/fs.py\", line 259, in copy_files\n",
      "    _copy_files_selector(source_fs, source_sel,\n",
      "  File \"pyarrow/_fs.pyx\", line 1624, in pyarrow._fs._copy_files_selector\n",
      "  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\n",
      "FileNotFoundError: [Errno 2] Failed to open local file '/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_a70dbacd_15_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=11,data_transform=None_2025-02-12_10-33-04/epoch=3-val_rmse=18.06.ckpt'. Detail: [errno 2] No such file or directory\n",
      "2025-02-12 10:34:11,286\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 3.040 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:11,288\tWARNING util.py:201 -- The `process_trial_result` operation took 3.043 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:11,289\tWARNING util.py:201 -- Processing trial results took 3.044 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:34:11,289\tWARNING util.py:201 -- The `process_trial_result` operation took 3.044 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3417235)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_4450d409_11_batch_size=128,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0031,data_trans_2025-02-12_10-29-59/checkpoint_000029)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "2025-02-12 10:34:13,209\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.906 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:13,212\tWARNING util.py:201 -- The `process_trial_result` operation took 1.909 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:13,213\tWARNING util.py:201 -- Processing trial results took 1.910 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:34:13,213\tWARNING util.py:201 -- The `process_trial_result` operation took 1.911 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:14,367\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.153 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:14,369\tWARNING util.py:201 -- The `process_trial_result` operation took 1.155 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:14,370\tWARNING util.py:201 -- Processing trial results took 1.156 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:34:14,371\tWARNING util.py:201 -- The `process_trial_result` operation took 1.157 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:15,862\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.467 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:15,864\tWARNING util.py:201 -- The `process_trial_result` operation took 1.470 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:15,865\tWARNING util.py:201 -- Processing trial results took 1.471 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:34:15,866\tWARNING util.py:201 -- The `process_trial_result` operation took 1.471 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:17,126\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.258 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:17,128\tWARNING util.py:201 -- The `process_trial_result` operation took 1.260 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:17,129\tWARNING util.py:201 -- Processing trial results took 1.261 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:34:17,129\tWARNING util.py:201 -- The `process_trial_result` operation took 1.261 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:17,167\tERROR experiment_state.py:173 -- Saving experiment state to storage at '/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg' failed with exception: \n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/execution/experiment_state.py\", line 171, in wait_for_sync\n",
      "    self._storage.syncer.wait()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 474, in wait\n",
      "    raise e\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 472, in wait\n",
      "    self._sync_process.wait(timeout=timeout or self.sync_timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 173, in wait\n",
      "    raise exception\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 136, in entrypoint\n",
      "    result = self._fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/storage.py\", line 218, in _upload_to_fs_path\n",
      "    _pyarrow_fs_copy_files(local_path, fs_path, destination_filesystem=fs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/storage.py\", line 116, in _pyarrow_fs_copy_files\n",
      "    return pyarrow.fs.copy_files(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/pyarrow/fs.py\", line 259, in copy_files\n",
      "    _copy_files_selector(source_fs, source_sel,\n",
      "  File \"pyarrow/_fs.pyx\", line 1624, in pyarrow._fs._copy_files_selector\n",
      "  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\n",
      "FileNotFoundError: [Errno 2] Failed to open local file '/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_b9035250_4_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0002,data_transf_2025-02-12_10-27-48/epoch=44-val_rmse=15.14.ckpt'. Detail: [errno 2] No such file or directory\n",
      "\u001b[36m(train_nn_reg pid=3412418)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_b9035250_4_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0002,data_transf_2025-02-12_10-27-48/checkpoint_000048)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2025-02-12 10:34:20,329\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 3.159 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:20,332\tWARNING util.py:201 -- The `process_trial_result` operation took 3.161 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:20,333\tWARNING util.py:201 -- Processing trial results took 3.162 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:34:20,333\tWARNING util.py:201 -- The `process_trial_result` operation took 3.163 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:22,002\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.668 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:22,005\tWARNING util.py:201 -- The `process_trial_result` operation took 1.670 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:22,005\tWARNING util.py:201 -- Processing trial results took 1.671 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:34:22,005\tWARNING util.py:201 -- The `process_trial_result` operation took 1.671 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:23,161\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.154 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:23,163\tWARNING util.py:201 -- The `process_trial_result` operation took 1.157 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:23,164\tWARNING util.py:201 -- Processing trial results took 1.157 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:34:23,164\tWARNING util.py:201 -- The `process_trial_result` operation took 1.157 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3413652)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_e26bf114_8_batch_size=128,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0018,data_transf_2025-02-12_10-28-19/checkpoint_000036)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "2025-02-12 10:34:24,744\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.556 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:24,747\tWARNING util.py:201 -- The `process_trial_result` operation took 1.559 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:24,747\tWARNING util.py:201 -- Processing trial results took 1.560 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:34:24,748\tWARNING util.py:201 -- The `process_trial_result` operation took 1.560 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:26,258\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.478 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:26,261\tWARNING util.py:201 -- The `process_trial_result` operation took 1.480 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:26,261\tWARNING util.py:201 -- Processing trial results took 1.481 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:34:26,261\tWARNING util.py:201 -- The `process_trial_result` operation took 1.481 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:29,226\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.937 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:29,228\tWARNING util.py:201 -- The `process_trial_result` operation took 2.940 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:29,229\tWARNING util.py:201 -- Processing trial results took 2.940 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:34:29,230\tWARNING util.py:201 -- The `process_trial_result` operation took 2.941 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3412418)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_b9035250_4_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0002,data_transf_2025-02-12_10-27-48/checkpoint_000049)\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "2025-02-12 10:34:30,658\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.377 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:30,660\tWARNING util.py:201 -- The `process_trial_result` operation took 1.379 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:30,661\tWARNING util.py:201 -- Processing trial results took 1.381 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:34:30,662\tWARNING util.py:201 -- The `process_trial_result` operation took 1.381 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:32,978\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.271 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:32,980\tWARNING util.py:201 -- The `process_trial_result` operation took 2.274 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:32,981\tWARNING util.py:201 -- Processing trial results took 2.275 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:34:32,982\tWARNING util.py:201 -- The `process_trial_result` operation took 2.275 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:33,988\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.004 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:33,990\tWARNING util.py:201 -- The `process_trial_result` operation took 1.007 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:33,991\tWARNING util.py:201 -- Processing trial results took 1.007 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:34:33,991\tWARNING util.py:201 -- The `process_trial_result` operation took 1.008 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:35,943\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.939 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:35,945\tWARNING util.py:201 -- The `process_trial_result` operation took 1.941 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:35,946\tWARNING util.py:201 -- Processing trial results took 1.942 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:34:35,946\tWARNING util.py:201 -- The `process_trial_result` operation took 1.942 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:37,784\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.824 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:37,786\tWARNING util.py:201 -- The `process_trial_result` operation took 1.826 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:37,787\tWARNING util.py:201 -- Processing trial results took 1.828 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:34:37,787\tWARNING util.py:201 -- The `process_trial_result` operation took 1.828 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3422740)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_a70dbacd_15_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=11,data_transform=None_2025-02-12_10-33-04/checkpoint_000008)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "2025-02-12 10:34:40,264\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.411 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:40,267\tWARNING util.py:201 -- The `process_trial_result` operation took 2.414 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:40,268\tWARNING util.py:201 -- Processing trial results took 2.415 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:34:40,268\tWARNING util.py:201 -- The `process_trial_result` operation took 2.415 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:41,949\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.679 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:41,951\tWARNING util.py:201 -- The `process_trial_result` operation took 1.682 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:41,952\tWARNING util.py:201 -- Processing trial results took 1.682 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:34:41,952\tWARNING util.py:201 -- The `process_trial_result` operation took 1.683 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:43,554\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.577 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:43,556\tWARNING util.py:201 -- The `process_trial_result` operation took 1.579 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:43,557\tWARNING util.py:201 -- Processing trial results took 1.580 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:34:43,558\tWARNING util.py:201 -- The `process_trial_result` operation took 1.581 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3412262)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_301534e7_3_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=13,data_transform=None,_2025-02-12_10-27-42/checkpoint_000049)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "2025-02-12 10:34:45,704\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.143 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:45,706\tWARNING util.py:201 -- The `process_trial_result` operation took 2.146 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:45,707\tWARNING util.py:201 -- Processing trial results took 2.146 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:34:45,707\tWARNING util.py:201 -- The `process_trial_result` operation took 2.147 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:47,437\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.728 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:47,439\tWARNING util.py:201 -- The `process_trial_result` operation took 1.730 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:47,440\tWARNING util.py:201 -- Processing trial results took 1.731 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:34:47,440\tWARNING util.py:201 -- The `process_trial_result` operation took 1.732 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:49,017\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.552 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:49,019\tWARNING util.py:201 -- The `process_trial_result` operation took 1.555 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:49,020\tWARNING util.py:201 -- Processing trial results took 1.555 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:34:49,021\tWARNING util.py:201 -- The `process_trial_result` operation took 1.556 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:49,036\tWARNING syncer.py:406 -- Last sync command failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 404, in _launch_sync_process\n",
      "    self.wait()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 474, in wait\n",
      "    raise e\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 472, in wait\n",
      "    self._sync_process.wait(timeout=timeout or self.sync_timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 173, in wait\n",
      "    raise exception\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 136, in entrypoint\n",
      "    result = self._fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/storage.py\", line 218, in _upload_to_fs_path\n",
      "    _pyarrow_fs_copy_files(local_path, fs_path, destination_filesystem=fs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/storage.py\", line 116, in _pyarrow_fs_copy_files\n",
      "    return pyarrow.fs.copy_files(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/pyarrow/fs.py\", line 259, in copy_files\n",
      "    _copy_files_selector(source_fs, source_sel,\n",
      "  File \"pyarrow/_fs.pyx\", line 1624, in pyarrow._fs._copy_files_selector\n",
      "  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\n",
      "FileNotFoundError: [Errno 2] Failed to open local file '/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_301534e7_3_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=13,data_transform=None,_2025-02-12_10-27-42/epoch=44-val_rmse=0.95.ckpt'. Detail: [errno 2] No such file or directory\n",
      "\n",
      "\u001b[36m(train_nn_reg pid=3413652)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_e26bf114_8_batch_size=128,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0018,data_transf_2025-02-12_10-28-19/checkpoint_000039)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "2025-02-12 10:34:52,071\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.998 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:52,074\tWARNING util.py:201 -- The `process_trial_result` operation took 3.001 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:52,074\tWARNING util.py:201 -- Processing trial results took 3.002 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:34:52,075\tWARNING util.py:201 -- The `process_trial_result` operation took 3.002 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:53,182\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.105 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:53,184\tWARNING util.py:201 -- The `process_trial_result` operation took 1.108 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:53,185\tWARNING util.py:201 -- Processing trial results took 1.109 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:34:53,186\tWARNING util.py:201 -- The `process_trial_result` operation took 1.110 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3412262)\u001b[0m `Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "2025-02-12 10:34:54,754\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.474 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:54,757\tWARNING util.py:201 -- The `process_trial_result` operation took 1.477 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:54,757\tWARNING util.py:201 -- Processing trial results took 1.478 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:34:54,758\tWARNING util.py:201 -- The `process_trial_result` operation took 1.478 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:55,935\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.176 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:55,937\tWARNING util.py:201 -- The `process_trial_result` operation took 1.178 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:55,938\tWARNING util.py:201 -- Processing trial results took 1.179 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:34:55,939\tWARNING util.py:201 -- The `process_trial_result` operation took 1.180 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:57,055\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.115 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:57,057\tWARNING util.py:201 -- The `process_trial_result` operation took 1.117 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:34:57,058\tWARNING util.py:201 -- Processing trial results took 1.118 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:34:57,058\tWARNING util.py:201 -- The `process_trial_result` operation took 1.118 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3412418)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_b9035250_4_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0002,data_transf_2025-02-12_10-27-48/checkpoint_000051)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "2025-02-12 10:35:00,507\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 3.396 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:00,509\tWARNING util.py:201 -- The `process_trial_result` operation took 3.398 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:00,510\tWARNING util.py:201 -- Processing trial results took 3.399 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:35:00,510\tWARNING util.py:201 -- The `process_trial_result` operation took 3.399 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:01,463\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 0.939 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:01,466\tWARNING util.py:201 -- The `process_trial_result` operation took 0.942 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:01,466\tWARNING util.py:201 -- Processing trial results took 0.943 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:35:01,467\tWARNING util.py:201 -- The `process_trial_result` operation took 0.943 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:02,583\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.115 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:02,586\tWARNING util.py:201 -- The `process_trial_result` operation took 1.118 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:02,587\tWARNING util.py:201 -- Processing trial results took 1.119 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:35:02,587\tWARNING util.py:201 -- The `process_trial_result` operation took 1.119 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3413652)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_e26bf114_8_batch_size=128,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0018,data_transf_2025-02-12_10-28-19/checkpoint_000042)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2025-02-12 10:35:04,086\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.470 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:04,089\tWARNING util.py:201 -- The `process_trial_result` operation took 1.473 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:04,089\tWARNING util.py:201 -- Processing trial results took 1.473 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:35:04,089\tWARNING util.py:201 -- The `process_trial_result` operation took 1.473 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:06,091\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.000 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:06,093\tWARNING util.py:201 -- The `process_trial_result` operation took 2.002 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:06,093\tWARNING util.py:201 -- Processing trial results took 2.003 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:35:06,094\tWARNING util.py:201 -- The `process_trial_result` operation took 2.003 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:07,744\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.214 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:07,746\tWARNING util.py:201 -- The `process_trial_result` operation took 1.216 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:07,747\tWARNING util.py:201 -- Processing trial results took 1.217 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:35:07,748\tWARNING util.py:201 -- The `process_trial_result` operation took 1.218 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3422740)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_a70dbacd_15_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=11,data_transform=None_2025-02-12_10-33-04/checkpoint_000011)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "2025-02-12 10:35:10,005\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.221 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:10,008\tWARNING util.py:201 -- The `process_trial_result` operation took 2.223 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:10,008\tWARNING util.py:201 -- Processing trial results took 2.224 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:35:10,009\tWARNING util.py:201 -- The `process_trial_result` operation took 2.224 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:11,008\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 0.996 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:11,010\tWARNING util.py:201 -- The `process_trial_result` operation took 0.998 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:11,011\tWARNING util.py:201 -- Processing trial results took 0.999 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:35:11,011\tWARNING util.py:201 -- The `process_trial_result` operation took 0.999 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:12,254\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.240 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:12,256\tWARNING util.py:201 -- The `process_trial_result` operation took 1.242 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:12,257\tWARNING util.py:201 -- Processing trial results took 1.243 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:35:12,257\tWARNING util.py:201 -- The `process_trial_result` operation took 1.243 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:13,792\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.501 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:13,794\tWARNING util.py:201 -- The `process_trial_result` operation took 1.504 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:13,795\tWARNING util.py:201 -- Processing trial results took 1.504 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:35:13,796\tWARNING util.py:201 -- The `process_trial_result` operation took 1.505 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3426945)\u001b[0m Seed set to 12\n",
      "\u001b[36m(train_nn_reg pid=3417235)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_4450d409_11_batch_size=128,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0031,data_trans_2025-02-12_10-29-59/checkpoint_000036)\u001b[32m [repeated 4x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3426945)\u001b[0m Train: (156, 3241), Test: (65, 3241)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3426945)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3426945)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3426945)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3426945)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_e03fa800_16_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=19,data_transform=None_2025-02-12_10-35-06 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3426945)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3426945)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3426945)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3426945)\u001b[0m 0 | layers | ModuleList | 1.3 M  | train\n",
      "\u001b[36m(train_nn_reg pid=3426945)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3426945)\u001b[0m 1.3 M     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3426945)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3426945)\u001b[0m 1.3 M     Total params\n",
      "\u001b[36m(train_nn_reg pid=3426945)\u001b[0m 5.018     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3426945)\u001b[0m 14        Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3426945)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3426945)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(train_nn_reg pid=3426945)\u001b[0m Metric train_rmse does not exist in `trainer.callback_metrics.\n",
      "\u001b[36m(train_nn_reg pid=3426945)\u001b[0m Metric train_r2 does not exist in `trainer.callback_metrics.\n",
      "\u001b[36m(train_nn_reg pid=3426945)\u001b[0m Metric train_loss does not exist in `trainer.callback_metrics.\n",
      "2025-02-12 10:35:16,590\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.379 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:16,592\tWARNING util.py:201 -- The `process_trial_result` operation took 1.382 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:16,592\tWARNING util.py:201 -- Processing trial results took 1.382 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:35:16,593\tWARNING util.py:201 -- The `process_trial_result` operation took 1.383 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:18,655\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.061 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:18,657\tWARNING util.py:201 -- The `process_trial_result` operation took 2.063 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:18,658\tWARNING util.py:201 -- Processing trial results took 2.064 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:35:18,659\tWARNING util.py:201 -- The `process_trial_result` operation took 2.064 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:18,672\tWARNING syncer.py:406 -- Last sync command failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 404, in _launch_sync_process\n",
      "    self.wait()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 474, in wait\n",
      "    raise e\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 472, in wait\n",
      "    self._sync_process.wait(timeout=timeout or self.sync_timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 173, in wait\n",
      "    raise exception\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 136, in entrypoint\n",
      "    result = self._fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/storage.py\", line 218, in _upload_to_fs_path\n",
      "    _pyarrow_fs_copy_files(local_path, fs_path, destination_filesystem=fs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/storage.py\", line 116, in _pyarrow_fs_copy_files\n",
      "    return pyarrow.fs.copy_files(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/pyarrow/fs.py\", line 259, in copy_files\n",
      "    _copy_files_selector(source_fs, source_sel,\n",
      "  File \"pyarrow/_fs.pyx\", line 1624, in pyarrow._fs._copy_files_selector\n",
      "  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\n",
      "FileNotFoundError: [Errno 2] Failed to open local file '/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_a70dbacd_15_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=11,data_transform=None_2025-02-12_10-33-04/epoch=9-val_rmse=15.88.ckpt'. Detail: [errno 2] No such file or directory\n",
      "\n",
      "2025-02-12 10:35:21,359\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.547 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:21,361\tWARNING util.py:201 -- The `process_trial_result` operation took 2.549 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:21,362\tWARNING util.py:201 -- Processing trial results took 2.550 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:35:21,362\tWARNING util.py:201 -- The `process_trial_result` operation took 2.550 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:22,508\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.144 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:22,510\tWARNING util.py:201 -- The `process_trial_result` operation took 1.146 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:22,511\tWARNING util.py:201 -- Processing trial results took 1.147 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:35:22,511\tWARNING util.py:201 -- The `process_trial_result` operation took 1.148 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3426945)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_e03fa800_16_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=19,data_transform=None_2025-02-12_10-35-06/checkpoint_000001)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "2025-02-12 10:35:23,689\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.175 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:23,691\tWARNING util.py:201 -- The `process_trial_result` operation took 1.178 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:23,693\tWARNING util.py:201 -- Processing trial results took 1.179 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:35:23,693\tWARNING util.py:201 -- The `process_trial_result` operation took 1.179 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:25,183\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.477 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:25,185\tWARNING util.py:201 -- The `process_trial_result` operation took 1.480 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:25,186\tWARNING util.py:201 -- Processing trial results took 1.480 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:35:25,186\tWARNING util.py:201 -- The `process_trial_result` operation took 1.480 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:27,059\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.871 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:27,061\tWARNING util.py:201 -- The `process_trial_result` operation took 1.873 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:27,062\tWARNING util.py:201 -- Processing trial results took 1.874 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:35:27,062\tWARNING util.py:201 -- The `process_trial_result` operation took 1.875 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:29,063\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.557 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:29,065\tWARNING util.py:201 -- The `process_trial_result` operation took 1.559 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:29,066\tWARNING util.py:201 -- Processing trial results took 1.560 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:35:29,066\tWARNING util.py:201 -- The `process_trial_result` operation took 1.560 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:29,080\tWARNING syncer.py:406 -- Last sync command failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 404, in _launch_sync_process\n",
      "    self.wait()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 474, in wait\n",
      "    raise e\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 472, in wait\n",
      "    self._sync_process.wait(timeout=timeout or self.sync_timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 173, in wait\n",
      "    raise exception\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 136, in entrypoint\n",
      "    result = self._fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/storage.py\", line 218, in _upload_to_fs_path\n",
      "    _pyarrow_fs_copy_files(local_path, fs_path, destination_filesystem=fs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/storage.py\", line 116, in _pyarrow_fs_copy_files\n",
      "    return pyarrow.fs.copy_files(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/pyarrow/fs.py\", line 259, in copy_files\n",
      "    _copy_files_selector(source_fs, source_sel,\n",
      "  File \"pyarrow/_fs.pyx\", line 1624, in pyarrow._fs._copy_files_selector\n",
      "  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\n",
      "FileNotFoundError: [Errno 2] Failed to open local file '/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_b9035250_4_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0002,data_transf_2025-02-12_10-27-48/epoch=52-val_rmse=13.97.ckpt'. Detail: [errno 2] No such file or directory\n",
      "\n",
      "2025-02-12 10:35:31,342\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.249 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:31,344\tWARNING util.py:201 -- The `process_trial_result` operation took 2.251 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:31,345\tWARNING util.py:201 -- Processing trial results took 2.252 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:35:31,345\tWARNING util.py:201 -- The `process_trial_result` operation took 2.252 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:32,813\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.466 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:32,815\tWARNING util.py:201 -- The `process_trial_result` operation took 1.468 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:32,816\tWARNING util.py:201 -- Processing trial results took 1.469 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:35:32,816\tWARNING util.py:201 -- The `process_trial_result` operation took 1.470 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3422740)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_a70dbacd_15_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=11,data_transform=None_2025-02-12_10-33-04/checkpoint_000014)\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "2025-02-12 10:35:34,254\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.423 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:34,256\tWARNING util.py:201 -- The `process_trial_result` operation took 1.425 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:34,257\tWARNING util.py:201 -- Processing trial results took 1.426 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:35:34,257\tWARNING util.py:201 -- The `process_trial_result` operation took 1.426 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:35,324\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.043 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:35,327\tWARNING util.py:201 -- The `process_trial_result` operation took 1.046 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:35,327\tWARNING util.py:201 -- Processing trial results took 1.046 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:35:35,328\tWARNING util.py:201 -- The `process_trial_result` operation took 1.047 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:36,917\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.588 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:36,919\tWARNING util.py:201 -- The `process_trial_result` operation took 1.590 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:36,920\tWARNING util.py:201 -- Processing trial results took 1.591 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:35:36,920\tWARNING util.py:201 -- The `process_trial_result` operation took 1.591 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:38,756\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.834 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:38,758\tWARNING util.py:201 -- The `process_trial_result` operation took 1.837 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:38,759\tWARNING util.py:201 -- Processing trial results took 1.838 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:35:38,759\tWARNING util.py:201 -- The `process_trial_result` operation took 1.838 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3412418)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_b9035250_4_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0002,data_transf_2025-02-12_10-27-48/checkpoint_000056)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2025-02-12 10:35:40,448\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.655 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:40,450\tWARNING util.py:201 -- The `process_trial_result` operation took 1.658 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:40,451\tWARNING util.py:201 -- Processing trial results took 1.658 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:35:40,451\tWARNING util.py:201 -- The `process_trial_result` operation took 1.659 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:42,991\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.514 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:42,993\tWARNING util.py:201 -- The `process_trial_result` operation took 2.516 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:42,994\tWARNING util.py:201 -- Processing trial results took 2.517 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:35:42,994\tWARNING util.py:201 -- The `process_trial_result` operation took 2.517 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:44,693\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.652 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:44,695\tWARNING util.py:201 -- The `process_trial_result` operation took 1.654 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:44,696\tWARNING util.py:201 -- Processing trial results took 1.655 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:35:44,696\tWARNING util.py:201 -- The `process_trial_result` operation took 1.656 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:46,526\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.828 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:46,529\tWARNING util.py:201 -- The `process_trial_result` operation took 1.831 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:46,529\tWARNING util.py:201 -- Processing trial results took 1.831 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:35:46,530\tWARNING util.py:201 -- The `process_trial_result` operation took 1.832 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3417235)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_4450d409_11_batch_size=128,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0031,data_trans_2025-02-12_10-29-59/checkpoint_000040)\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "2025-02-12 10:35:47,732\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.175 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:47,734\tWARNING util.py:201 -- The `process_trial_result` operation took 1.177 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:47,735\tWARNING util.py:201 -- Processing trial results took 1.178 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:35:47,735\tWARNING util.py:201 -- The `process_trial_result` operation took 1.178 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:49,531\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.794 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:49,533\tWARNING util.py:201 -- The `process_trial_result` operation took 1.797 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:49,534\tWARNING util.py:201 -- Processing trial results took 1.798 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:35:49,534\tWARNING util.py:201 -- The `process_trial_result` operation took 1.798 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:50,863\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.304 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:50,865\tWARNING util.py:201 -- The `process_trial_result` operation took 1.306 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:50,866\tWARNING util.py:201 -- Processing trial results took 1.307 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:35:50,866\tWARNING util.py:201 -- The `process_trial_result` operation took 1.307 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:53,305\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.399 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:53,308\tWARNING util.py:201 -- The `process_trial_result` operation took 2.402 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:53,308\tWARNING util.py:201 -- Processing trial results took 2.402 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:35:53,308\tWARNING util.py:201 -- The `process_trial_result` operation took 2.402 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:54,450\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.128 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:54,452\tWARNING util.py:201 -- The `process_trial_result` operation took 1.130 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:54,452\tWARNING util.py:201 -- Processing trial results took 1.130 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:35:54,452\tWARNING util.py:201 -- The `process_trial_result` operation took 1.131 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:55,422\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 0.968 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:55,424\tWARNING util.py:201 -- The `process_trial_result` operation took 0.970 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:55,425\tWARNING util.py:201 -- Processing trial results took 0.971 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:35:55,425\tWARNING util.py:201 -- The `process_trial_result` operation took 0.971 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:56,970\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.543 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:56,972\tWARNING util.py:201 -- The `process_trial_result` operation took 1.546 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:56,973\tWARNING util.py:201 -- Processing trial results took 1.546 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:35:56,974\tWARNING util.py:201 -- The `process_trial_result` operation took 1.547 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3412418)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_b9035250_4_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0002,data_transf_2025-02-12_10-27-48/checkpoint_000058)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "2025-02-12 10:35:58,891\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.494 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:58,894\tWARNING util.py:201 -- The `process_trial_result` operation took 1.497 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:35:58,894\tWARNING util.py:201 -- Processing trial results took 1.497 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:35:58,895\tWARNING util.py:201 -- The `process_trial_result` operation took 1.498 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:00,388\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.480 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:00,391\tWARNING util.py:201 -- The `process_trial_result` operation took 1.483 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:00,391\tWARNING util.py:201 -- Processing trial results took 1.484 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:36:00,392\tWARNING util.py:201 -- The `process_trial_result` operation took 1.484 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:01,323\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 0.930 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:01,325\tWARNING util.py:201 -- The `process_trial_result` operation took 0.932 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:01,326\tWARNING util.py:201 -- Processing trial results took 0.933 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:36:01,326\tWARNING util.py:201 -- The `process_trial_result` operation took 0.933 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:01,340\tWARNING syncer.py:406 -- Last sync command failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 404, in _launch_sync_process\n",
      "    self.wait()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 474, in wait\n",
      "    raise e\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 472, in wait\n",
      "    self._sync_process.wait(timeout=timeout or self.sync_timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 173, in wait\n",
      "    raise exception\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 136, in entrypoint\n",
      "    result = self._fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/storage.py\", line 218, in _upload_to_fs_path\n",
      "    _pyarrow_fs_copy_files(local_path, fs_path, destination_filesystem=fs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/storage.py\", line 116, in _pyarrow_fs_copy_files\n",
      "    return pyarrow.fs.copy_files(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/pyarrow/fs.py\", line 259, in copy_files\n",
      "    _copy_files_selector(source_fs, source_sel,\n",
      "  File \"pyarrow/_fs.pyx\", line 1624, in pyarrow._fs._copy_files_selector\n",
      "  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\n",
      "FileNotFoundError: [Errno 2] Failed to open local file '/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_b9035250_4_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0002,data_transf_2025-02-12_10-27-48/epoch=55-val_rmse=13.49.ckpt'. Detail: [errno 2] No such file or directory\n",
      "\n",
      "2025-02-12 10:36:04,436\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 3.093 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:04,438\tWARNING util.py:201 -- The `process_trial_result` operation took 3.096 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:04,439\tWARNING util.py:201 -- Processing trial results took 3.097 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:36:04,439\tWARNING util.py:201 -- The `process_trial_result` operation took 3.097 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3412418)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_b9035250_4_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0002,data_transf_2025-02-12_10-27-48/checkpoint_000059)\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "2025-02-12 10:36:06,405\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.902 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:06,408\tWARNING util.py:201 -- The `process_trial_result` operation took 1.905 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:06,409\tWARNING util.py:201 -- Processing trial results took 1.906 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:36:06,409\tWARNING util.py:201 -- The `process_trial_result` operation took 1.906 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:07,549\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.138 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:07,551\tWARNING util.py:201 -- The `process_trial_result` operation took 1.141 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:07,551\tWARNING util.py:201 -- Processing trial results took 1.141 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:36:07,552\tWARNING util.py:201 -- The `process_trial_result` operation took 1.142 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:09,613\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.047 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:09,616\tWARNING util.py:201 -- The `process_trial_result` operation took 2.049 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:09,616\tWARNING util.py:201 -- Processing trial results took 2.050 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:36:09,617\tWARNING util.py:201 -- The `process_trial_result` operation took 2.050 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3417235)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_4450d409_11_batch_size=128,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0031,data_trans_2025-02-12_10-29-59/checkpoint_000043)\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "2025-02-12 10:36:11,448\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.797 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:11,450\tWARNING util.py:201 -- The `process_trial_result` operation took 1.799 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:11,450\tWARNING util.py:201 -- Processing trial results took 1.799 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:36:11,451\tWARNING util.py:201 -- The `process_trial_result` operation took 1.800 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:13,960\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.494 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:13,962\tWARNING util.py:201 -- The `process_trial_result` operation took 2.497 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:13,962\tWARNING util.py:201 -- Processing trial results took 2.497 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:36:13,963\tWARNING util.py:201 -- The `process_trial_result` operation took 2.498 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:15,129\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.165 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:15,132\tWARNING util.py:201 -- The `process_trial_result` operation took 1.168 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:15,132\tWARNING util.py:201 -- Processing trial results took 1.168 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:36:15,132\tWARNING util.py:201 -- The `process_trial_result` operation took 1.168 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3412418)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_b9035250_4_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0002,data_transf_2025-02-12_10-27-48/checkpoint_000061)\n",
      "\u001b[36m(train_nn_reg pid=3422740)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_a70dbacd_15_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=11,data_transform=None_2025-02-12_10-33-04/checkpoint_000020)\n",
      "2025-02-12 10:36:17,133\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.964 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:17,135\tWARNING util.py:201 -- The `process_trial_result` operation took 1.967 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:17,135\tWARNING util.py:201 -- Processing trial results took 1.967 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:36:17,136\tWARNING util.py:201 -- The `process_trial_result` operation took 1.968 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:18,081\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 0.944 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:18,083\tWARNING util.py:201 -- The `process_trial_result` operation took 0.946 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:18,084\tWARNING util.py:201 -- Processing trial results took 0.947 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:36:18,084\tWARNING util.py:201 -- The `process_trial_result` operation took 0.947 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:19,235\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.149 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:19,237\tWARNING util.py:201 -- The `process_trial_result` operation took 1.151 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:19,238\tWARNING util.py:201 -- Processing trial results took 1.152 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:36:19,238\tWARNING util.py:201 -- The `process_trial_result` operation took 1.153 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3422740)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_a70dbacd_15_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=11,data_transform=None_2025-02-12_10-33-04/checkpoint_000021)\n",
      "2025-02-12 10:36:20,591\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.340 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:20,594\tWARNING util.py:201 -- The `process_trial_result` operation took 1.342 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:20,594\tWARNING util.py:201 -- Processing trial results took 1.343 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:36:20,594\tWARNING util.py:201 -- The `process_trial_result` operation took 1.343 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3412418)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_b9035250_4_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0002,data_transf_2025-02-12_10-27-48/checkpoint_000062)\n",
      "\u001b[36m(train_nn_reg pid=3413652)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_e26bf114_8_batch_size=128,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0018,data_transf_2025-02-12_10-28-19/checkpoint_000050)\n",
      "2025-02-12 10:36:21,966\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.328 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:21,968\tWARNING util.py:201 -- The `process_trial_result` operation took 1.331 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:21,969\tWARNING util.py:201 -- Processing trial results took 1.331 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:36:21,969\tWARNING util.py:201 -- The `process_trial_result` operation took 1.331 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:24,383\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.402 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:24,385\tWARNING util.py:201 -- The `process_trial_result` operation took 2.404 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:24,386\tWARNING util.py:201 -- Processing trial results took 2.405 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:36:24,386\tWARNING util.py:201 -- The `process_trial_result` operation took 2.405 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:25,641\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.239 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:25,644\tWARNING util.py:201 -- The `process_trial_result` operation took 1.242 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:25,645\tWARNING util.py:201 -- Processing trial results took 1.243 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:36:25,645\tWARNING util.py:201 -- The `process_trial_result` operation took 1.243 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:26,692\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.022 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:26,695\tWARNING util.py:201 -- The `process_trial_result` operation took 1.025 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:26,695\tWARNING util.py:201 -- Processing trial results took 1.026 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:36:26,696\tWARNING util.py:201 -- The `process_trial_result` operation took 1.026 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3413652)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_e26bf114_8_batch_size=128,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0018,data_transf_2025-02-12_10-28-19/checkpoint_000051)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "2025-02-12 10:36:28,115\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.402 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:28,118\tWARNING util.py:201 -- The `process_trial_result` operation took 1.405 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:28,119\tWARNING util.py:201 -- Processing trial results took 1.405 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:36:28,119\tWARNING util.py:201 -- The `process_trial_result` operation took 1.406 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:29,928\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.807 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:29,930\tWARNING util.py:201 -- The `process_trial_result` operation took 1.810 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:29,931\tWARNING util.py:201 -- Processing trial results took 1.811 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:36:29,931\tWARNING util.py:201 -- The `process_trial_result` operation took 1.811 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:31,468\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.513 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:31,471\tWARNING util.py:201 -- The `process_trial_result` operation took 1.516 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:31,472\tWARNING util.py:201 -- Processing trial results took 1.517 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:36:31,473\tWARNING util.py:201 -- The `process_trial_result` operation took 1.518 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:33,285\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.799 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:33,287\tWARNING util.py:201 -- The `process_trial_result` operation took 1.801 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:33,288\tWARNING util.py:201 -- Processing trial results took 1.802 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:36:33,288\tWARNING util.py:201 -- The `process_trial_result` operation took 1.802 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3417235)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_4450d409_11_batch_size=128,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0031,data_trans_2025-02-12_10-29-59/checkpoint_000046)\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "2025-02-12 10:36:35,855\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.535 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:35,857\tWARNING util.py:201 -- The `process_trial_result` operation took 2.538 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:35,858\tWARNING util.py:201 -- Processing trial results took 2.539 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:36:35,859\tWARNING util.py:201 -- The `process_trial_result` operation took 2.540 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:37,689\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.828 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:37,691\tWARNING util.py:201 -- The `process_trial_result` operation took 1.830 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:37,692\tWARNING util.py:201 -- Processing trial results took 1.831 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:36:37,693\tWARNING util.py:201 -- The `process_trial_result` operation took 1.832 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:39,111\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.387 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:39,113\tWARNING util.py:201 -- The `process_trial_result` operation took 1.390 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:39,114\tWARNING util.py:201 -- Processing trial results took 1.390 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:36:39,114\tWARNING util.py:201 -- The `process_trial_result` operation took 1.391 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3412418)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_b9035250_4_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0002,data_transf_2025-02-12_10-27-48/checkpoint_000065)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "2025-02-12 10:36:40,845\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.700 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:40,847\tWARNING util.py:201 -- The `process_trial_result` operation took 1.703 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:40,848\tWARNING util.py:201 -- Processing trial results took 1.704 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:36:40,848\tWARNING util.py:201 -- The `process_trial_result` operation took 1.704 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:41,798\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 0.949 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:41,800\tWARNING util.py:201 -- The `process_trial_result` operation took 0.951 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:41,801\tWARNING util.py:201 -- Processing trial results took 0.952 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:36:41,802\tWARNING util.py:201 -- The `process_trial_result` operation took 0.953 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:43,886\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.065 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:43,888\tWARNING util.py:201 -- The `process_trial_result` operation took 2.067 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:43,889\tWARNING util.py:201 -- Processing trial results took 2.068 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:36:43,890\tWARNING util.py:201 -- The `process_trial_result` operation took 2.069 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:46,768\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.852 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:46,770\tWARNING util.py:201 -- The `process_trial_result` operation took 2.855 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:46,772\tWARNING util.py:201 -- Processing trial results took 2.856 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:36:46,772\tWARNING util.py:201 -- The `process_trial_result` operation took 2.857 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3422740)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_a70dbacd_15_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=11,data_transform=None_2025-02-12_10-33-04/checkpoint_000025)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "2025-02-12 10:36:48,453\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.595 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:48,455\tWARNING util.py:201 -- The `process_trial_result` operation took 1.597 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:48,456\tWARNING util.py:201 -- Processing trial results took 1.598 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:36:48,456\tWARNING util.py:201 -- The `process_trial_result` operation took 1.598 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:50,782\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.324 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:50,784\tWARNING util.py:201 -- The `process_trial_result` operation took 2.326 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:50,785\tWARNING util.py:201 -- Processing trial results took 2.327 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:36:50,785\tWARNING util.py:201 -- The `process_trial_result` operation took 2.327 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:52,365\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.539 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:52,369\tWARNING util.py:201 -- The `process_trial_result` operation took 1.542 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:52,369\tWARNING util.py:201 -- Processing trial results took 1.543 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:36:52,370\tWARNING util.py:201 -- The `process_trial_result` operation took 1.544 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:53,343\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 0.972 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:53,345\tWARNING util.py:201 -- The `process_trial_result` operation took 0.974 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:53,346\tWARNING util.py:201 -- Processing trial results took 0.975 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:36:53,346\tWARNING util.py:201 -- The `process_trial_result` operation took 0.975 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:54,505\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.157 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:54,508\tWARNING util.py:201 -- The `process_trial_result` operation took 1.160 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:54,508\tWARNING util.py:201 -- Processing trial results took 1.161 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:36:54,508\tWARNING util.py:201 -- The `process_trial_result` operation took 1.161 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3422740)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_a70dbacd_15_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=11,data_transform=None_2025-02-12_10-33-04/checkpoint_000026)\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "2025-02-12 10:36:57,753\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 3.212 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:57,756\tWARNING util.py:201 -- The `process_trial_result` operation took 3.215 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:57,756\tWARNING util.py:201 -- Processing trial results took 3.215 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:36:57,757\tWARNING util.py:201 -- The `process_trial_result` operation took 3.216 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:58,751\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 0.980 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:58,753\tWARNING util.py:201 -- The `process_trial_result` operation took 0.982 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:36:58,754\tWARNING util.py:201 -- Processing trial results took 0.983 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:36:58,754\tWARNING util.py:201 -- The `process_trial_result` operation took 0.983 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:01,232\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.398 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:01,234\tWARNING util.py:201 -- The `process_trial_result` operation took 2.400 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:01,235\tWARNING util.py:201 -- Processing trial results took 2.401 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:37:01,235\tWARNING util.py:201 -- The `process_trial_result` operation took 2.402 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:03,295\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.058 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:03,297\tWARNING util.py:201 -- The `process_trial_result` operation took 2.061 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:03,298\tWARNING util.py:201 -- Processing trial results took 2.061 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:37:03,298\tWARNING util.py:201 -- The `process_trial_result` operation took 2.062 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:04,469\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.158 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:04,471\tWARNING util.py:201 -- The `process_trial_result` operation took 1.160 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:04,472\tWARNING util.py:201 -- Processing trial results took 1.161 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:37:04,473\tWARNING util.py:201 -- The `process_trial_result` operation took 1.162 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3426945)\u001b[0m `Trainer.fit` stopped: `max_epochs=10` reached.\n",
      "\u001b[36m(train_nn_reg pid=3417235)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_4450d409_11_batch_size=128,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0031,data_trans_2025-02-12_10-29-59/checkpoint_000050)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "2025-02-12 10:37:06,057\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.547 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:06,059\tWARNING util.py:201 -- The `process_trial_result` operation took 1.550 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:06,060\tWARNING util.py:201 -- Processing trial results took 1.551 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:37:06,060\tWARNING util.py:201 -- The `process_trial_result` operation took 1.551 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:06,074\tWARNING syncer.py:406 -- Last sync command failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 404, in _launch_sync_process\n",
      "    self.wait()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 474, in wait\n",
      "    raise e\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 472, in wait\n",
      "    self._sync_process.wait(timeout=timeout or self.sync_timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 173, in wait\n",
      "    raise exception\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 136, in entrypoint\n",
      "    result = self._fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/storage.py\", line 218, in _upload_to_fs_path\n",
      "    _pyarrow_fs_copy_files(local_path, fs_path, destination_filesystem=fs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/storage.py\", line 116, in _pyarrow_fs_copy_files\n",
      "    return pyarrow.fs.copy_files(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/pyarrow/fs.py\", line 259, in copy_files\n",
      "    _copy_files_selector(source_fs, source_sel,\n",
      "  File \"pyarrow/_fs.pyx\", line 1624, in pyarrow._fs._copy_files_selector\n",
      "  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\n",
      "FileNotFoundError: [Errno 2] Failed to open local file '/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_a70dbacd_15_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=11,data_transform=None_2025-02-12_10-33-04/epoch=24-val_rmse=2.82.ckpt'. Detail: [errno 2] No such file or directory\n",
      "\n",
      "2025-02-12 10:37:08,839\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.763 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:08,841\tWARNING util.py:201 -- The `process_trial_result` operation took 2.765 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:08,841\tWARNING util.py:201 -- Processing trial results took 2.766 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:37:08,842\tWARNING util.py:201 -- The `process_trial_result` operation took 2.766 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:11,998\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 3.131 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:12,001\tWARNING util.py:201 -- The `process_trial_result` operation took 3.133 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:12,001\tWARNING util.py:201 -- Processing trial results took 3.134 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:37:12,002\tWARNING util.py:201 -- The `process_trial_result` operation took 3.134 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:14,205\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.345 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:14,207\tWARNING util.py:201 -- The `process_trial_result` operation took 1.347 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:14,208\tWARNING util.py:201 -- Processing trial results took 1.348 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:37:14,209\tWARNING util.py:201 -- The `process_trial_result` operation took 1.348 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:15,216\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 0.991 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:15,218\tWARNING util.py:201 -- The `process_trial_result` operation took 0.994 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:15,219\tWARNING util.py:201 -- Processing trial results took 0.995 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:37:15,219\tWARNING util.py:201 -- The `process_trial_result` operation took 0.995 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3412418)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_b9035250_4_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0002,data_transf_2025-02-12_10-27-48/checkpoint_000068)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "2025-02-12 10:37:17,006\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.412 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:17,007\tWARNING util.py:201 -- The `process_trial_result` operation took 1.414 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:17,008\tWARNING util.py:201 -- Processing trial results took 1.415 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:37:17,009\tWARNING util.py:201 -- The `process_trial_result` operation took 1.416 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:19,318\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.280 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:19,320\tWARNING util.py:201 -- The `process_trial_result` operation took 2.283 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:19,321\tWARNING util.py:201 -- Processing trial results took 2.283 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:37:19,321\tWARNING util.py:201 -- The `process_trial_result` operation took 2.284 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:20,450\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.114 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:20,452\tWARNING util.py:201 -- The `process_trial_result` operation took 1.116 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:20,453\tWARNING util.py:201 -- Processing trial results took 1.117 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:37:20,454\tWARNING util.py:201 -- The `process_trial_result` operation took 1.118 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3412418)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_b9035250_4_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0002,data_transf_2025-02-12_10-27-48/checkpoint_000070)\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_nn_reg pid=3431251)\u001b[0m Seed set to 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3431251)\u001b[0m Train: (156, 52), Test: (65, 52)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3431251)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3431251)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3431251)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3431251)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_885d4181_17_batch_size=64,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0002,data_transf_2025-02-12_10-37-12 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3431251)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3431251)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3431251)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3431251)\u001b[0m 0 | layers | ModuleList | 274 K  | train\n",
      "\u001b[36m(train_nn_reg pid=3431251)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3431251)\u001b[0m 274 K     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3431251)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3431251)\u001b[0m 274 K     Total params\n",
      "\u001b[36m(train_nn_reg pid=3431251)\u001b[0m 1.098     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3431251)\u001b[0m 14        Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3431251)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3431251)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3431251)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-02-12 10:37:23,575\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 0.991 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:23,577\tWARNING util.py:201 -- The `process_trial_result` operation took 0.993 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:23,578\tWARNING util.py:201 -- Processing trial results took 0.994 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:37:23,578\tWARNING util.py:201 -- The `process_trial_result` operation took 0.995 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:23,582\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_885d4181\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3431251, ip=172.31.181.85, actor_id=541b5b041e79665177ce231301000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1024, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1053, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 1) cannot be concatenated\n",
      "2025-02-12 10:37:25,525\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.798 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:25,528\tWARNING util.py:201 -- The `process_trial_result` operation took 1.800 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:25,528\tWARNING util.py:201 -- Processing trial results took 1.801 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:37:25,529\tWARNING util.py:201 -- The `process_trial_result` operation took 1.801 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3413652)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_e26bf114_8_batch_size=128,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0018,data_transf_2025-02-12_10-28-19/checkpoint_000057)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "2025-02-12 10:37:26,874\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.317 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:26,876\tWARNING util.py:201 -- The `process_trial_result` operation took 1.319 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:26,877\tWARNING util.py:201 -- Processing trial results took 1.319 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:37:26,877\tWARNING util.py:201 -- The `process_trial_result` operation took 1.319 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:28,012\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.134 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:28,014\tWARNING util.py:201 -- The `process_trial_result` operation took 1.136 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:28,015\tWARNING util.py:201 -- Processing trial results took 1.137 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:37:28,016\tWARNING util.py:201 -- The `process_trial_result` operation took 1.138 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:28,029\tWARNING syncer.py:406 -- Last sync command failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 404, in _launch_sync_process\n",
      "    self.wait()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 474, in wait\n",
      "    raise e\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 472, in wait\n",
      "    self._sync_process.wait(timeout=timeout or self.sync_timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 173, in wait\n",
      "    raise exception\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 136, in entrypoint\n",
      "    result = self._fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/storage.py\", line 218, in _upload_to_fs_path\n",
      "    _pyarrow_fs_copy_files(local_path, fs_path, destination_filesystem=fs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/storage.py\", line 116, in _pyarrow_fs_copy_files\n",
      "    return pyarrow.fs.copy_files(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/pyarrow/fs.py\", line 259, in copy_files\n",
      "    _copy_files_selector(source_fs, source_sel,\n",
      "  File \"pyarrow/_fs.pyx\", line 1624, in pyarrow._fs._copy_files_selector\n",
      "  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\n",
      "FileNotFoundError: [Errno 2] Failed to open local file '/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_b9035250_4_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0002,data_transf_2025-02-12_10-27-48/epoch=67-val_rmse=11.32.ckpt'. Detail: [errno 2] No such file or directory\n",
      "\n",
      "2025-02-12 10:37:31,306\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 3.236 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:31,308\tWARNING util.py:201 -- The `process_trial_result` operation took 3.238 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:31,310\tWARNING util.py:201 -- Processing trial results took 3.239 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:37:31,310\tWARNING util.py:201 -- The `process_trial_result` operation took 3.240 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:32,436\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.110 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:32,438\tWARNING util.py:201 -- The `process_trial_result` operation took 1.113 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:32,438\tWARNING util.py:201 -- Processing trial results took 1.113 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:37:32,439\tWARNING util.py:201 -- The `process_trial_result` operation took 1.114 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:33,559\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.118 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:33,561\tWARNING util.py:201 -- The `process_trial_result` operation took 1.120 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:33,562\tWARNING util.py:201 -- Processing trial results took 1.122 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:37:33,563\tWARNING util.py:201 -- The `process_trial_result` operation took 1.122 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3412418)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_b9035250_4_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0002,data_transf_2025-02-12_10-27-48/checkpoint_000072)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_nn_reg pid=3431740)\u001b[0m Seed set to 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3431740)\u001b[0m Train: (156, 1173), Test: (65, 1173)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3431740)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3431740)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3431740)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3431740)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_f82534bf_18_batch_size=32,data_aggregation=None,data_selection=abundance_ith,data_selection_i=5,data_transform=None,e_2025-02-12_10-37-23 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3431740)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3431740)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3431740)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3431740)\u001b[0m 0 | layers | ModuleList | 1.0 M  | train\n",
      "\u001b[36m(train_nn_reg pid=3431740)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3431740)\u001b[0m 1.0 M     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3431740)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3431740)\u001b[0m 1.0 M     Total params\n",
      "\u001b[36m(train_nn_reg pid=3431740)\u001b[0m 4.164     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3431740)\u001b[0m 44        Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3431740)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3431740)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(train_nn_reg pid=3431740)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3431740)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-02-12 10:37:37,494\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.344 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:37,496\tWARNING util.py:201 -- The `process_trial_result` operation took 1.347 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:37,497\tWARNING util.py:201 -- Processing trial results took 1.347 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:37:37,497\tWARNING util.py:201 -- The `process_trial_result` operation took 1.348 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:39,432\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.910 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:39,434\tWARNING util.py:201 -- The `process_trial_result` operation took 1.912 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:39,435\tWARNING util.py:201 -- Processing trial results took 1.912 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:37:39,435\tWARNING util.py:201 -- The `process_trial_result` operation took 1.913 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:39,449\tWARNING syncer.py:406 -- Last sync command failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 404, in _launch_sync_process\n",
      "    self.wait()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 474, in wait\n",
      "    raise e\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 472, in wait\n",
      "    self._sync_process.wait(timeout=timeout or self.sync_timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 173, in wait\n",
      "    raise exception\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 136, in entrypoint\n",
      "    result = self._fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/storage.py\", line 218, in _upload_to_fs_path\n",
      "    _pyarrow_fs_copy_files(local_path, fs_path, destination_filesystem=fs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/storage.py\", line 116, in _pyarrow_fs_copy_files\n",
      "    return pyarrow.fs.copy_files(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/pyarrow/fs.py\", line 259, in copy_files\n",
      "    _copy_files_selector(source_fs, source_sel,\n",
      "  File \"pyarrow/_fs.pyx\", line 1624, in pyarrow._fs._copy_files_selector\n",
      "  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\n",
      "FileNotFoundError: [Errno 2] Failed to open local file '/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_b9035250_4_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0002,data_transf_2025-02-12_10-27-48/epoch=69-val_rmse=10.93.ckpt'. Detail: [errno 2] No such file or directory\n",
      "\n",
      "2025-02-12 10:37:39,452\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_f82534bf\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3431740, ip=172.31.181.85, actor_id=b8f09e9b245c9c6fa61b230101000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1026, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 216, in run\n",
      "    self.advance()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 455, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 151, in run\n",
      "    self.on_advance_end(data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 370, in on_advance_end\n",
      "    self.val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 2) cannot be concatenated\n",
      "\u001b[36m(train_nn_reg pid=3417235)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_4450d409_11_batch_size=128,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0031,data_trans_2025-02-12_10-29-59/checkpoint_000054)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "2025-02-12 10:37:42,475\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.485 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:42,477\tWARNING util.py:201 -- The `process_trial_result` operation took 1.488 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:42,478\tWARNING util.py:201 -- Processing trial results took 1.489 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:37:42,479\tWARNING util.py:201 -- The `process_trial_result` operation took 1.489 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:43,512\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.018 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:43,515\tWARNING util.py:201 -- The `process_trial_result` operation took 1.021 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:43,515\tWARNING util.py:201 -- Processing trial results took 1.021 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:37:43,516\tWARNING util.py:201 -- The `process_trial_result` operation took 1.021 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:44,657\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.127 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:44,659\tWARNING util.py:201 -- The `process_trial_result` operation took 1.129 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:44,659\tWARNING util.py:201 -- Processing trial results took 1.130 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:37:44,660\tWARNING util.py:201 -- The `process_trial_result` operation took 1.130 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:46,518\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.857 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:46,520\tWARNING util.py:201 -- The `process_trial_result` operation took 1.859 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:46,521\tWARNING util.py:201 -- Processing trial results took 1.860 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:37:46,521\tWARNING util.py:201 -- The `process_trial_result` operation took 1.860 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:47,768\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.246 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:47,770\tWARNING util.py:201 -- The `process_trial_result` operation took 1.248 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:47,771\tWARNING util.py:201 -- Processing trial results took 1.249 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:37:47,772\tWARNING util.py:201 -- The `process_trial_result` operation took 1.249 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3432408)\u001b[0m Seed set to 12\n",
      "\u001b[36m(train_nn_reg pid=3413652)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_e26bf114_8_batch_size=128,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0018,data_transf_2025-02-12_10-28-19/checkpoint_000060)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3432408)\u001b[0m Train: (156, 2594), Test: (65, 2594)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3432408)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3432408)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3432408)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3432408)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_db54a0f9_19_batch_size=64,data_aggregation=None,data_selection=abundance_ith,data_selection_i=13,data_transform=None,_2025-02-12_10-37-40 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3432408)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3432408)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3432408)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3432408)\u001b[0m 0 | layers | ModuleList | 655 K  | train\n",
      "\u001b[36m(train_nn_reg pid=3432408)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3432408)\u001b[0m 655 K     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3432408)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3432408)\u001b[0m 655 K     Total params\n",
      "\u001b[36m(train_nn_reg pid=3432408)\u001b[0m 2.622     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3432408)\u001b[0m 4         Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3432408)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3412418)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_b9035250_4_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0002,data_transf_2025-02-12_10-27-48/checkpoint_000073)\n",
      "\u001b[36m(train_nn_reg pid=3432408)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3432408)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-02-12 10:37:50,281\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.154 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:50,283\tWARNING util.py:201 -- The `process_trial_result` operation took 1.157 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:50,284\tWARNING util.py:201 -- Processing trial results took 1.158 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:37:50,284\tWARNING util.py:201 -- The `process_trial_result` operation took 1.158 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:52,796\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.498 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:52,798\tWARNING util.py:201 -- The `process_trial_result` operation took 2.500 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:52,799\tWARNING util.py:201 -- Processing trial results took 2.501 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:37:52,799\tWARNING util.py:201 -- The `process_trial_result` operation took 2.502 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:52,803\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_db54a0f9\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3432408, ip=172.31.181.85, actor_id=060d990be94ce35e62e960fe01000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1024, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1053, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 1) cannot be concatenated\n",
      "2025-02-12 10:37:54,995\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.044 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:54,996\tWARNING util.py:201 -- The `process_trial_result` operation took 2.046 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:54,997\tWARNING util.py:201 -- Processing trial results took 2.047 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:37:54,998\tWARNING util.py:201 -- The `process_trial_result` operation took 2.048 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3422740)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_a70dbacd_15_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=11,data_transform=None_2025-02-12_10-33-04/checkpoint_000033)\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "2025-02-12 10:37:56,245\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.215 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:56,247\tWARNING util.py:201 -- The `process_trial_result` operation took 1.217 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:56,247\tWARNING util.py:201 -- Processing trial results took 1.217 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:37:56,248\tWARNING util.py:201 -- The `process_trial_result` operation took 1.218 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:57,801\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.551 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:57,803\tWARNING util.py:201 -- The `process_trial_result` operation took 1.554 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:57,804\tWARNING util.py:201 -- Processing trial results took 1.554 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:37:57,805\tWARNING util.py:201 -- The `process_trial_result` operation took 1.555 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:59,861\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.985 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:59,864\tWARNING util.py:201 -- The `process_trial_result` operation took 1.988 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:37:59,864\tWARNING util.py:201 -- Processing trial results took 1.988 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:37:59,865\tWARNING util.py:201 -- The `process_trial_result` operation took 1.989 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3432842)\u001b[0m Seed set to 12\n",
      "\u001b[36m(train_nn_reg pid=3417235)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_4450d409_11_batch_size=128,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0031,data_trans_2025-02-12_10-29-59/checkpoint_000056)\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3432842)\u001b[0m Train: (156, 37), Test: (65, 37)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3432842)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3432842)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3432842)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3432842)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_40b434d4_20_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0074,data_trans_2025-02-12_10-37-52 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3432842)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3432842)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3432842)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3432842)\u001b[0m 0 | layers | ModuleList | 439 K  | train\n",
      "\u001b[36m(train_nn_reg pid=3432842)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3432842)\u001b[0m 439 K     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3432842)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3432842)\u001b[0m 439 K     Total params\n",
      "\u001b[36m(train_nn_reg pid=3432842)\u001b[0m 1.758     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3432842)\u001b[0m 44        Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3432842)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3432842)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(train_nn_reg pid=3432842)\u001b[0m Metric train_rmse does not exist in `trainer.callback_metrics.\n",
      "\u001b[36m(train_nn_reg pid=3432842)\u001b[0m Metric train_r2 does not exist in `trainer.callback_metrics.\n",
      "\u001b[36m(train_nn_reg pid=3432842)\u001b[0m Metric train_loss does not exist in `trainer.callback_metrics.\n",
      "2025-02-12 10:38:04,849\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 3.241 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:04,851\tWARNING util.py:201 -- The `process_trial_result` operation took 3.243 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:04,851\tWARNING util.py:201 -- Processing trial results took 3.244 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:38:04,852\tWARNING util.py:201 -- The `process_trial_result` operation took 3.244 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:06,135\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.261 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:06,137\tWARNING util.py:201 -- The `process_trial_result` operation took 1.264 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:06,138\tWARNING util.py:201 -- Processing trial results took 1.265 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:38:06,138\tWARNING util.py:201 -- The `process_trial_result` operation took 1.265 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:07,270\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.130 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:07,272\tWARNING util.py:201 -- The `process_trial_result` operation took 1.133 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:07,273\tWARNING util.py:201 -- Processing trial results took 1.133 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:38:07,273\tWARNING util.py:201 -- The `process_trial_result` operation took 1.134 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:09,139\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.851 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:09,142\tWARNING util.py:201 -- The `process_trial_result` operation took 1.854 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:09,143\tWARNING util.py:201 -- Processing trial results took 1.855 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:38:09,143\tWARNING util.py:201 -- The `process_trial_result` operation took 1.855 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3412418)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_b9035250_4_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0002,data_transf_2025-02-12_10-27-48/checkpoint_000075)\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "2025-02-12 10:38:11,449\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.286 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:11,451\tWARNING util.py:201 -- The `process_trial_result` operation took 2.288 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:11,452\tWARNING util.py:201 -- Processing trial results took 2.289 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:38:11,453\tWARNING util.py:201 -- The `process_trial_result` operation took 2.290 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:12,708\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.243 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:12,710\tWARNING util.py:201 -- The `process_trial_result` operation took 1.246 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:12,711\tWARNING util.py:201 -- Processing trial results took 1.247 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:38:12,711\tWARNING util.py:201 -- The `process_trial_result` operation took 1.247 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:15,981\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 3.244 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:15,983\tWARNING util.py:201 -- The `process_trial_result` operation took 3.246 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:15,984\tWARNING util.py:201 -- Processing trial results took 3.247 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:38:15,985\tWARNING util.py:201 -- The `process_trial_result` operation took 3.247 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3422740)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_a70dbacd_15_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=11,data_transform=None_2025-02-12_10-33-04/checkpoint_000036)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2025-02-12 10:38:17,337\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.328 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:17,339\tWARNING util.py:201 -- The `process_trial_result` operation took 1.331 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:17,340\tWARNING util.py:201 -- Processing trial results took 1.331 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:38:17,340\tWARNING util.py:201 -- The `process_trial_result` operation took 1.332 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:18,991\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.649 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:18,993\tWARNING util.py:201 -- The `process_trial_result` operation took 1.652 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:18,994\tWARNING util.py:201 -- Processing trial results took 1.652 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:38:18,995\tWARNING util.py:201 -- The `process_trial_result` operation took 1.653 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:20,742\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.732 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:20,745\tWARNING util.py:201 -- The `process_trial_result` operation took 1.734 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:20,745\tWARNING util.py:201 -- Processing trial results took 1.735 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:38:20,746\tWARNING util.py:201 -- The `process_trial_result` operation took 1.735 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:22,397\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.650 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:22,399\tWARNING util.py:201 -- The `process_trial_result` operation took 1.652 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:22,400\tWARNING util.py:201 -- Processing trial results took 1.653 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:38:22,400\tWARNING util.py:201 -- The `process_trial_result` operation took 1.653 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3412418)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_b9035250_4_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0002,data_transf_2025-02-12_10-27-48/checkpoint_000076)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2025-02-12 10:38:26,022\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.859 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:26,024\tWARNING util.py:201 -- The `process_trial_result` operation took 2.861 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:26,025\tWARNING util.py:201 -- Processing trial results took 2.862 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:38:26,025\tWARNING util.py:201 -- The `process_trial_result` operation took 2.863 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:27,086\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.033 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:27,088\tWARNING util.py:201 -- The `process_trial_result` operation took 1.035 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:27,088\tWARNING util.py:201 -- Processing trial results took 1.036 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:38:27,089\tWARNING util.py:201 -- The `process_trial_result` operation took 1.036 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:28,225\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.134 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:28,226\tWARNING util.py:201 -- The `process_trial_result` operation took 1.136 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:28,227\tWARNING util.py:201 -- Processing trial results took 1.137 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:38:28,228\tWARNING util.py:201 -- The `process_trial_result` operation took 1.138 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:29,886\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.656 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:29,888\tWARNING util.py:201 -- The `process_trial_result` operation took 1.659 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:29,888\tWARNING util.py:201 -- Processing trial results took 1.660 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:38:29,889\tWARNING util.py:201 -- The `process_trial_result` operation took 1.660 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:31,122\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.232 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:31,124\tWARNING util.py:201 -- The `process_trial_result` operation took 1.234 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:31,125\tWARNING util.py:201 -- Processing trial results took 1.235 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:38:31,125\tWARNING util.py:201 -- The `process_trial_result` operation took 1.235 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3413652)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_e26bf114_8_batch_size=128,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0018,data_transf_2025-02-12_10-28-19/checkpoint_000065)\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "2025-02-12 10:38:33,065\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.900 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:33,068\tWARNING util.py:201 -- The `process_trial_result` operation took 1.902 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:33,068\tWARNING util.py:201 -- Processing trial results took 1.903 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:38:33,069\tWARNING util.py:201 -- The `process_trial_result` operation took 1.903 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:33,083\tWARNING syncer.py:406 -- Last sync command failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 404, in _launch_sync_process\n",
      "    self.wait()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 474, in wait\n",
      "    raise e\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 472, in wait\n",
      "    self._sync_process.wait(timeout=timeout or self.sync_timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 173, in wait\n",
      "    raise exception\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 136, in entrypoint\n",
      "    result = self._fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/storage.py\", line 218, in _upload_to_fs_path\n",
      "    _pyarrow_fs_copy_files(local_path, fs_path, destination_filesystem=fs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/storage.py\", line 116, in _pyarrow_fs_copy_files\n",
      "    return pyarrow.fs.copy_files(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/pyarrow/fs.py\", line 259, in copy_files\n",
      "    _copy_files_selector(source_fs, source_sel,\n",
      "  File \"pyarrow/_fs.pyx\", line 1624, in pyarrow._fs._copy_files_selector\n",
      "  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\n",
      "FileNotFoundError: [Errno 2] Failed to open local file '/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_40b434d4_20_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0074,data_trans_2025-02-12_10-37-52/epoch=1-val_rmse=18.67.ckpt'. Detail: [errno 2] No such file or directory\n",
      "\n",
      "2025-02-12 10:38:35,846\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.731 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:35,848\tWARNING util.py:201 -- The `process_trial_result` operation took 2.733 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:35,849\tWARNING util.py:201 -- Processing trial results took 2.734 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:38:35,849\tWARNING util.py:201 -- The `process_trial_result` operation took 2.734 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:37,693\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.822 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:37,695\tWARNING util.py:201 -- The `process_trial_result` operation took 1.824 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:37,696\tWARNING util.py:201 -- Processing trial results took 1.825 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:38:37,696\tWARNING util.py:201 -- The `process_trial_result` operation took 1.826 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:38,858\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.146 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:38,860\tWARNING util.py:201 -- The `process_trial_result` operation took 1.148 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:38,861\tWARNING util.py:201 -- Processing trial results took 1.149 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:38:38,861\tWARNING util.py:201 -- The `process_trial_result` operation took 1.149 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:39,990\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.128 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:39,992\tWARNING util.py:201 -- The `process_trial_result` operation took 1.130 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:39,993\tWARNING util.py:201 -- Processing trial results took 1.131 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:38:39,993\tWARNING util.py:201 -- The `process_trial_result` operation took 1.131 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3432842)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_40b434d4_20_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0074,data_trans_2025-02-12_10-37-52/checkpoint_000005)\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "2025-02-12 10:38:41,099\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.090 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:41,101\tWARNING util.py:201 -- The `process_trial_result` operation took 1.092 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:41,102\tWARNING util.py:201 -- Processing trial results took 1.093 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:38:41,102\tWARNING util.py:201 -- The `process_trial_result` operation took 1.093 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:43,038\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.879 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:43,040\tWARNING util.py:201 -- The `process_trial_result` operation took 1.881 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:43,041\tWARNING util.py:201 -- Processing trial results took 1.882 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:38:43,041\tWARNING util.py:201 -- The `process_trial_result` operation took 1.883 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:44,189\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.133 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:44,191\tWARNING util.py:201 -- The `process_trial_result` operation took 1.136 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:44,192\tWARNING util.py:201 -- Processing trial results took 1.136 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:38:44,192\tWARNING util.py:201 -- The `process_trial_result` operation took 1.137 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:44,206\tWARNING syncer.py:406 -- Last sync command failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 404, in _launch_sync_process\n",
      "    self.wait()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 474, in wait\n",
      "    raise e\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 472, in wait\n",
      "    self._sync_process.wait(timeout=timeout or self.sync_timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 173, in wait\n",
      "    raise exception\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 136, in entrypoint\n",
      "    result = self._fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/storage.py\", line 218, in _upload_to_fs_path\n",
      "    _pyarrow_fs_copy_files(local_path, fs_path, destination_filesystem=fs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/storage.py\", line 116, in _pyarrow_fs_copy_files\n",
      "    return pyarrow.fs.copy_files(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/pyarrow/fs.py\", line 259, in copy_files\n",
      "    _copy_files_selector(source_fs, source_sel,\n",
      "  File \"pyarrow/_fs.pyx\", line 1624, in pyarrow._fs._copy_files_selector\n",
      "  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\n",
      "FileNotFoundError: [Errno 2] Failed to open local file '/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_b9035250_4_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0002,data_transf_2025-02-12_10-27-48/epoch=76-val_rmse=9.50.ckpt'. Detail: [errno 2] No such file or directory\n",
      "\n",
      "2025-02-12 10:38:47,050\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.792 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:47,052\tWARNING util.py:201 -- The `process_trial_result` operation took 2.795 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:47,053\tWARNING util.py:201 -- Processing trial results took 2.796 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:38:47,053\tWARNING util.py:201 -- The `process_trial_result` operation took 2.796 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:48,724\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.669 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:48,726\tWARNING util.py:201 -- The `process_trial_result` operation took 1.671 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:48,726\tWARNING util.py:201 -- Processing trial results took 1.672 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:38:48,727\tWARNING util.py:201 -- The `process_trial_result` operation took 1.672 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:49,862\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.119 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:49,863\tWARNING util.py:201 -- The `process_trial_result` operation took 1.122 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:49,864\tWARNING util.py:201 -- Processing trial results took 1.122 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:38:49,864\tWARNING util.py:201 -- The `process_trial_result` operation took 1.122 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3412418)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_b9035250_4_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0002,data_transf_2025-02-12_10-27-48/checkpoint_000081)\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "2025-02-12 10:38:51,712\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.426 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:51,714\tWARNING util.py:201 -- The `process_trial_result` operation took 1.429 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:51,715\tWARNING util.py:201 -- Processing trial results took 1.430 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:38:51,716\tWARNING util.py:201 -- The `process_trial_result` operation took 1.430 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:53,596\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.879 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:53,598\tWARNING util.py:201 -- The `process_trial_result` operation took 1.881 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:53,599\tWARNING util.py:201 -- Processing trial results took 1.882 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:38:53,599\tWARNING util.py:201 -- The `process_trial_result` operation took 1.882 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:53,628\tERROR experiment_state.py:173 -- Saving experiment state to storage at '/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg' failed with exception: \n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/execution/experiment_state.py\", line 171, in wait_for_sync\n",
      "    self._storage.syncer.wait()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 474, in wait\n",
      "    raise e\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 472, in wait\n",
      "    self._sync_process.wait(timeout=timeout or self.sync_timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 173, in wait\n",
      "    raise exception\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 136, in entrypoint\n",
      "    result = self._fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/storage.py\", line 218, in _upload_to_fs_path\n",
      "    _pyarrow_fs_copy_files(local_path, fs_path, destination_filesystem=fs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/storage.py\", line 116, in _pyarrow_fs_copy_files\n",
      "    return pyarrow.fs.copy_files(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/pyarrow/fs.py\", line 259, in copy_files\n",
      "    _copy_files_selector(source_fs, source_sel,\n",
      "  File \"pyarrow/_fs.pyx\", line 1624, in pyarrow._fs._copy_files_selector\n",
      "  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\n",
      "FileNotFoundError: [Errno 2] Failed to open local file '/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_b9035250_4_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0002,data_transf_2025-02-12_10-27-48/epoch=78-val_rmse=9.09.ckpt'. Detail: [errno 2] No such file or directory\n",
      "2025-02-12 10:38:56,643\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.974 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:56,646\tWARNING util.py:201 -- The `process_trial_result` operation took 2.976 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:56,646\tWARNING util.py:201 -- Processing trial results took 2.977 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:38:56,647\tWARNING util.py:201 -- The `process_trial_result` operation took 2.978 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3413652)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_e26bf114_8_batch_size=128,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0018,data_transf_2025-02-12_10-28-19/checkpoint_000068)\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "2025-02-12 10:38:57,975\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.289 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:57,978\tWARNING util.py:201 -- The `process_trial_result` operation took 1.292 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:57,978\tWARNING util.py:201 -- Processing trial results took 1.293 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:38:57,978\tWARNING util.py:201 -- The `process_trial_result` operation took 1.293 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:59,928\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.948 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:59,931\tWARNING util.py:201 -- The `process_trial_result` operation took 1.951 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:38:59,931\tWARNING util.py:201 -- Processing trial results took 1.951 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:38:59,931\tWARNING util.py:201 -- The `process_trial_result` operation took 1.952 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:01,837\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.904 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:01,840\tWARNING util.py:201 -- The `process_trial_result` operation took 1.906 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:01,841\tWARNING util.py:201 -- Processing trial results took 1.907 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:39:01,841\tWARNING util.py:201 -- The `process_trial_result` operation took 1.908 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3432842)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_40b434d4_20_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0074,data_trans_2025-02-12_10-37-52/checkpoint_000008)\n",
      "2025-02-12 10:39:02,959\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.091 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:02,961\tWARNING util.py:201 -- The `process_trial_result` operation took 1.093 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:02,962\tWARNING util.py:201 -- Processing trial results took 1.094 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:39:02,962\tWARNING util.py:201 -- The `process_trial_result` operation took 1.095 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:04,140\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.176 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:04,142\tWARNING util.py:201 -- The `process_trial_result` operation took 1.178 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:04,142\tWARNING util.py:201 -- Processing trial results took 1.179 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:39:04,143\tWARNING util.py:201 -- The `process_trial_result` operation took 1.179 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:07,338\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 3.181 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:07,340\tWARNING util.py:201 -- The `process_trial_result` operation took 3.184 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:07,341\tWARNING util.py:201 -- Processing trial results took 3.184 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:39:07,341\tWARNING util.py:201 -- The `process_trial_result` operation took 3.185 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3412418)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_b9035250_4_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0002,data_transf_2025-02-12_10-27-48/checkpoint_000083)\n",
      "2025-02-12 10:39:09,411\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.568 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:09,413\tWARNING util.py:201 -- The `process_trial_result` operation took 1.571 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:09,414\tWARNING util.py:201 -- Processing trial results took 1.571 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:39:09,415\tWARNING util.py:201 -- The `process_trial_result` operation took 1.572 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:11,304\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.888 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:11,306\tWARNING util.py:201 -- The `process_trial_result` operation took 1.891 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:11,307\tWARNING util.py:201 -- Processing trial results took 1.892 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:39:11,308\tWARNING util.py:201 -- The `process_trial_result` operation took 1.892 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:12,482\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.172 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:12,484\tWARNING util.py:201 -- The `process_trial_result` operation took 1.174 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:12,484\tWARNING util.py:201 -- Processing trial results took 1.175 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:39:12,485\tWARNING util.py:201 -- The `process_trial_result` operation took 1.175 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:14,290\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.779 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:14,292\tWARNING util.py:201 -- The `process_trial_result` operation took 1.782 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:14,292\tWARNING util.py:201 -- Processing trial results took 1.782 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:39:14,292\tWARNING util.py:201 -- The `process_trial_result` operation took 1.783 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:16,764\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.457 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:16,766\tWARNING util.py:201 -- The `process_trial_result` operation took 2.459 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:16,766\tWARNING util.py:201 -- Processing trial results took 2.459 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:39:16,767\tWARNING util.py:201 -- The `process_trial_result` operation took 2.460 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3417235)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_4450d409_11_batch_size=128,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0031,data_trans_2025-02-12_10-29-59/checkpoint_000063)\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "2025-02-12 10:39:18,144\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.364 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:18,146\tWARNING util.py:201 -- The `process_trial_result` operation took 1.367 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:18,147\tWARNING util.py:201 -- Processing trial results took 1.367 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:39:18,147\tWARNING util.py:201 -- The `process_trial_result` operation took 1.368 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:20,068\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.906 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:20,070\tWARNING util.py:201 -- The `process_trial_result` operation took 1.908 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:20,071\tWARNING util.py:201 -- Processing trial results took 1.909 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:39:20,071\tWARNING util.py:201 -- The `process_trial_result` operation took 1.909 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:21,906\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.152 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:21,908\tWARNING util.py:201 -- The `process_trial_result` operation took 1.154 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:21,909\tWARNING util.py:201 -- Processing trial results took 1.156 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:39:21,910\tWARNING util.py:201 -- The `process_trial_result` operation took 1.157 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:23,102\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.189 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:23,103\tWARNING util.py:201 -- The `process_trial_result` operation took 1.191 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:23,104\tWARNING util.py:201 -- Processing trial results took 1.192 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:39:23,105\tWARNING util.py:201 -- The `process_trial_result` operation took 1.193 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3412418)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_b9035250_4_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0002,data_transf_2025-02-12_10-27-48/checkpoint_000085)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "2025-02-12 10:39:24,426\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.297 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:24,428\tWARNING util.py:201 -- The `process_trial_result` operation took 1.299 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:24,429\tWARNING util.py:201 -- Processing trial results took 1.300 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:39:24,429\tWARNING util.py:201 -- The `process_trial_result` operation took 1.300 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:27,336\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.838 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:27,338\tWARNING util.py:201 -- The `process_trial_result` operation took 2.840 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:27,338\tWARNING util.py:201 -- Processing trial results took 2.840 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:39:27,339\tWARNING util.py:201 -- The `process_trial_result` operation took 2.841 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:28,539\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.199 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:28,541\tWARNING util.py:201 -- The `process_trial_result` operation took 1.201 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:28,542\tWARNING util.py:201 -- Processing trial results took 1.202 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:39:28,542\tWARNING util.py:201 -- The `process_trial_result` operation took 1.202 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3412418)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_b9035250_4_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0002,data_transf_2025-02-12_10-27-48/checkpoint_000086)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_nn_reg pid=3435637)\u001b[0m Seed set to 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3435637)\u001b[0m Train: (156, 3241), Test: (65, 3241)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3435637)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3435637)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3435637)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3435637)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_dac60601_21_batch_size=256,data_aggregation=None,data_selection=abundance_ith,data_selection_i=19,data_transform=None_2025-02-12_10-39-20 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3435637)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3435637)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3435637)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3435637)\u001b[0m 0 | layers | ModuleList | 205 K  | train\n",
      "\u001b[36m(train_nn_reg pid=3435637)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3435637)\u001b[0m 205 K     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3435637)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3435637)\u001b[0m 205 K     Total params\n",
      "\u001b[36m(train_nn_reg pid=3435637)\u001b[0m 0.821     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3435637)\u001b[0m 4         Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3435637)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3435637)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(train_nn_reg pid=3435637)\u001b[0m Metric train_rmse does not exist in `trainer.callback_metrics.\n",
      "\u001b[36m(train_nn_reg pid=3435637)\u001b[0m Metric train_r2 does not exist in `trainer.callback_metrics.\n",
      "\u001b[36m(train_nn_reg pid=3435637)\u001b[0m Metric train_loss does not exist in `trainer.callback_metrics.\n",
      "2025-02-12 10:39:31,858\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.529 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:31,861\tWARNING util.py:201 -- The `process_trial_result` operation took 1.531 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:31,861\tWARNING util.py:201 -- Processing trial results took 1.532 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:39:31,862\tWARNING util.py:201 -- The `process_trial_result` operation took 1.532 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:33,224\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.361 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:33,226\tWARNING util.py:201 -- The `process_trial_result` operation took 1.363 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:33,227\tWARNING util.py:201 -- Processing trial results took 1.364 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:39:33,227\tWARNING util.py:201 -- The `process_trial_result` operation took 1.364 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3413652)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_e26bf114_8_batch_size=128,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0018,data_transf_2025-02-12_10-28-19/checkpoint_000074)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "2025-02-12 10:39:36,438\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 3.147 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:36,441\tWARNING util.py:201 -- The `process_trial_result` operation took 3.150 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:36,442\tWARNING util.py:201 -- Processing trial results took 3.150 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:39:36,442\tWARNING util.py:201 -- The `process_trial_result` operation took 3.151 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:38,338\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.882 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:38,340\tWARNING util.py:201 -- The `process_trial_result` operation took 1.884 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:38,341\tWARNING util.py:201 -- Processing trial results took 1.885 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:39:38,342\tWARNING util.py:201 -- The `process_trial_result` operation took 1.885 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:39,492\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.149 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:39,494\tWARNING util.py:201 -- The `process_trial_result` operation took 1.151 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:39,495\tWARNING util.py:201 -- Processing trial results took 1.152 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:39:39,495\tWARNING util.py:201 -- The `process_trial_result` operation took 1.152 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:40,486\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 0.989 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:40,488\tWARNING util.py:201 -- The `process_trial_result` operation took 0.992 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:40,488\tWARNING util.py:201 -- Processing trial results took 0.992 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:39:40,489\tWARNING util.py:201 -- The `process_trial_result` operation took 0.993 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3412418)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_b9035250_4_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0002,data_transf_2025-02-12_10-27-48/checkpoint_000087)\n",
      "\u001b[36m(train_nn_reg pid=3413652)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_e26bf114_8_batch_size=128,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0018,data_transf_2025-02-12_10-28-19/checkpoint_000075)\n",
      "2025-02-12 10:39:42,364\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.431 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:42,366\tWARNING util.py:201 -- The `process_trial_result` operation took 1.433 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:42,367\tWARNING util.py:201 -- Processing trial results took 1.434 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:39:42,367\tWARNING util.py:201 -- The `process_trial_result` operation took 1.434 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:43,516\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.134 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:43,518\tWARNING util.py:201 -- The `process_trial_result` operation took 1.136 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:43,519\tWARNING util.py:201 -- Processing trial results took 1.137 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:39:43,519\tWARNING util.py:201 -- The `process_trial_result` operation took 1.137 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:46,749\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 3.200 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:46,752\tWARNING util.py:201 -- The `process_trial_result` operation took 3.203 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:46,754\tWARNING util.py:201 -- Processing trial results took 3.205 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:39:46,754\tWARNING util.py:201 -- The `process_trial_result` operation took 3.205 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3422740)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_a70dbacd_15_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=11,data_transform=None_2025-02-12_10-33-04/checkpoint_000044)\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "2025-02-12 10:39:48,676\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.909 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:48,678\tWARNING util.py:201 -- The `process_trial_result` operation took 1.912 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:48,679\tWARNING util.py:201 -- Processing trial results took 1.913 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:39:48,679\tWARNING util.py:201 -- The `process_trial_result` operation took 1.913 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:50,366\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.672 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:50,369\tWARNING util.py:201 -- The `process_trial_result` operation took 1.674 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:50,369\tWARNING util.py:201 -- Processing trial results took 1.675 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:39:50,370\tWARNING util.py:201 -- The `process_trial_result` operation took 1.675 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:51,514\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.143 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:51,517\tWARNING util.py:201 -- The `process_trial_result` operation took 1.146 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:51,517\tWARNING util.py:201 -- Processing trial results took 1.147 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:39:51,518\tWARNING util.py:201 -- The `process_trial_result` operation took 1.147 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3417235)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_4450d409_11_batch_size=128,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0031,data_trans_2025-02-12_10-29-59/checkpoint_000066)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2025-02-12 10:39:52,848\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.297 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:52,851\tWARNING util.py:201 -- The `process_trial_result` operation took 1.300 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:52,851\tWARNING util.py:201 -- Processing trial results took 1.300 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:39:52,851\tWARNING util.py:201 -- The `process_trial_result` operation took 1.301 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:54,157\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.294 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:54,160\tWARNING util.py:201 -- The `process_trial_result` operation took 1.297 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:54,161\tWARNING util.py:201 -- Processing trial results took 1.297 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:39:54,161\tWARNING util.py:201 -- The `process_trial_result` operation took 1.297 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:54,176\tWARNING syncer.py:406 -- Last sync command failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 404, in _launch_sync_process\n",
      "    self.wait()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 474, in wait\n",
      "    raise e\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 472, in wait\n",
      "    self._sync_process.wait(timeout=timeout or self.sync_timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 173, in wait\n",
      "    raise exception\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 136, in entrypoint\n",
      "    result = self._fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/storage.py\", line 218, in _upload_to_fs_path\n",
      "    _pyarrow_fs_copy_files(local_path, fs_path, destination_filesystem=fs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/storage.py\", line 116, in _pyarrow_fs_copy_files\n",
      "    return pyarrow.fs.copy_files(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/pyarrow/fs.py\", line 259, in copy_files\n",
      "    _copy_files_selector(source_fs, source_sel,\n",
      "  File \"pyarrow/_fs.pyx\", line 1624, in pyarrow._fs._copy_files_selector\n",
      "  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\n",
      "FileNotFoundError: [Errno 2] Failed to open local file '/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_b9035250_4_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0002,data_transf_2025-02-12_10-27-48/epoch=86-val_rmse=7.39.ckpt'. Detail: [errno 2] No such file or directory\n",
      "\n",
      "2025-02-12 10:39:57,381\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 3.190 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:57,383\tWARNING util.py:201 -- The `process_trial_result` operation took 3.192 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:57,384\tWARNING util.py:201 -- Processing trial results took 3.193 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:39:57,385\tWARNING util.py:201 -- The `process_trial_result` operation took 3.193 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:58,438\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.052 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:58,440\tWARNING util.py:201 -- The `process_trial_result` operation took 1.055 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:58,441\tWARNING util.py:201 -- Processing trial results took 1.055 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:39:58,441\tWARNING util.py:201 -- The `process_trial_result` operation took 1.055 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:59,919\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.477 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:59,922\tWARNING util.py:201 -- The `process_trial_result` operation took 1.479 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:39:59,923\tWARNING util.py:201 -- Processing trial results took 1.480 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:39:59,923\tWARNING util.py:201 -- The `process_trial_result` operation took 1.481 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3412418)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_b9035250_4_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0002,data_transf_2025-02-12_10-27-48/checkpoint_000090)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2025-02-12 10:40:02,125\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.141 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:02,128\tWARNING util.py:201 -- The `process_trial_result` operation took 2.145 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:02,129\tWARNING util.py:201 -- Processing trial results took 2.146 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:40:02,129\tWARNING util.py:201 -- The `process_trial_result` operation took 2.146 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:03,268\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.136 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:03,270\tWARNING util.py:201 -- The `process_trial_result` operation took 1.139 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:03,272\tWARNING util.py:201 -- Processing trial results took 1.140 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:40:03,272\tWARNING util.py:201 -- The `process_trial_result` operation took 1.141 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:04,934\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.660 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:04,936\tWARNING util.py:201 -- The `process_trial_result` operation took 1.662 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:04,937\tWARNING util.py:201 -- Processing trial results took 1.663 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:40:04,937\tWARNING util.py:201 -- The `process_trial_result` operation took 1.663 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:07,417\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.432 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:07,419\tWARNING util.py:201 -- The `process_trial_result` operation took 2.434 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:07,420\tWARNING util.py:201 -- Processing trial results took 2.435 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:40:07,420\tWARNING util.py:201 -- The `process_trial_result` operation took 2.435 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:08,559\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.138 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:08,562\tWARNING util.py:201 -- The `process_trial_result` operation took 1.140 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:08,562\tWARNING util.py:201 -- Processing trial results took 1.141 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:40:08,563\tWARNING util.py:201 -- The `process_trial_result` operation took 1.141 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3412418)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_b9035250_4_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0002,data_transf_2025-02-12_10-27-48/checkpoint_000092)\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "2025-02-12 10:40:10,313\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.693 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:10,316\tWARNING util.py:201 -- The `process_trial_result` operation took 1.695 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:10,316\tWARNING util.py:201 -- Processing trial results took 1.696 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:40:10,317\tWARNING util.py:201 -- The `process_trial_result` operation took 1.696 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:12,234\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.903 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:12,236\tWARNING util.py:201 -- The `process_trial_result` operation took 1.905 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:12,237\tWARNING util.py:201 -- Processing trial results took 1.906 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:40:12,237\tWARNING util.py:201 -- The `process_trial_result` operation took 1.906 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:13,963\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.724 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:13,965\tWARNING util.py:201 -- The `process_trial_result` operation took 1.726 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:13,965\tWARNING util.py:201 -- Processing trial results took 1.727 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:40:13,966\tWARNING util.py:201 -- The `process_trial_result` operation took 1.727 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3417235)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_4450d409_11_batch_size=128,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0031,data_trans_2025-02-12_10-29-59/checkpoint_000068)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "2025-02-12 10:40:15,400\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.419 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:15,403\tWARNING util.py:201 -- The `process_trial_result` operation took 1.421 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:15,403\tWARNING util.py:201 -- Processing trial results took 1.421 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:40:15,404\tWARNING util.py:201 -- The `process_trial_result` operation took 1.422 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:15,419\tWARNING syncer.py:406 -- Last sync command failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 404, in _launch_sync_process\n",
      "    self.wait()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 474, in wait\n",
      "    raise e\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 472, in wait\n",
      "    self._sync_process.wait(timeout=timeout or self.sync_timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 173, in wait\n",
      "    raise exception\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 136, in entrypoint\n",
      "    result = self._fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/storage.py\", line 218, in _upload_to_fs_path\n",
      "    _pyarrow_fs_copy_files(local_path, fs_path, destination_filesystem=fs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/storage.py\", line 116, in _pyarrow_fs_copy_files\n",
      "    return pyarrow.fs.copy_files(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/pyarrow/fs.py\", line 259, in copy_files\n",
      "    _copy_files_selector(source_fs, source_sel,\n",
      "  File \"pyarrow/_fs.pyx\", line 1624, in pyarrow._fs._copy_files_selector\n",
      "  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\n",
      "FileNotFoundError: [Errno 2] Failed to open local file '/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_b9035250_4_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0002,data_transf_2025-02-12_10-27-48/epoch=89-val_rmse=6.76.ckpt'. Detail: [errno 2] No such file or directory\n",
      "\n",
      "2025-02-12 10:40:17,944\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.451 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:17,947\tWARNING util.py:201 -- The `process_trial_result` operation took 2.454 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:17,947\tWARNING util.py:201 -- Processing trial results took 2.454 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:40:17,948\tWARNING util.py:201 -- The `process_trial_result` operation took 2.455 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:19,071\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.122 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:19,074\tWARNING util.py:201 -- The `process_trial_result` operation took 1.124 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:19,074\tWARNING util.py:201 -- Processing trial results took 1.125 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:40:19,075\tWARNING util.py:201 -- The `process_trial_result` operation took 1.126 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:20,799\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.722 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:20,801\tWARNING util.py:201 -- The `process_trial_result` operation took 1.725 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:20,802\tWARNING util.py:201 -- Processing trial results took 1.725 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:40:20,802\tWARNING util.py:201 -- The `process_trial_result` operation took 1.726 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:21,979\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.162 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:21,981\tWARNING util.py:201 -- The `process_trial_result` operation took 1.164 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:21,981\tWARNING util.py:201 -- Processing trial results took 1.164 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:40:21,982\tWARNING util.py:201 -- The `process_trial_result` operation took 1.165 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3413652)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_e26bf114_8_batch_size=128,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0018,data_transf_2025-02-12_10-28-19/checkpoint_000079)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "2025-02-12 10:40:24,085\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.080 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:24,088\tWARNING util.py:201 -- The `process_trial_result` operation took 2.083 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:24,089\tWARNING util.py:201 -- Processing trial results took 2.084 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:40:24,089\tWARNING util.py:201 -- The `process_trial_result` operation took 2.084 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:26,091\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.975 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:26,094\tWARNING util.py:201 -- The `process_trial_result` operation took 1.978 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:26,094\tWARNING util.py:201 -- Processing trial results took 1.978 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:40:26,095\tWARNING util.py:201 -- The `process_trial_result` operation took 1.979 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:26,109\tWARNING syncer.py:406 -- Last sync command failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 404, in _launch_sync_process\n",
      "    self.wait()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 474, in wait\n",
      "    raise e\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 472, in wait\n",
      "    self._sync_process.wait(timeout=timeout or self.sync_timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 173, in wait\n",
      "    raise exception\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 136, in entrypoint\n",
      "    result = self._fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/storage.py\", line 218, in _upload_to_fs_path\n",
      "    _pyarrow_fs_copy_files(local_path, fs_path, destination_filesystem=fs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/storage.py\", line 116, in _pyarrow_fs_copy_files\n",
      "    return pyarrow.fs.copy_files(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/pyarrow/fs.py\", line 259, in copy_files\n",
      "    _copy_files_selector(source_fs, source_sel,\n",
      "  File \"pyarrow/_fs.pyx\", line 1624, in pyarrow._fs._copy_files_selector\n",
      "  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\n",
      "FileNotFoundError: [Errno 2] Failed to open local file '/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_b9035250_4_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0002,data_transf_2025-02-12_10-27-48/epoch=91-val_rmse=6.34.ckpt'. Detail: [errno 2] No such file or directory\n",
      "\n",
      "2025-02-12 10:40:29,659\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 3.515 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:29,663\tWARNING util.py:201 -- The `process_trial_result` operation took 3.519 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:29,663\tWARNING util.py:201 -- Processing trial results took 3.520 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:40:29,664\tWARNING util.py:201 -- The `process_trial_result` operation took 3.521 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:31,284\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.619 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:31,286\tWARNING util.py:201 -- The `process_trial_result` operation took 1.621 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:31,287\tWARNING util.py:201 -- Processing trial results took 1.622 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:40:31,287\tWARNING util.py:201 -- The `process_trial_result` operation took 1.622 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3412418)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_b9035250_4_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0002,data_transf_2025-02-12_10-27-48/checkpoint_000094)\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "2025-02-12 10:40:33,176\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.799 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:33,178\tWARNING util.py:201 -- The `process_trial_result` operation took 1.801 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:33,179\tWARNING util.py:201 -- Processing trial results took 1.802 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:40:33,179\tWARNING util.py:201 -- The `process_trial_result` operation took 1.802 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:34,463\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.283 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:34,466\tWARNING util.py:201 -- The `process_trial_result` operation took 1.285 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:34,466\tWARNING util.py:201 -- Processing trial results took 1.285 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:40:34,466\tWARNING util.py:201 -- The `process_trial_result` operation took 1.286 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:36,179\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.701 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:36,182\tWARNING util.py:201 -- The `process_trial_result` operation took 1.703 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:36,183\tWARNING util.py:201 -- Processing trial results took 1.704 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:40:36,183\tWARNING util.py:201 -- The `process_trial_result` operation took 1.705 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:36,197\tWARNING syncer.py:406 -- Last sync command failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 404, in _launch_sync_process\n",
      "    self.wait()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 474, in wait\n",
      "    raise e\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 472, in wait\n",
      "    self._sync_process.wait(timeout=timeout or self.sync_timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 173, in wait\n",
      "    raise exception\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 136, in entrypoint\n",
      "    result = self._fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/storage.py\", line 218, in _upload_to_fs_path\n",
      "    _pyarrow_fs_copy_files(local_path, fs_path, destination_filesystem=fs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/storage.py\", line 116, in _pyarrow_fs_copy_files\n",
      "    return pyarrow.fs.copy_files(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/pyarrow/fs.py\", line 259, in copy_files\n",
      "    _copy_files_selector(source_fs, source_sel,\n",
      "  File \"pyarrow/_fs.pyx\", line 1624, in pyarrow._fs._copy_files_selector\n",
      "  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\n",
      "FileNotFoundError: [Errno 2] Failed to open local file '/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_dac60601_21_batch_size=256,data_aggregation=None,data_selection=abundance_ith,data_selection_i=19,data_transform=None_2025-02-12_10-39-20/epoch=5-val_rmse=18.52.ckpt'. Detail: [errno 2] No such file or directory\n",
      "\n",
      "\u001b[36m(train_nn_reg pid=3412418)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_b9035250_4_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0002,data_transf_2025-02-12_10-27-48/checkpoint_000095)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "2025-02-12 10:40:39,487\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 3.271 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:39,489\tWARNING util.py:201 -- The `process_trial_result` operation took 3.273 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:39,490\tWARNING util.py:201 -- Processing trial results took 3.273 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:40:39,490\tWARNING util.py:201 -- The `process_trial_result` operation took 3.274 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:40,616\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.111 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:40,618\tWARNING util.py:201 -- The `process_trial_result` operation took 1.114 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:40,619\tWARNING util.py:201 -- Processing trial results took 1.114 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:40:40,619\tWARNING util.py:201 -- The `process_trial_result` operation took 1.114 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:41,583\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 0.963 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:41,586\tWARNING util.py:201 -- The `process_trial_result` operation took 0.965 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:41,586\tWARNING util.py:201 -- Processing trial results took 0.966 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:40:41,586\tWARNING util.py:201 -- The `process_trial_result` operation took 0.966 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3412418)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_b9035250_4_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0002,data_transf_2025-02-12_10-27-48/checkpoint_000096)\n",
      "2025-02-12 10:40:42,804\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.205 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:42,806\tWARNING util.py:201 -- The `process_trial_result` operation took 1.207 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:42,807\tWARNING util.py:201 -- Processing trial results took 1.209 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:40:42,808\tWARNING util.py:201 -- The `process_trial_result` operation took 1.210 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3417235)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_4450d409_11_batch_size=128,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0031,data_trans_2025-02-12_10-29-59/checkpoint_000071)\n",
      "2025-02-12 10:40:44,308\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.458 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:44,310\tWARNING util.py:201 -- The `process_trial_result` operation took 1.461 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:44,310\tWARNING util.py:201 -- Processing trial results took 1.461 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:40:44,311\tWARNING util.py:201 -- The `process_trial_result` operation took 1.462 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:44,372\tERROR experiment_state.py:173 -- Saving experiment state to storage at '/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg' failed with exception: \n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/execution/experiment_state.py\", line 171, in wait_for_sync\n",
      "    self._storage.syncer.wait()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 474, in wait\n",
      "    raise e\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 472, in wait\n",
      "    self._sync_process.wait(timeout=timeout or self.sync_timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 173, in wait\n",
      "    raise exception\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 136, in entrypoint\n",
      "    result = self._fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/storage.py\", line 218, in _upload_to_fs_path\n",
      "    _pyarrow_fs_copy_files(local_path, fs_path, destination_filesystem=fs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/storage.py\", line 116, in _pyarrow_fs_copy_files\n",
      "    return pyarrow.fs.copy_files(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/pyarrow/fs.py\", line 259, in copy_files\n",
      "    _copy_files_selector(source_fs, source_sel,\n",
      "  File \"pyarrow/_fs.pyx\", line 1624, in pyarrow._fs._copy_files_selector\n",
      "  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\n",
      "FileNotFoundError: [Errno 2] Failed to open local file '/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_b9035250_4_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0002,data_transf_2025-02-12_10-27-48/epoch=93-val_rmse=5.93.ckpt'. Detail: [errno 2] No such file or directory\n",
      "2025-02-12 10:40:47,899\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 3.524 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:47,902\tWARNING util.py:201 -- The `process_trial_result` operation took 3.527 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:47,902\tWARNING util.py:201 -- Processing trial results took 3.527 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:40:47,903\tWARNING util.py:201 -- The `process_trial_result` operation took 3.528 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:49,057\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.138 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:49,060\tWARNING util.py:201 -- The `process_trial_result` operation took 1.141 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:49,060\tWARNING util.py:201 -- Processing trial results took 1.142 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:40:49,061\tWARNING util.py:201 -- The `process_trial_result` operation took 1.142 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3422740)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_a70dbacd_15_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=11,data_transform=None_2025-02-12_10-33-04/checkpoint_000049)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2025-02-12 10:40:50,283\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.209 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:50,285\tWARNING util.py:201 -- The `process_trial_result` operation took 1.211 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:50,286\tWARNING util.py:201 -- Processing trial results took 1.212 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:40:50,286\tWARNING util.py:201 -- The `process_trial_result` operation took 1.212 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:51,702\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.377 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:51,704\tWARNING util.py:201 -- The `process_trial_result` operation took 1.380 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:51,705\tWARNING util.py:201 -- Processing trial results took 1.380 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:40:51,706\tWARNING util.py:201 -- The `process_trial_result` operation took 1.381 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:53,016\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.298 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:53,018\tWARNING util.py:201 -- The `process_trial_result` operation took 1.300 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:53,019\tWARNING util.py:201 -- Processing trial results took 1.301 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:40:53,020\tWARNING util.py:201 -- The `process_trial_result` operation took 1.302 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:54,168\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.134 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:54,170\tWARNING util.py:201 -- The `process_trial_result` operation took 1.136 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:54,171\tWARNING util.py:201 -- Processing trial results took 1.137 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:40:54,171\tWARNING util.py:201 -- The `process_trial_result` operation took 1.137 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3435637)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_dac60601_21_batch_size=256,data_aggregation=None,data_selection=abundance_ith,data_selection_i=19,data_transform=None_2025-02-12_10-39-20/checkpoint_000009)\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "2025-02-12 10:40:56,455\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.214 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:56,457\tWARNING util.py:201 -- The `process_trial_result` operation took 2.217 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:40:56,457\tWARNING util.py:201 -- Processing trial results took 2.217 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:40:56,458\tWARNING util.py:201 -- The `process_trial_result` operation took 2.218 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:41:00,186\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 3.694 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:41:00,188\tWARNING util.py:201 -- The `process_trial_result` operation took 3.696 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:41:00,189\tWARNING util.py:201 -- Processing trial results took 3.697 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:41:00,189\tWARNING util.py:201 -- The `process_trial_result` operation took 3.697 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3413652)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_e26bf114_8_batch_size=128,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0018,data_transf_2025-02-12_10-28-19/checkpoint_000084)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "2025-02-12 10:41:01,455\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.249 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:41:01,457\tWARNING util.py:201 -- The `process_trial_result` operation took 1.251 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:41:01,458\tWARNING util.py:201 -- Processing trial results took 1.251 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:41:01,458\tWARNING util.py:201 -- The `process_trial_result` operation took 1.252 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:41:02,628\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.168 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:41:02,630\tWARNING util.py:201 -- The `process_trial_result` operation took 1.170 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:41:02,631\tWARNING util.py:201 -- Processing trial results took 1.171 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:41:02,631\tWARNING util.py:201 -- The `process_trial_result` operation took 1.172 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:41:04,367\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.733 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:41:04,368\tWARNING util.py:201 -- The `process_trial_result` operation took 1.736 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:41:04,369\tWARNING util.py:201 -- Processing trial results took 1.736 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:41:04,371\tWARNING util.py:201 -- The `process_trial_result` operation took 1.738 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:41:05,431\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.048 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:41:05,433\tWARNING util.py:201 -- The `process_trial_result` operation took 1.051 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:41:05,434\tWARNING util.py:201 -- Processing trial results took 1.052 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:41:05,435\tWARNING util.py:201 -- The `process_trial_result` operation took 1.053 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3422740)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_a70dbacd_15_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=11,data_transform=None_2025-02-12_10-33-04/checkpoint_000051)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "2025-02-12 10:41:07,474\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.012 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:41:07,476\tWARNING util.py:201 -- The `process_trial_result` operation took 2.014 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:41:07,477\tWARNING util.py:201 -- Processing trial results took 2.015 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:41:07,477\tWARNING util.py:201 -- The `process_trial_result` operation took 2.015 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:41:11,133\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.814 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:41:11,135\tWARNING util.py:201 -- The `process_trial_result` operation took 1.817 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:41:11,136\tWARNING util.py:201 -- Processing trial results took 1.817 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:41:11,136\tWARNING util.py:201 -- The `process_trial_result` operation took 1.818 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3417235)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_4450d409_11_batch_size=128,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0031,data_trans_2025-02-12_10-29-59/checkpoint_000075)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "2025-02-12 10:41:12,356\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.191 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:41:12,359\tWARNING util.py:201 -- The `process_trial_result` operation took 1.194 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:41:12,359\tWARNING util.py:201 -- Processing trial results took 1.194 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:41:12,359\tWARNING util.py:201 -- The `process_trial_result` operation took 1.195 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:41:14,264\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.891 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:41:14,266\tWARNING util.py:201 -- The `process_trial_result` operation took 1.893 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:41:14,267\tWARNING util.py:201 -- Processing trial results took 1.894 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:41:14,267\tWARNING util.py:201 -- The `process_trial_result` operation took 1.894 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:41:15,738\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.144 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:41:15,740\tWARNING util.py:201 -- The `process_trial_result` operation took 1.146 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:41:15,741\tWARNING util.py:201 -- Processing trial results took 1.147 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:41:15,742\tWARNING util.py:201 -- The `process_trial_result` operation took 1.148 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3439792)\u001b[0m Seed set to 12\n",
      "\u001b[36m(train_nn_reg pid=3417235)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_4450d409_11_batch_size=128,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0031,data_trans_2025-02-12_10-29-59/checkpoint_000076)\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3439792)\u001b[0m Train: (156, 66), Test: (65, 66)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3439792)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3439792)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3439792)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3439792)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_bd4ccfe5_22_batch_size=64,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0001,data_transf_2025-02-12_10-41-09 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3439792)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3439792)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3439792)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3439792)\u001b[0m 0 | layers | ModuleList | 141 K  | train\n",
      "\u001b[36m(train_nn_reg pid=3439792)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3439792)\u001b[0m 141 K     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3439792)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3439792)\u001b[0m 141 K     Total params\n",
      "\u001b[36m(train_nn_reg pid=3439792)\u001b[0m 0.566     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3439792)\u001b[0m 14        Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3439792)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3439792)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3439792)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-02-12 10:41:19,802\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.157 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:41:19,805\tWARNING util.py:201 -- The `process_trial_result` operation took 1.160 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:41:19,805\tWARNING util.py:201 -- Processing trial results took 1.160 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:41:19,806\tWARNING util.py:201 -- The `process_trial_result` operation took 1.161 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:41:19,810\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_bd4ccfe5\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3439792, ip=172.31.181.85, actor_id=e28af5eb420138935ea5651501000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1024, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1053, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 1) cannot be concatenated\n",
      "2025-02-12 10:41:21,854\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.903 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:41:21,857\tWARNING util.py:201 -- The `process_trial_result` operation took 1.906 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:41:21,857\tWARNING util.py:201 -- Processing trial results took 1.906 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:41:21,858\tWARNING util.py:201 -- The `process_trial_result` operation took 1.907 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:41:23,181\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.295 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:41:23,183\tWARNING util.py:201 -- The `process_trial_result` operation took 1.298 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:41:23,183\tWARNING util.py:201 -- Processing trial results took 1.298 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:41:23,184\tWARNING util.py:201 -- The `process_trial_result` operation took 1.299 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:41:24,376\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.191 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:41:24,379\tWARNING util.py:201 -- The `process_trial_result` operation took 1.193 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:41:24,380\tWARNING util.py:201 -- Processing trial results took 1.194 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:41:24,380\tWARNING util.py:201 -- The `process_trial_result` operation took 1.195 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:41:26,227\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.832 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:41:26,229\tWARNING util.py:201 -- The `process_trial_result` operation took 1.834 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:41:26,230\tWARNING util.py:201 -- Processing trial results took 1.834 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:41:26,232\tWARNING util.py:201 -- The `process_trial_result` operation took 1.837 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3422740)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_a70dbacd_15_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=11,data_transform=None_2025-02-12_10-33-04/checkpoint_000053)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_nn_reg pid=3440117)\u001b[0m Seed set to 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3440117)\u001b[0m Train: (156, 1821), Test: (65, 1821)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3440117)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3440117)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3440117)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3440117)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_f0dd288e_23_batch_size=32,data_aggregation=None,data_selection=abundance_ith,data_selection_i=7,data_transform=None,e_2025-02-12_10-41-18 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3440117)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3440117)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3440117)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3440117)\u001b[0m 0 | layers | ModuleList | 57.2 K | train\n",
      "\u001b[36m(train_nn_reg pid=3440117)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3440117)\u001b[0m 57.2 K    Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3440117)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3440117)\u001b[0m 57.2 K    Total params\n",
      "\u001b[36m(train_nn_reg pid=3440117)\u001b[0m 0.229     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3440117)\u001b[0m 4         Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3440117)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3440117)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(train_nn_reg pid=3440117)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3440117)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-02-12 10:41:29,894\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.014 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:41:29,896\tWARNING util.py:201 -- The `process_trial_result` operation took 1.017 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:41:29,896\tWARNING util.py:201 -- Processing trial results took 1.017 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:41:29,897\tWARNING util.py:201 -- The `process_trial_result` operation took 1.017 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:41:29,935\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_f0dd288e\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3440117, ip=172.31.181.85, actor_id=78e905ca3fb062c1e0edd92201000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1026, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 216, in run\n",
      "    self.advance()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 455, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 151, in run\n",
      "    self.on_advance_end(data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 370, in on_advance_end\n",
      "    self.val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 2) cannot be concatenated\n",
      "2025-02-12 10:41:32,102\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.968 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:41:32,104\tWARNING util.py:201 -- The `process_trial_result` operation took 1.970 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:41:32,105\tWARNING util.py:201 -- Processing trial results took 1.971 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:41:32,105\tWARNING util.py:201 -- The `process_trial_result` operation took 1.971 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3422740)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_a70dbacd_15_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=11,data_transform=None_2025-02-12_10-33-04/checkpoint_000054)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2025-02-12 10:41:33,595\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.462 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:41:33,597\tWARNING util.py:201 -- The `process_trial_result` operation took 1.464 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:41:33,598\tWARNING util.py:201 -- Processing trial results took 1.465 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:41:33,598\tWARNING util.py:201 -- The `process_trial_result` operation took 1.465 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:41:34,593\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 0.993 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:41:34,595\tWARNING util.py:201 -- The `process_trial_result` operation took 0.996 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:41:34,596\tWARNING util.py:201 -- Processing trial results took 0.996 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:41:34,596\tWARNING util.py:201 -- The `process_trial_result` operation took 0.997 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:41:38,163\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 3.530 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:41:38,165\tWARNING util.py:201 -- The `process_trial_result` operation took 3.532 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:41:38,166\tWARNING util.py:201 -- Processing trial results took 3.533 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:41:38,166\tWARNING util.py:201 -- The `process_trial_result` operation took 3.533 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:41:39,138\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 0.957 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:41:39,140\tWARNING util.py:201 -- The `process_trial_result` operation took 0.959 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:41:39,141\tWARNING util.py:201 -- Processing trial results took 0.960 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:41:39,142\tWARNING util.py:201 -- The `process_trial_result` operation took 0.960 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3422740)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_a70dbacd_15_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=11,data_transform=None_2025-02-12_10-33-04/checkpoint_000056)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_nn_reg pid=3440497)\u001b[0m Seed set to 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3440497)\u001b[0m Train: (156, 39), Test: (65, 39)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3440497)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3440497)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3440497)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3440497)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_0ed9ed1e_24_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0038,data_trans_2025-02-12_10-41-28 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3440497)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3440497)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3440497)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3440497)\u001b[0m 0 | layers | ModuleList | 23.1 K | train\n",
      "\u001b[36m(train_nn_reg pid=3440497)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3440497)\u001b[0m 23.1 K    Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3440497)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3440497)\u001b[0m 23.1 K    Total params\n",
      "\u001b[36m(train_nn_reg pid=3440497)\u001b[0m 0.092     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3440497)\u001b[0m 14        Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3440497)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3440497)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(train_nn_reg pid=3440497)\u001b[0m Metric train_rmse does not exist in `trainer.callback_metrics.\n",
      "\u001b[36m(train_nn_reg pid=3440497)\u001b[0m Metric train_r2 does not exist in `trainer.callback_metrics.\n",
      "\u001b[36m(train_nn_reg pid=3440497)\u001b[0m Metric train_loss does not exist in `trainer.callback_metrics.\n",
      "2025-02-12 10:41:42,544\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.042 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:41:42,547\tWARNING util.py:201 -- The `process_trial_result` operation took 2.044 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:41:42,547\tWARNING util.py:201 -- Processing trial results took 2.045 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:41:42,548\tWARNING util.py:201 -- The `process_trial_result` operation took 2.045 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:41:43,662\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.109 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:41:43,664\tWARNING util.py:201 -- The `process_trial_result` operation took 1.112 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:41:43,664\tWARNING util.py:201 -- Processing trial results took 1.112 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:41:43,665\tWARNING util.py:201 -- The `process_trial_result` operation took 1.113 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:41:45,742\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.060 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:41:45,744\tWARNING util.py:201 -- The `process_trial_result` operation took 2.062 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:41:45,745\tWARNING util.py:201 -- Processing trial results took 2.063 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:41:45,746\tWARNING util.py:201 -- The `process_trial_result` operation took 2.064 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3417235)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_4450d409_11_batch_size=128,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0031,data_trans_2025-02-12_10-29-59/checkpoint_000081)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "2025-02-12 10:41:48,790\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 3.011 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:41:48,792\tWARNING util.py:201 -- The `process_trial_result` operation took 3.013 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:41:48,794\tWARNING util.py:201 -- Processing trial results took 3.014 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:41:48,794\tWARNING util.py:201 -- The `process_trial_result` operation took 3.015 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:41:49,788\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 0.978 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:41:49,790\tWARNING util.py:201 -- The `process_trial_result` operation took 0.980 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:41:49,791\tWARNING util.py:201 -- Processing trial results took 0.981 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:41:49,791\tWARNING util.py:201 -- The `process_trial_result` operation took 0.981 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3441044)\u001b[0m Seed set to 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3441044)\u001b[0m Train: (156, 98), Test: (65, 98)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3441044)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3441044)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3441044)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3441044)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_9c57c6f3_25_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0000,data_trans_2025-02-12_10-41-40 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3441044)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3441044)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3441044)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3441044)\u001b[0m 0 | layers | ModuleList | 887 K  | train\n",
      "\u001b[36m(train_nn_reg pid=3441044)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3441044)\u001b[0m 887 K     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3441044)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3441044)\u001b[0m 887 K     Total params\n",
      "\u001b[36m(train_nn_reg pid=3441044)\u001b[0m 3.549     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3441044)\u001b[0m 14        Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3441044)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3441044)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(train_nn_reg pid=3441044)\u001b[0m Metric train_rmse does not exist in `trainer.callback_metrics.\n",
      "\u001b[36m(train_nn_reg pid=3441044)\u001b[0m Metric train_r2 does not exist in `trainer.callback_metrics.\n",
      "\u001b[36m(train_nn_reg pid=3441044)\u001b[0m Metric train_loss does not exist in `trainer.callback_metrics.\n",
      "\u001b[36m(train_nn_reg pid=3440497)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_0ed9ed1e_24_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0038,data_trans_2025-02-12_10-41-28/checkpoint_000002)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "2025-02-12 10:41:52,986\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.073 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:41:52,988\tWARNING util.py:201 -- The `process_trial_result` operation took 2.075 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:41:52,988\tWARNING util.py:201 -- Processing trial results took 2.076 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:41:52,989\tWARNING util.py:201 -- The `process_trial_result` operation took 2.076 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:41:54,995\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.004 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:41:54,997\tWARNING util.py:201 -- The `process_trial_result` operation took 2.006 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:41:54,998\tWARNING util.py:201 -- Processing trial results took 2.007 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:41:54,998\tWARNING util.py:201 -- The `process_trial_result` operation took 2.007 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:41:56,700\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.664 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:41:56,702\tWARNING util.py:201 -- The `process_trial_result` operation took 1.666 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:41:56,703\tWARNING util.py:201 -- Processing trial results took 1.667 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:41:56,703\tWARNING util.py:201 -- The `process_trial_result` operation took 1.668 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:41:59,948\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 3.230 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:41:59,950\tWARNING util.py:201 -- The `process_trial_result` operation took 3.232 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:41:59,951\tWARNING util.py:201 -- Processing trial results took 3.233 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:41:59,951\tWARNING util.py:201 -- The `process_trial_result` operation took 3.233 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:01,105\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.153 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:01,107\tWARNING util.py:201 -- The `process_trial_result` operation took 1.155 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:01,108\tWARNING util.py:201 -- Processing trial results took 1.156 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:42:01,109\tWARNING util.py:201 -- The `process_trial_result` operation took 1.157 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:03,135\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.011 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:03,138\tWARNING util.py:201 -- The `process_trial_result` operation took 2.013 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:03,138\tWARNING util.py:201 -- Processing trial results took 2.014 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:42:03,139\tWARNING util.py:201 -- The `process_trial_result` operation took 2.014 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:04,284\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.143 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:04,286\tWARNING util.py:201 -- The `process_trial_result` operation took 1.146 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:04,287\tWARNING util.py:201 -- Processing trial results took 1.146 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:42:04,288\tWARNING util.py:201 -- The `process_trial_result` operation took 1.147 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3440497)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_0ed9ed1e_24_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0038,data_trans_2025-02-12_10-41-28/checkpoint_000003)\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "2025-02-12 10:42:07,572\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.418 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:07,574\tWARNING util.py:201 -- The `process_trial_result` operation took 1.420 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:07,574\tWARNING util.py:201 -- Processing trial results took 1.420 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:42:07,575\tWARNING util.py:201 -- The `process_trial_result` operation took 1.421 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:10,477\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.888 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:10,480\tWARNING util.py:201 -- The `process_trial_result` operation took 2.890 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:10,480\tWARNING util.py:201 -- Processing trial results took 2.891 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:42:10,481\tWARNING util.py:201 -- The `process_trial_result` operation took 2.891 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:11,681\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.184 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:11,684\tWARNING util.py:201 -- The `process_trial_result` operation took 1.187 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:11,685\tWARNING util.py:201 -- Processing trial results took 1.187 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:42:11,685\tWARNING util.py:201 -- The `process_trial_result` operation took 1.188 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:12,838\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.137 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:12,839\tWARNING util.py:201 -- The `process_trial_result` operation took 1.139 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:12,840\tWARNING util.py:201 -- Processing trial results took 1.139 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:42:12,841\tWARNING util.py:201 -- The `process_trial_result` operation took 1.140 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:14,716\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.874 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:14,719\tWARNING util.py:201 -- The `process_trial_result` operation took 1.877 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:14,719\tWARNING util.py:201 -- Processing trial results took 1.877 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:42:14,719\tWARNING util.py:201 -- The `process_trial_result` operation took 1.877 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3413652)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_e26bf114_8_batch_size=128,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0018,data_transf_2025-02-12_10-28-19/checkpoint_000092)\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "2025-02-12 10:42:16,301\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.547 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:16,303\tWARNING util.py:201 -- The `process_trial_result` operation took 1.549 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:16,304\tWARNING util.py:201 -- Processing trial results took 1.550 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:42:16,304\tWARNING util.py:201 -- The `process_trial_result` operation took 1.550 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:17,704\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.399 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:17,706\tWARNING util.py:201 -- The `process_trial_result` operation took 1.401 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:17,707\tWARNING util.py:201 -- Processing trial results took 1.402 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:42:17,707\tWARNING util.py:201 -- The `process_trial_result` operation took 1.402 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:21,436\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 3.656 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:21,439\tWARNING util.py:201 -- The `process_trial_result` operation took 3.659 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:21,439\tWARNING util.py:201 -- Processing trial results took 3.659 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:42:21,440\tWARNING util.py:201 -- The `process_trial_result` operation took 3.660 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:22,428\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 0.987 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:22,429\tWARNING util.py:201 -- The `process_trial_result` operation took 0.989 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:22,430\tWARNING util.py:201 -- Processing trial results took 0.989 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:42:22,431\tWARNING util.py:201 -- The `process_trial_result` operation took 0.990 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:23,578\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.145 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:23,580\tWARNING util.py:201 -- The `process_trial_result` operation took 1.147 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:23,581\tWARNING util.py:201 -- Processing trial results took 1.148 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:42:23,582\tWARNING util.py:201 -- The `process_trial_result` operation took 1.149 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3440497)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_0ed9ed1e_24_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0038,data_trans_2025-02-12_10-41-28/checkpoint_000006)\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "2025-02-12 10:42:25,312\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.681 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:25,314\tWARNING util.py:201 -- The `process_trial_result` operation took 1.683 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:25,315\tWARNING util.py:201 -- Processing trial results took 1.684 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:42:25,315\tWARNING util.py:201 -- The `process_trial_result` operation took 1.684 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:26,270\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 0.953 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:26,272\tWARNING util.py:201 -- The `process_trial_result` operation took 0.955 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:26,273\tWARNING util.py:201 -- Processing trial results took 0.956 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:42:26,274\tWARNING util.py:201 -- The `process_trial_result` operation took 0.956 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:27,516\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.229 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:27,518\tWARNING util.py:201 -- The `process_trial_result` operation took 1.231 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:27,519\tWARNING util.py:201 -- Processing trial results took 1.232 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:42:27,519\tWARNING util.py:201 -- The `process_trial_result` operation took 1.232 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:28,749\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.228 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:28,751\tWARNING util.py:201 -- The `process_trial_result` operation took 1.230 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:28,751\tWARNING util.py:201 -- Processing trial results took 1.231 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:42:28,752\tWARNING util.py:201 -- The `process_trial_result` operation took 1.231 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:32,348\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 3.566 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:32,350\tWARNING util.py:201 -- The `process_trial_result` operation took 3.568 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:32,350\tWARNING util.py:201 -- Processing trial results took 3.569 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:42:32,351\tWARNING util.py:201 -- The `process_trial_result` operation took 3.569 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:33,627\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.274 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:33,630\tWARNING util.py:201 -- The `process_trial_result` operation took 1.278 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:33,631\tWARNING util.py:201 -- Processing trial results took 1.279 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:42:33,631\tWARNING util.py:201 -- The `process_trial_result` operation took 1.279 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3422740)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_a70dbacd_15_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=11,data_transform=None_2025-02-12_10-33-04/checkpoint_000061)\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "2025-02-12 10:42:35,508\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.443 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:35,510\tWARNING util.py:201 -- The `process_trial_result` operation took 1.446 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:35,511\tWARNING util.py:201 -- Processing trial results took 1.446 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:42:35,511\tWARNING util.py:201 -- The `process_trial_result` operation took 1.447 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:36,663\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.150 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:36,665\tWARNING util.py:201 -- The `process_trial_result` operation took 1.152 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:36,666\tWARNING util.py:201 -- Processing trial results took 1.153 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:42:36,666\tWARNING util.py:201 -- The `process_trial_result` operation took 1.153 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:38,534\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.867 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:38,536\tWARNING util.py:201 -- The `process_trial_result` operation took 1.869 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:38,537\tWARNING util.py:201 -- Processing trial results took 1.870 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:42:38,537\tWARNING util.py:201 -- The `process_trial_result` operation took 1.870 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3417235)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_4450d409_11_batch_size=128,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0031,data_trans_2025-02-12_10-29-59/checkpoint_000087)\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "2025-02-12 10:42:40,144\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.582 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:40,146\tWARNING util.py:201 -- The `process_trial_result` operation took 1.584 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:40,146\tWARNING util.py:201 -- Processing trial results took 1.585 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:42:40,147\tWARNING util.py:201 -- The `process_trial_result` operation took 1.585 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:44,027\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 3.834 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:44,029\tWARNING util.py:201 -- The `process_trial_result` operation took 3.836 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:44,029\tWARNING util.py:201 -- Processing trial results took 3.837 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:42:44,030\tWARNING util.py:201 -- The `process_trial_result` operation took 3.837 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:45,227\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.196 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:45,229\tWARNING util.py:201 -- The `process_trial_result` operation took 1.198 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:45,230\tWARNING util.py:201 -- Processing trial results took 1.199 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:42:45,231\tWARNING util.py:201 -- The `process_trial_result` operation took 1.200 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:46,422\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.175 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:46,425\tWARNING util.py:201 -- The `process_trial_result` operation took 1.178 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:46,425\tWARNING util.py:201 -- Processing trial results took 1.178 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:42:46,426\tWARNING util.py:201 -- The `process_trial_result` operation took 1.179 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:47,453\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.024 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:47,455\tWARNING util.py:201 -- The `process_trial_result` operation took 1.026 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:47,456\tWARNING util.py:201 -- Processing trial results took 1.028 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:42:47,457\tWARNING util.py:201 -- The `process_trial_result` operation took 1.028 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3440497)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_0ed9ed1e_24_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0038,data_trans_2025-02-12_10-41-28/checkpoint_000009)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "2025-02-12 10:42:49,366\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.489 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:49,369\tWARNING util.py:201 -- The `process_trial_result` operation took 1.493 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:49,369\tWARNING util.py:201 -- Processing trial results took 1.493 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:42:49,369\tWARNING util.py:201 -- The `process_trial_result` operation took 1.494 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:50,360\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 0.989 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:50,362\tWARNING util.py:201 -- The `process_trial_result` operation took 0.991 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:50,362\tWARNING util.py:201 -- Processing trial results took 0.992 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:42:50,363\tWARNING util.py:201 -- The `process_trial_result` operation took 0.993 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:50,378\tWARNING syncer.py:406 -- Last sync command failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 404, in _launch_sync_process\n",
      "    self.wait()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 474, in wait\n",
      "    raise e\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 472, in wait\n",
      "    self._sync_process.wait(timeout=timeout or self.sync_timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 173, in wait\n",
      "    raise exception\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 136, in entrypoint\n",
      "    result = self._fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/storage.py\", line 218, in _upload_to_fs_path\n",
      "    _pyarrow_fs_copy_files(local_path, fs_path, destination_filesystem=fs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/storage.py\", line 116, in _pyarrow_fs_copy_files\n",
      "    return pyarrow.fs.copy_files(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/pyarrow/fs.py\", line 259, in copy_files\n",
      "    _copy_files_selector(source_fs, source_sel,\n",
      "  File \"pyarrow/_fs.pyx\", line 1624, in pyarrow._fs._copy_files_selector\n",
      "  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\n",
      "FileNotFoundError: [Errno 2] Failed to open local file '/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_0ed9ed1e_24_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0038,data_trans_2025-02-12_10-41-28/epoch=6-val_rmse=8.70.ckpt'. Detail: [errno 2] No such file or directory\n",
      "\n",
      "2025-02-12 10:42:53,310\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.916 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:53,312\tWARNING util.py:201 -- The `process_trial_result` operation took 2.919 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:53,313\tWARNING util.py:201 -- Processing trial results took 2.919 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:42:53,313\tWARNING util.py:201 -- The `process_trial_result` operation took 2.919 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:54,512\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.198 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:54,514\tWARNING util.py:201 -- The `process_trial_result` operation took 1.201 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:54,515\tWARNING util.py:201 -- Processing trial results took 1.201 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:42:54,516\tWARNING util.py:201 -- The `process_trial_result` operation took 1.202 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3440497)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_0ed9ed1e_24_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0038,data_trans_2025-02-12_10-41-28/checkpoint_000010)\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "2025-02-12 10:42:56,793\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.230 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:56,796\tWARNING util.py:201 -- The `process_trial_result` operation took 2.232 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:56,796\tWARNING util.py:201 -- Processing trial results took 2.233 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:42:56,796\tWARNING util.py:201 -- The `process_trial_result` operation took 2.233 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:57,983\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.170 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:57,985\tWARNING util.py:201 -- The `process_trial_result` operation took 1.173 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:57,986\tWARNING util.py:201 -- Processing trial results took 1.173 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:42:57,986\tWARNING util.py:201 -- The `process_trial_result` operation took 1.173 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:59,478\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.476 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:59,480\tWARNING util.py:201 -- The `process_trial_result` operation took 1.478 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:42:59,481\tWARNING util.py:201 -- Processing trial results took 1.479 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:42:59,481\tWARNING util.py:201 -- The `process_trial_result` operation took 1.479 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:43:00,760\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.277 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:43:00,762\tWARNING util.py:201 -- The `process_trial_result` operation took 1.279 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:43:00,763\tWARNING util.py:201 -- Processing trial results took 1.280 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:43:00,763\tWARNING util.py:201 -- The `process_trial_result` operation took 1.281 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:43:03,596\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.818 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:43:03,598\tWARNING util.py:201 -- The `process_trial_result` operation took 2.820 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:43:03,599\tWARNING util.py:201 -- Processing trial results took 2.821 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:43:03,600\tWARNING util.py:201 -- The `process_trial_result` operation took 2.822 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3413652)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_e26bf114_8_batch_size=128,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0018,data_transf_2025-02-12_10-28-19/checkpoint_000097)\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "2025-02-12 10:43:05,781\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.100 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:43:05,784\tWARNING util.py:201 -- The `process_trial_result` operation took 2.103 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:43:05,784\tWARNING util.py:201 -- Processing trial results took 2.103 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:43:05,785\tWARNING util.py:201 -- The `process_trial_result` operation took 2.104 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:43:06,764\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 0.978 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:43:06,766\tWARNING util.py:201 -- The `process_trial_result` operation took 0.980 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:43:06,767\tWARNING util.py:201 -- Processing trial results took 0.981 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:43:06,767\tWARNING util.py:201 -- The `process_trial_result` operation took 0.981 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:43:08,194\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.404 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:43:08,196\tWARNING util.py:201 -- The `process_trial_result` operation took 1.406 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:43:08,197\tWARNING util.py:201 -- Processing trial results took 1.407 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:43:08,198\tWARNING util.py:201 -- The `process_trial_result` operation took 1.408 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:43:09,414\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.203 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:43:09,416\tWARNING util.py:201 -- The `process_trial_result` operation took 1.206 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:43:09,417\tWARNING util.py:201 -- Processing trial results took 1.206 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:43:09,417\tWARNING util.py:201 -- The `process_trial_result` operation took 1.207 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3413652)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_e26bf114_8_batch_size=128,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0018,data_transf_2025-02-12_10-28-19/checkpoint_000098)\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "2025-02-12 10:43:11,380\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.946 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:43:11,382\tWARNING util.py:201 -- The `process_trial_result` operation took 1.948 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:43:11,383\tWARNING util.py:201 -- Processing trial results took 1.949 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:43:11,383\tWARNING util.py:201 -- The `process_trial_result` operation took 1.949 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:43:14,610\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 3.191 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:43:14,612\tWARNING util.py:201 -- The `process_trial_result` operation took 3.194 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:43:14,613\tWARNING util.py:201 -- Processing trial results took 3.195 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:43:14,613\tWARNING util.py:201 -- The `process_trial_result` operation took 3.195 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:43:17,392\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.763 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:43:17,395\tWARNING util.py:201 -- The `process_trial_result` operation took 2.766 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:43:17,395\tWARNING util.py:201 -- Processing trial results took 2.766 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:43:17,396\tWARNING util.py:201 -- The `process_trial_result` operation took 2.767 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:43:18,839\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.206 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:43:18,840\tWARNING util.py:201 -- The `process_trial_result` operation took 1.208 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:43:18,841\tWARNING util.py:201 -- Processing trial results took 1.209 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:43:18,842\tWARNING util.py:201 -- The `process_trial_result` operation took 1.209 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3440497)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_0ed9ed1e_24_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0038,data_trans_2025-02-12_10-41-28/checkpoint_000011)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "2025-02-12 10:43:20,650\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.441 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:43:20,652\tWARNING util.py:201 -- The `process_trial_result` operation took 1.444 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:43:20,652\tWARNING util.py:201 -- Processing trial results took 1.444 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:43:20,653\tWARNING util.py:201 -- The `process_trial_result` operation took 1.444 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:43:21,880\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.195 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:43:21,882\tWARNING util.py:201 -- The `process_trial_result` operation took 1.197 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:43:21,883\tWARNING util.py:201 -- Processing trial results took 1.198 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:43:21,883\tWARNING util.py:201 -- The `process_trial_result` operation took 1.199 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:43:24,719\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.820 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:43:24,721\tWARNING util.py:201 -- The `process_trial_result` operation took 2.823 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:43:24,722\tWARNING util.py:201 -- Processing trial results took 2.823 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:43:24,722\tWARNING util.py:201 -- The `process_trial_result` operation took 2.824 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Seed set to 12\n",
      "\u001b[36m(train_nn_reg pid=3440497)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_0ed9ed1e_24_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0038,data_trans_2025-02-12_10-41-28/checkpoint_000012)\u001b[32m [repeated 4x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Train: (156, 1032), Test: (65, 1032)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_3c8a1025_26_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,_2025-02-12_10-43-17 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m 0 | layers | ModuleList | 560 K  | train\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m 560 K     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m 560 K     Total params\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m 2.241     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m 24        Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Metric train_rmse does not exist in `trainer.callback_metrics.\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Metric train_r2 does not exist in `trainer.callback_metrics.\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Metric train_loss does not exist in `trainer.callback_metrics.\n",
      "2025-02-12 10:43:28,640\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.442 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:43:28,643\tWARNING util.py:201 -- The `process_trial_result` operation took 2.445 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:43:28,643\tWARNING util.py:201 -- Processing trial results took 2.445 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:43:28,643\tWARNING util.py:201 -- The `process_trial_result` operation took 2.445 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:43:29,987\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.342 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:43:29,989\tWARNING util.py:201 -- The `process_trial_result` operation took 1.344 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:43:29,990\tWARNING util.py:201 -- Processing trial results took 1.345 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:43:29,990\tWARNING util.py:201 -- The `process_trial_result` operation took 1.345 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:43:31,151\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.159 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:43:31,153\tWARNING util.py:201 -- The `process_trial_result` operation took 1.161 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:43:31,155\tWARNING util.py:201 -- Processing trial results took 1.163 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:43:31,156\tWARNING util.py:201 -- The `process_trial_result` operation took 1.164 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_3c8a1025_26_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,_2025-02-12_10-43-17/checkpoint_000001)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2025-02-12 10:43:32,263\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.104 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:43:32,265\tWARNING util.py:201 -- The `process_trial_result` operation took 1.107 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:43:32,265\tWARNING util.py:201 -- Processing trial results took 1.107 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:43:32,266\tWARNING util.py:201 -- The `process_trial_result` operation took 1.108 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:43:35,657\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 3.364 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:43:35,659\tWARNING util.py:201 -- The `process_trial_result` operation took 3.366 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:43:35,660\tWARNING util.py:201 -- Processing trial results took 3.367 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:43:35,660\tWARNING util.py:201 -- The `process_trial_result` operation took 3.367 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:43:37,186\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 0.968 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:43:37,188\tWARNING util.py:201 -- The `process_trial_result` operation took 0.970 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:43:37,189\tWARNING util.py:201 -- Processing trial results took 0.971 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:43:37,189\tWARNING util.py:201 -- The `process_trial_result` operation took 0.971 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3440497)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_0ed9ed1e_24_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0038,data_trans_2025-02-12_10-41-28/checkpoint_000013)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2025-02-12 10:43:39,358\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.148 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:43:39,360\tWARNING util.py:201 -- The `process_trial_result` operation took 2.150 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:43:39,361\tWARNING util.py:201 -- Processing trial results took 2.151 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:43:39,361\tWARNING util.py:201 -- The `process_trial_result` operation took 2.151 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:43:40,758\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.353 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:43:40,761\tWARNING util.py:201 -- The `process_trial_result` operation took 1.356 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:43:40,762\tWARNING util.py:201 -- Processing trial results took 1.357 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:43:40,762\tWARNING util.py:201 -- The `process_trial_result` operation took 1.357 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:43:42,639\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.874 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:43:42,641\tWARNING util.py:201 -- The `process_trial_result` operation took 1.877 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:43:42,642\tWARNING util.py:201 -- Processing trial results took 1.877 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:43:42,642\tWARNING util.py:201 -- The `process_trial_result` operation took 1.878 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3422740)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_a70dbacd_15_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=11,data_transform=None_2025-02-12_10-33-04/checkpoint_000070)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_nn_reg pid=3444755)\u001b[0m Seed set to 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3444755)\u001b[0m Train: (156, 42), Test: (65, 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3444755)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3444755)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3444755)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3444755)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_7f5aa875_27_batch_size=64,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0006,data_transf_2025-02-12_10-43-36 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3444755)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3444755)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3444755)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3444755)\u001b[0m 0 | layers | ModuleList | 1.2 K  | train\n",
      "\u001b[36m(train_nn_reg pid=3444755)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3444755)\u001b[0m 1.2 K     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3444755)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3444755)\u001b[0m 1.2 K     Total params\n",
      "\u001b[36m(train_nn_reg pid=3444755)\u001b[0m 0.005     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3444755)\u001b[0m 4         Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3444755)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3444755)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3444755)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-02-12 10:43:46,581\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.015 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:43:46,583\tWARNING util.py:201 -- The `process_trial_result` operation took 1.017 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:43:46,583\tWARNING util.py:201 -- Processing trial results took 1.018 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:43:46,584\tWARNING util.py:201 -- The `process_trial_result` operation took 1.018 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:43:48,068\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.469 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:43:48,070\tWARNING util.py:201 -- The `process_trial_result` operation took 1.472 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:43:48,072\tWARNING util.py:201 -- Processing trial results took 1.473 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:43:48,072\tWARNING util.py:201 -- The `process_trial_result` operation took 1.474 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3440497)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_0ed9ed1e_24_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0038,data_trans_2025-02-12_10-41-28/checkpoint_000014)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "2025-02-12 10:43:50,021\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.934 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:43:50,023\tWARNING util.py:201 -- The `process_trial_result` operation took 1.937 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:43:50,024\tWARNING util.py:201 -- Processing trial results took 1.937 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:43:50,024\tWARNING util.py:201 -- The `process_trial_result` operation took 1.938 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:43:51,165\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.139 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:43:51,167\tWARNING util.py:201 -- The `process_trial_result` operation took 1.141 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:43:51,168\tWARNING util.py:201 -- Processing trial results took 1.142 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:43:51,168\tWARNING util.py:201 -- The `process_trial_result` operation took 1.143 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:43:51,186\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_7f5aa875\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3444755, ip=172.31.181.85, actor_id=5c86cd21ee603c62838e740501000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1024, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1053, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 1) cannot be concatenated\n",
      "2025-02-12 10:43:53,185\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.823 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:43:53,187\tWARNING util.py:201 -- The `process_trial_result` operation took 1.826 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:43:53,188\tWARNING util.py:201 -- Processing trial results took 1.827 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:43:53,188\tWARNING util.py:201 -- The `process_trial_result` operation took 1.827 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_3c8a1025_26_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,_2025-02-12_10-43-17/checkpoint_000003)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2025-02-12 10:43:57,028\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 3.824 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:43:57,031\tWARNING util.py:201 -- The `process_trial_result` operation took 3.826 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:43:57,032\tWARNING util.py:201 -- Processing trial results took 3.827 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:43:57,032\tWARNING util.py:201 -- The `process_trial_result` operation took 3.828 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:43:58,238\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.190 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:43:58,240\tWARNING util.py:201 -- The `process_trial_result` operation took 1.192 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:43:58,241\tWARNING util.py:201 -- Processing trial results took 1.193 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:43:58,241\tWARNING util.py:201 -- The `process_trial_result` operation took 1.193 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3445363)\u001b[0m Seed set to 12\n",
      "\u001b[36m(train_nn_reg pid=3422740)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_a70dbacd_15_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=11,data_transform=None_2025-02-12_10-33-04/checkpoint_000071)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3445363)\u001b[0m Train: (156, 42), Test: (65, 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3445363)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3445363)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3445363)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3445363)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_b1eab41a_28_batch_size=64,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0007,data_transf_2025-02-12_10-43-51 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3445363)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3445363)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3445363)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3445363)\u001b[0m 0 | layers | ModuleList | 1.2 K  | train\n",
      "\u001b[36m(train_nn_reg pid=3445363)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3445363)\u001b[0m 1.2 K     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3445363)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3445363)\u001b[0m 1.2 K     Total params\n",
      "\u001b[36m(train_nn_reg pid=3445363)\u001b[0m 0.005     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3445363)\u001b[0m 4         Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3445363)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3445363)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3445363)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-02-12 10:44:00,692\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.477 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:44:00,694\tWARNING util.py:201 -- The `process_trial_result` operation took 1.479 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:44:00,695\tWARNING util.py:201 -- Processing trial results took 1.480 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:44:00,695\tWARNING util.py:201 -- The `process_trial_result` operation took 1.481 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:44:00,730\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_b1eab41a\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3445363, ip=172.31.181.85, actor_id=78ef02f83e5cf1205f461a8401000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1024, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1053, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 1) cannot be concatenated\n",
      "\u001b[36m(train_nn_reg pid=3440497)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_0ed9ed1e_24_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0038,data_trans_2025-02-12_10-41-28/checkpoint_000016)\n",
      "2025-02-12 10:44:02,404\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.329 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:44:02,406\tWARNING util.py:201 -- The `process_trial_result` operation took 1.331 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:44:02,406\tWARNING util.py:201 -- Processing trial results took 1.332 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:44:02,407\tWARNING util.py:201 -- The `process_trial_result` operation took 1.332 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:44:03,844\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.402 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:44:03,846\tWARNING util.py:201 -- The `process_trial_result` operation took 1.404 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:44:03,847\tWARNING util.py:201 -- Processing trial results took 1.405 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:44:03,848\tWARNING util.py:201 -- The `process_trial_result` operation took 1.406 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:44:03,864\tWARNING syncer.py:406 -- Last sync command failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 404, in _launch_sync_process\n",
      "    self.wait()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 474, in wait\n",
      "    raise e\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 472, in wait\n",
      "    self._sync_process.wait(timeout=timeout or self.sync_timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 173, in wait\n",
      "    raise exception\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 136, in entrypoint\n",
      "    result = self._fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/storage.py\", line 218, in _upload_to_fs_path\n",
      "    _pyarrow_fs_copy_files(local_path, fs_path, destination_filesystem=fs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/storage.py\", line 116, in _pyarrow_fs_copy_files\n",
      "    return pyarrow.fs.copy_files(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/pyarrow/fs.py\", line 259, in copy_files\n",
      "    _copy_files_selector(source_fs, source_sel,\n",
      "  File \"pyarrow/_fs.pyx\", line 1624, in pyarrow._fs._copy_files_selector\n",
      "  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\n",
      "FileNotFoundError: [Errno 2] Failed to open local file '/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_3c8a1025_26_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,_2025-02-12_10-43-17/epoch=1-val_rmse=6.42.ckpt'. Detail: [errno 2] No such file or directory\n",
      "\n",
      "2025-02-12 10:44:07,127\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 3.261 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:44:07,129\tWARNING util.py:201 -- The `process_trial_result` operation took 3.263 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:44:07,130\tWARNING util.py:201 -- Processing trial results took 3.264 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:44:07,130\tWARNING util.py:201 -- The `process_trial_result` operation took 3.264 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:44:08,860\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.728 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:44:08,862\tWARNING util.py:201 -- The `process_trial_result` operation took 1.731 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:44:08,863\tWARNING util.py:201 -- Processing trial results took 1.731 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:44:08,863\tWARNING util.py:201 -- The `process_trial_result` operation took 1.732 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3440497)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_0ed9ed1e_24_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0038,data_trans_2025-02-12_10-41-28/checkpoint_000018)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_nn_reg pid=3445857)\u001b[0m Seed set to 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3445857)\u001b[0m Train: (156, 42), Test: (65, 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3445857)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3445857)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3445857)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3445857)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_52256c85_29_batch_size=64,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0007,data_transf_2025-02-12_10-44-01 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3445857)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3445857)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3445857)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3445857)\u001b[0m 0 | layers | ModuleList | 1.2 K  | train\n",
      "\u001b[36m(train_nn_reg pid=3445857)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3445857)\u001b[0m 1.2 K     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3445857)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3445857)\u001b[0m 1.2 K     Total params\n",
      "\u001b[36m(train_nn_reg pid=3445857)\u001b[0m 0.005     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3445857)\u001b[0m 4         Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3445857)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3445857)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3445857)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-02-12 10:44:11,226\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.245 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:44:11,228\tWARNING util.py:201 -- The `process_trial_result` operation took 1.247 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:44:11,229\tWARNING util.py:201 -- Processing trial results took 1.247 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:44:11,229\tWARNING util.py:201 -- The `process_trial_result` operation took 1.248 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:44:11,232\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_52256c85\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3445857, ip=172.31.181.85, actor_id=c5d7c201209a183b30c432cf01000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1024, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1053, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 1) cannot be concatenated\n",
      "2025-02-12 10:44:13,046\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.645 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:44:13,048\tWARNING util.py:201 -- The `process_trial_result` operation took 1.648 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:44:13,049\tWARNING util.py:201 -- Processing trial results took 1.648 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:44:13,049\tWARNING util.py:201 -- The `process_trial_result` operation took 1.649 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:44:15,401\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.327 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:44:15,403\tWARNING util.py:201 -- The `process_trial_result` operation took 2.330 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:44:15,404\tWARNING util.py:201 -- Processing trial results took 2.330 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:44:15,404\tWARNING util.py:201 -- The `process_trial_result` operation took 2.331 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3417235)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_4450d409_11_batch_size=128,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0031,data_trans_2025-02-12_10-29-59/checkpoint_000098)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "2025-02-12 10:44:19,042\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 3.580 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:44:19,044\tWARNING util.py:201 -- The `process_trial_result` operation took 3.582 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:44:19,044\tWARNING util.py:201 -- Processing trial results took 3.583 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:44:19,045\tWARNING util.py:201 -- The `process_trial_result` operation took 3.583 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:44:20,605\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.559 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:44:20,607\tWARNING util.py:201 -- The `process_trial_result` operation took 1.561 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:44:20,608\tWARNING util.py:201 -- Processing trial results took 1.562 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:44:20,608\tWARNING util.py:201 -- The `process_trial_result` operation took 1.562 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3446244)\u001b[0m Seed set to 12\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_3c8a1025_26_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,_2025-02-12_10-43-17/checkpoint_000005)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3446244)\u001b[0m Train: (156, 42), Test: (65, 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3446244)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3446244)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3446244)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3446244)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_fe0d4d04_30_batch_size=64,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0008,data_transf_2025-02-12_10-44-11 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3446244)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3446244)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3446244)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3446244)\u001b[0m 0 | layers | ModuleList | 1.2 K  | train\n",
      "\u001b[36m(train_nn_reg pid=3446244)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3446244)\u001b[0m 1.2 K     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3446244)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3446244)\u001b[0m 1.2 K     Total params\n",
      "\u001b[36m(train_nn_reg pid=3446244)\u001b[0m 0.005     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3446244)\u001b[0m 4         Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3446244)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3446244)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3446244)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-02-12 10:44:23,491\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.979 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:44:23,493\tWARNING util.py:201 -- The `process_trial_result` operation took 1.982 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:44:23,494\tWARNING util.py:201 -- Processing trial results took 1.983 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:44:23,495\tWARNING util.py:201 -- The `process_trial_result` operation took 1.983 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3417235)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_4450d409_11_batch_size=128,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0031,data_trans_2025-02-12_10-29-59/checkpoint_000099)\n",
      "2025-02-12 10:44:24,746\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.237 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:44:24,748\tWARNING util.py:201 -- The `process_trial_result` operation took 1.239 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:44:24,748\tWARNING util.py:201 -- Processing trial results took 1.239 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:44:24,749\tWARNING util.py:201 -- The `process_trial_result` operation took 1.240 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:44:26,699\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.948 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:44:26,700\tWARNING util.py:201 -- The `process_trial_result` operation took 1.950 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:44:26,701\tWARNING util.py:201 -- Processing trial results took 1.951 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:44:26,701\tWARNING util.py:201 -- The `process_trial_result` operation took 1.951 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:44:28,988\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_fe0d4d04\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3446244, ip=172.31.181.85, actor_id=d993a045b27549964424b45101000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1024, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1053, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 1) cannot be concatenated\n",
      "\u001b[36m(train_nn_reg pid=3440497)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_0ed9ed1e_24_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0038,data_trans_2025-02-12_10-41-28/checkpoint_000020)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "2025-02-12 10:44:30,619\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.392 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:44:30,621\tWARNING util.py:201 -- The `process_trial_result` operation took 1.394 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:44:30,622\tWARNING util.py:201 -- Processing trial results took 1.395 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:44:30,622\tWARNING util.py:201 -- The `process_trial_result` operation took 1.395 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:44:31,623\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.000 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:44:31,625\tWARNING util.py:201 -- The `process_trial_result` operation took 1.002 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:44:31,626\tWARNING util.py:201 -- Processing trial results took 1.003 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:44:31,627\tWARNING util.py:201 -- The `process_trial_result` operation took 1.003 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:44:33,225\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.303 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:44:33,227\tWARNING util.py:201 -- The `process_trial_result` operation took 1.305 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:44:33,228\tWARNING util.py:201 -- Processing trial results took 1.306 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:44:33,228\tWARNING util.py:201 -- The `process_trial_result` operation took 1.307 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:44:36,324\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 3.073 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:44:36,326\tWARNING util.py:201 -- The `process_trial_result` operation took 3.074 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:44:36,326\tWARNING util.py:201 -- Processing trial results took 3.075 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:44:36,327\tWARNING util.py:201 -- The `process_trial_result` operation took 3.075 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3446692)\u001b[0m Seed set to 12\n",
      "\u001b[36m(train_nn_reg pid=3440497)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_0ed9ed1e_24_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0038,data_trans_2025-02-12_10-41-28/checkpoint_000022)\u001b[32m [repeated 4x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3446692)\u001b[0m Train: (156, 42), Test: (65, 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3446692)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3446692)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3446692)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3446692)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_a68b3ce9_31_batch_size=64,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0013,data_transf_2025-02-12_10-44-28 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3446692)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3446692)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3446692)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3446692)\u001b[0m 0 | layers | ModuleList | 1.2 K  | train\n",
      "\u001b[36m(train_nn_reg pid=3446692)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3446692)\u001b[0m 1.2 K     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3446692)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3446692)\u001b[0m 1.2 K     Total params\n",
      "\u001b[36m(train_nn_reg pid=3446692)\u001b[0m 0.005     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3446692)\u001b[0m 4         Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3446692)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3446692)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3446692)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-02-12 10:44:38,521\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.189 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:44:38,524\tWARNING util.py:201 -- The `process_trial_result` operation took 1.191 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:44:38,524\tWARNING util.py:201 -- Processing trial results took 1.192 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:44:38,525\tWARNING util.py:201 -- The `process_trial_result` operation took 1.192 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:44:38,550\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_a68b3ce9\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3446692, ip=172.31.181.85, actor_id=a60714cfe4f9c2bcedec84fa01000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1024, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1053, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 1) cannot be concatenated\n",
      "2025-02-12 10:44:40,039\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.208 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:44:40,041\tWARNING util.py:201 -- The `process_trial_result` operation took 1.210 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:44:40,042\tWARNING util.py:201 -- Processing trial results took 1.211 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:44:40,042\tWARNING util.py:201 -- The `process_trial_result` operation took 1.211 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:44:41,574\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.530 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:44:41,576\tWARNING util.py:201 -- The `process_trial_result` operation took 1.532 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:44:41,577\tWARNING util.py:201 -- Processing trial results took 1.533 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:44:41,577\tWARNING util.py:201 -- The `process_trial_result` operation took 1.533 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:44:42,572\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 0.993 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:44:42,574\tWARNING util.py:201 -- The `process_trial_result` operation took 0.995 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:44:42,574\tWARNING util.py:201 -- Processing trial results took 0.996 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:44:42,575\tWARNING util.py:201 -- The `process_trial_result` operation took 0.996 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3422740)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_a70dbacd_15_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=11,data_transform=None_2025-02-12_10-33-04/checkpoint_000077)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2025-02-12 10:44:44,017\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.074 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:44:44,019\tWARNING util.py:201 -- The `process_trial_result` operation took 1.076 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:44:44,019\tWARNING util.py:201 -- Processing trial results took 1.077 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:44:44,020\tWARNING util.py:201 -- The `process_trial_result` operation took 1.077 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:44:47,521\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 3.486 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:44:47,523\tWARNING util.py:201 -- The `process_trial_result` operation took 3.489 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:44:47,524\tWARNING util.py:201 -- Processing trial results took 3.489 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:44:47,524\tWARNING util.py:201 -- The `process_trial_result` operation took 3.490 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:44:49,014\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.488 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:44:49,016\tWARNING util.py:201 -- The `process_trial_result` operation took 1.490 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:44:49,016\tWARNING util.py:201 -- Processing trial results took 1.490 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:44:49,017\tWARNING util.py:201 -- The `process_trial_result` operation took 1.491 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3440497)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_0ed9ed1e_24_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0038,data_trans_2025-02-12_10-41-28/checkpoint_000025)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_nn_reg pid=3447018)\u001b[0m Seed set to 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3447018)\u001b[0m Train: (156, 42), Test: (65, 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3447018)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3447018)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3447018)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3447018)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_6a6e83a2_32_batch_size=64,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0011,data_transf_2025-02-12_10-44-37 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3447018)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3447018)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3447018)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3447018)\u001b[0m 0 | layers | ModuleList | 1.2 K  | train\n",
      "\u001b[36m(train_nn_reg pid=3447018)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3447018)\u001b[0m 1.2 K     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3447018)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3447018)\u001b[0m 1.2 K     Total params\n",
      "\u001b[36m(train_nn_reg pid=3447018)\u001b[0m 0.005     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3447018)\u001b[0m 4         Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3447018)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3447018)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3447018)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-02-12 10:44:51,333\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.257 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:44:51,336\tWARNING util.py:201 -- The `process_trial_result` operation took 1.259 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:44:51,336\tWARNING util.py:201 -- Processing trial results took 1.260 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:44:51,336\tWARNING util.py:201 -- The `process_trial_result` operation took 1.260 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:44:52,523\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.184 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:44:52,525\tWARNING util.py:201 -- The `process_trial_result` operation took 1.186 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:44:52,526\tWARNING util.py:201 -- Processing trial results took 1.186 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:44:52,526\tWARNING util.py:201 -- The `process_trial_result` operation took 1.187 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:44:52,529\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_6a6e83a2\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3447018, ip=172.31.181.85, actor_id=e857d35ce066f9bdb4133e5101000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1024, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1053, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 1) cannot be concatenated\n",
      "2025-02-12 10:44:54,290\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.289 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:44:54,293\tWARNING util.py:201 -- The `process_trial_result` operation took 1.291 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:44:54,293\tWARNING util.py:201 -- Processing trial results took 1.292 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:44:54,294\tWARNING util.py:201 -- The `process_trial_result` operation took 1.292 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:44:57,893\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 3.570 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:44:57,896\tWARNING util.py:201 -- The `process_trial_result` operation took 3.573 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:44:57,897\tWARNING util.py:201 -- Processing trial results took 3.573 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:44:57,897\tWARNING util.py:201 -- The `process_trial_result` operation took 3.574 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:44:59,468\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.569 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:44:59,470\tWARNING util.py:201 -- The `process_trial_result` operation took 1.571 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:44:59,471\tWARNING util.py:201 -- Processing trial results took 1.572 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:44:59,472\tWARNING util.py:201 -- The `process_trial_result` operation took 1.572 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_3c8a1025_26_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,_2025-02-12_10-43-17/checkpoint_000008)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_nn_reg pid=3447518)\u001b[0m Seed set to 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3447518)\u001b[0m Train: (156, 42), Test: (65, 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3447518)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3447518)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3447518)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3447518)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_658848a7_33_batch_size=32,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0011,data_transf_2025-02-12_10-44-50 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3447518)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3447518)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3447518)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3447518)\u001b[0m 0 | layers | ModuleList | 1.2 K  | train\n",
      "\u001b[36m(train_nn_reg pid=3447518)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3447518)\u001b[0m 1.2 K     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3447518)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3447518)\u001b[0m 1.2 K     Total params\n",
      "\u001b[36m(train_nn_reg pid=3447518)\u001b[0m 0.005     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3447518)\u001b[0m 4         Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3447518)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3447518)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(train_nn_reg pid=3447518)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3447518)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-02-12 10:45:02,166\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.410 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:45:02,169\tWARNING util.py:201 -- The `process_trial_result` operation took 1.413 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:45:02,169\tWARNING util.py:201 -- Processing trial results took 1.413 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:45:02,170\tWARNING util.py:201 -- The `process_trial_result` operation took 1.414 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:45:02,173\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_658848a7\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3447518, ip=172.31.181.85, actor_id=7b6dfc61ec6ea1c845718ca901000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1026, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 216, in run\n",
      "    self.advance()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 455, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 151, in run\n",
      "    self.on_advance_end(data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 370, in on_advance_end\n",
      "    self.val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 2) cannot be concatenated\n",
      "2025-02-12 10:45:03,905\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.596 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:45:03,907\tWARNING util.py:201 -- The `process_trial_result` operation took 1.599 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:45:03,908\tWARNING util.py:201 -- Processing trial results took 1.600 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:45:03,908\tWARNING util.py:201 -- The `process_trial_result` operation took 1.600 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:45:04,983\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.073 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:45:04,985\tWARNING util.py:201 -- The `process_trial_result` operation took 1.075 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:45:04,985\tWARNING util.py:201 -- Processing trial results took 1.076 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:45:04,986\tWARNING util.py:201 -- The `process_trial_result` operation took 1.076 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3422740)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_a70dbacd_15_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=11,data_transform=None_2025-02-12_10-33-04/checkpoint_000081)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_nn_reg pid=3447752)\u001b[0m Seed set to 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3447752)\u001b[0m Train: (156, 42), Test: (65, 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3447752)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3447752)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3447752)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3447752)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_d7e6c1b5_34_batch_size=32,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0010,data_transf_2025-02-12_10-45-00 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3447752)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3447752)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3447752)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3447752)\u001b[0m 0 | layers | ModuleList | 1.2 K  | train\n",
      "\u001b[36m(train_nn_reg pid=3447752)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3447752)\u001b[0m 1.2 K     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3447752)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3447752)\u001b[0m 1.2 K     Total params\n",
      "\u001b[36m(train_nn_reg pid=3447752)\u001b[0m 0.005     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3447752)\u001b[0m 4         Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3447752)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3447752)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(train_nn_reg pid=3447752)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3447752)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-02-12 10:45:09,281\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.124 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:45:09,283\tWARNING util.py:201 -- The `process_trial_result` operation took 1.126 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:45:09,283\tWARNING util.py:201 -- Processing trial results took 1.127 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:45:09,284\tWARNING util.py:201 -- The `process_trial_result` operation took 1.127 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:45:10,530\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.243 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:45:10,532\tWARNING util.py:201 -- The `process_trial_result` operation took 1.245 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:45:10,533\tWARNING util.py:201 -- Processing trial results took 1.247 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:45:10,534\tWARNING util.py:201 -- The `process_trial_result` operation took 1.247 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3422740)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_a70dbacd_15_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=11,data_transform=None_2025-02-12_10-33-04/checkpoint_000082)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2025-02-12 10:45:12,404\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.810 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:45:12,406\tWARNING util.py:201 -- The `process_trial_result` operation took 1.812 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:45:12,407\tWARNING util.py:201 -- Processing trial results took 1.812 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:45:12,407\tWARNING util.py:201 -- The `process_trial_result` operation took 1.813 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:45:13,427\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.018 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:45:13,431\tWARNING util.py:201 -- The `process_trial_result` operation took 1.022 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:45:13,431\tWARNING util.py:201 -- Processing trial results took 1.023 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:45:13,431\tWARNING util.py:201 -- The `process_trial_result` operation took 1.023 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:45:17,151\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 3.656 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:45:17,154\tWARNING util.py:201 -- The `process_trial_result` operation took 3.659 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:45:17,154\tWARNING util.py:201 -- Processing trial results took 3.659 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:45:17,155\tWARNING util.py:201 -- The `process_trial_result` operation took 3.660 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:45:18,172\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.000 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:45:18,174\tWARNING util.py:201 -- The `process_trial_result` operation took 1.003 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:45:18,175\tWARNING util.py:201 -- Processing trial results took 1.003 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:45:18,175\tWARNING util.py:201 -- The `process_trial_result` operation took 1.004 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3448363)\u001b[0m Seed set to 12\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_3c8a1025_26_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,_2025-02-12_10-43-17/checkpoint_000011)\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3448363)\u001b[0m Train: (156, 42), Test: (65, 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3448363)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3448363)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3448363)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3448363)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_c085a7aa_35_batch_size=64,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0010,data_transf_2025-02-12_10-45-08 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3448363)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3448363)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3448363)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3448363)\u001b[0m 0 | layers | ModuleList | 1.2 K  | train\n",
      "\u001b[36m(train_nn_reg pid=3448363)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3448363)\u001b[0m 1.2 K     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3448363)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3448363)\u001b[0m 1.2 K     Total params\n",
      "\u001b[36m(train_nn_reg pid=3448363)\u001b[0m 0.005     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3448363)\u001b[0m 4         Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3448363)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3448363)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3448363)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-02-12 10:45:20,808\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.711 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:45:20,810\tWARNING util.py:201 -- The `process_trial_result` operation took 1.713 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:45:20,811\tWARNING util.py:201 -- Processing trial results took 1.714 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:45:20,812\tWARNING util.py:201 -- The `process_trial_result` operation took 1.715 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:45:22,050\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.219 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:45:22,052\tWARNING util.py:201 -- The `process_trial_result` operation took 1.221 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:45:22,052\tWARNING util.py:201 -- Processing trial results took 1.222 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:45:22,053\tWARNING util.py:201 -- The `process_trial_result` operation took 1.222 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:45:23,659\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.605 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:45:23,661\tWARNING util.py:201 -- The `process_trial_result` operation took 1.607 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:45:23,662\tWARNING util.py:201 -- Processing trial results took 1.608 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:45:23,662\tWARNING util.py:201 -- The `process_trial_result` operation took 1.608 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:45:23,739\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_d7e6c1b5\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3447752, ip=172.31.181.85, actor_id=d9f49087d81cf21bbf33b3c001000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1026, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 216, in run\n",
      "    self.advance()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 455, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 151, in run\n",
      "    self.on_advance_end(data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 370, in on_advance_end\n",
      "    self.val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 2) cannot be concatenated\n",
      "\u001b[36m(train_nn_reg pid=3422740)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_a70dbacd_15_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=11,data_transform=None_2025-02-12_10-33-04/checkpoint_000085)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2025-02-12 10:45:27,265\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.967 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:45:27,267\tWARNING util.py:201 -- The `process_trial_result` operation took 1.970 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:45:27,268\tWARNING util.py:201 -- Processing trial results took 1.970 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:45:27,268\tWARNING util.py:201 -- The `process_trial_result` operation took 1.971 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:45:28,939\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.666 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:45:28,942\tWARNING util.py:201 -- The `process_trial_result` operation took 1.668 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:45:28,942\tWARNING util.py:201 -- Processing trial results took 1.669 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:45:28,943\tWARNING util.py:201 -- The `process_trial_result` operation took 1.670 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:45:30,070\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.108 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:45:30,072\tWARNING util.py:201 -- The `process_trial_result` operation took 1.110 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:45:30,073\tWARNING util.py:201 -- Processing trial results took 1.112 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:45:30,073\tWARNING util.py:201 -- The `process_trial_result` operation took 1.112 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:45:30,121\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_c085a7aa\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3448363, ip=172.31.181.85, actor_id=32a1750f78b358d85478dbae01000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1024, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1053, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 1) cannot be concatenated\n",
      "\u001b[36m(train_nn_reg pid=3440497)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_0ed9ed1e_24_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0038,data_trans_2025-02-12_10-41-28/checkpoint_000031)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2025-02-12 10:45:31,438\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.028 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:45:31,441\tWARNING util.py:201 -- The `process_trial_result` operation took 1.031 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:45:31,442\tWARNING util.py:201 -- Processing trial results took 1.032 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:45:31,442\tWARNING util.py:201 -- The `process_trial_result` operation took 1.032 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:45:32,652\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.208 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:45:32,655\tWARNING util.py:201 -- The `process_trial_result` operation took 1.211 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:45:32,656\tWARNING util.py:201 -- Processing trial results took 1.212 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:45:32,656\tWARNING util.py:201 -- The `process_trial_result` operation took 1.212 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3449111)\u001b[0m Seed set to 12\n",
      "2025-02-12 10:45:33,688\tERROR experiment_state.py:173 -- Saving experiment state to storage at '/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg' failed with exception: \n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/execution/experiment_state.py\", line 171, in wait_for_sync\n",
      "    self._storage.syncer.wait()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 474, in wait\n",
      "    raise e\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 472, in wait\n",
      "    self._sync_process.wait(timeout=timeout or self.sync_timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 173, in wait\n",
      "    raise exception\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 136, in entrypoint\n",
      "    result = self._fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/storage.py\", line 218, in _upload_to_fs_path\n",
      "    _pyarrow_fs_copy_files(local_path, fs_path, destination_filesystem=fs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/storage.py\", line 116, in _pyarrow_fs_copy_files\n",
      "    return pyarrow.fs.copy_files(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/pyarrow/fs.py\", line 259, in copy_files\n",
      "    _copy_files_selector(source_fs, source_sel,\n",
      "  File \"pyarrow/_fs.pyx\", line 1624, in pyarrow._fs._copy_files_selector\n",
      "  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\n",
      "FileNotFoundError: [Errno 2] Failed to open local file '/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_3c8a1025_26_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,_2025-02-12_10-43-17/epoch=8-val_rmse=1.04.ckpt'. Detail: [errno 2] No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3449111)\u001b[0m Train: (156, 42), Test: (65, 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3449111)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3449111)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3449111)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3449111)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_832a9f56_36_batch_size=64,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0011,data_transf_2025-02-12_10-45-25 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3449111)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3449111)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3449111)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3449111)\u001b[0m 0 | layers | ModuleList | 1.2 K  | train\n",
      "\u001b[36m(train_nn_reg pid=3449111)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3449111)\u001b[0m 1.2 K     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3449111)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3449111)\u001b[0m 1.2 K     Total params\n",
      "\u001b[36m(train_nn_reg pid=3449111)\u001b[0m 0.005     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3449111)\u001b[0m 4         Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3449111)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3449111)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3449111)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-02-12 10:45:37,523\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 3.359 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:45:37,525\tWARNING util.py:201 -- The `process_trial_result` operation took 3.361 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:45:37,525\tWARNING util.py:201 -- Processing trial results took 3.362 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:45:37,526\tWARNING util.py:201 -- The `process_trial_result` operation took 3.362 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:45:39,343\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.816 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:45:39,345\tWARNING util.py:201 -- The `process_trial_result` operation took 1.818 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:45:39,346\tWARNING util.py:201 -- Processing trial results took 1.819 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:45:39,346\tWARNING util.py:201 -- The `process_trial_result` operation took 1.820 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3422740)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_a70dbacd_15_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=11,data_transform=None_2025-02-12_10-33-04/checkpoint_000088)\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "2025-02-12 10:45:40,377\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_832a9f56\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3449111, ip=172.31.181.85, actor_id=a8a472c5f13189209b587e4b01000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1024, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1053, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 1) cannot be concatenated\n",
      "\u001b[36m(train_nn_reg pid=3449654)\u001b[0m Seed set to 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3449654)\u001b[0m Train: (156, 42), Test: (65, 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3449654)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3449654)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3449654)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3449654)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_eee28542_37_batch_size=64,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0011,data_transf_2025-02-12_10-45-33 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3449654)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3449654)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3449654)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3449654)\u001b[0m 0 | layers | ModuleList | 1.2 K  | train\n",
      "\u001b[36m(train_nn_reg pid=3449654)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3449654)\u001b[0m 1.2 K     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3449654)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3449654)\u001b[0m 1.2 K     Total params\n",
      "\u001b[36m(train_nn_reg pid=3449654)\u001b[0m 0.005     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3449654)\u001b[0m 4         Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3449654)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3449654)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3449654)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-02-12 10:45:41,937\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.363 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:45:41,939\tWARNING util.py:201 -- The `process_trial_result` operation took 1.366 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:45:41,940\tWARNING util.py:201 -- Processing trial results took 1.366 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:45:41,940\tWARNING util.py:201 -- The `process_trial_result` operation took 1.367 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:45:43,573\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.629 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:45:43,575\tWARNING util.py:201 -- The `process_trial_result` operation took 1.631 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:45:43,576\tWARNING util.py:201 -- Processing trial results took 1.632 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:45:43,577\tWARNING util.py:201 -- The `process_trial_result` operation took 1.633 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:45:43,581\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_eee28542\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3449654, ip=172.31.181.85, actor_id=7b7e858d44e940a06c23e71001000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1024, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1053, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 1) cannot be concatenated\n",
      "2025-02-12 10:45:47,131\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 3.401 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:45:47,133\tWARNING util.py:201 -- The `process_trial_result` operation took 3.403 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:45:47,134\tWARNING util.py:201 -- Processing trial results took 3.404 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:45:47,134\tWARNING util.py:201 -- The `process_trial_result` operation took 3.404 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3422740)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_a70dbacd_15_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=11,data_transform=None_2025-02-12_10-33-04/checkpoint_000089)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_nn_reg pid=3450075)\u001b[0m Seed set to 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3450075)\u001b[0m Train: (156, 42), Test: (65, 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3450075)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3450075)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3450075)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3450075)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_8d531d6a_38_batch_size=32,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0011,data_transf_2025-02-12_10-45-40 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3450075)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3450075)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3450075)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3450075)\u001b[0m 0 | layers | ModuleList | 1.2 K  | train\n",
      "\u001b[36m(train_nn_reg pid=3450075)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3450075)\u001b[0m 1.2 K     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3450075)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3450075)\u001b[0m 1.2 K     Total params\n",
      "\u001b[36m(train_nn_reg pid=3450075)\u001b[0m 0.005     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3450075)\u001b[0m 4         Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3450075)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3450075)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(train_nn_reg pid=3450075)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3450075)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-02-12 10:45:49,883\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.686 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:45:49,885\tWARNING util.py:201 -- The `process_trial_result` operation took 1.688 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:45:49,885\tWARNING util.py:201 -- Processing trial results took 1.688 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:45:49,886\tWARNING util.py:201 -- The `process_trial_result` operation took 1.689 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:45:51,060\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.172 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:45:51,062\tWARNING util.py:201 -- The `process_trial_result` operation took 1.174 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:45:51,063\tWARNING util.py:201 -- Processing trial results took 1.175 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:45:51,064\tWARNING util.py:201 -- The `process_trial_result` operation took 1.176 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:45:51,067\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_8d531d6a\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3450075, ip=172.31.181.85, actor_id=8f0e06a56ea53de2409538c801000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1026, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 216, in run\n",
      "    self.advance()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 455, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 151, in run\n",
      "    self.on_advance_end(data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 370, in on_advance_end\n",
      "    self.val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 2) cannot be concatenated\n",
      "2025-02-12 10:45:52,398\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.178 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:45:52,400\tWARNING util.py:201 -- The `process_trial_result` operation took 1.181 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:45:52,400\tWARNING util.py:201 -- Processing trial results took 1.181 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:45:52,401\tWARNING util.py:201 -- The `process_trial_result` operation took 1.181 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3422740)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_a70dbacd_15_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=11,data_transform=None_2025-02-12_10-33-04/checkpoint_000090)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "2025-02-12 10:45:53,665\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.232 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:45:53,667\tWARNING util.py:201 -- The `process_trial_result` operation took 1.235 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:45:53,668\tWARNING util.py:201 -- Processing trial results took 1.236 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:45:53,669\tWARNING util.py:201 -- The `process_trial_result` operation took 1.236 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3450351)\u001b[0m Seed set to 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3450351)\u001b[0m Train: (156, 42), Test: (65, 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3450351)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3450351)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3450351)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3450351)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_6b109c2e_39_batch_size=32,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0010,data_transf_2025-02-12_10-45-48 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3450351)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3450351)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3450351)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3450351)\u001b[0m 0 | layers | ModuleList | 1.2 K  | train\n",
      "\u001b[36m(train_nn_reg pid=3450351)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3450351)\u001b[0m 1.2 K     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3450351)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3450351)\u001b[0m 1.2 K     Total params\n",
      "\u001b[36m(train_nn_reg pid=3450351)\u001b[0m 0.005     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3450351)\u001b[0m 4         Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3450351)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3450351)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(train_nn_reg pid=3450351)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3450351)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-02-12 10:45:58,329\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 3.751 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:45:58,331\tWARNING util.py:201 -- The `process_trial_result` operation took 3.753 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:45:58,332\tWARNING util.py:201 -- Processing trial results took 3.754 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:45:58,333\tWARNING util.py:201 -- The `process_trial_result` operation took 3.755 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:45:58,355\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_6b109c2e\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3450351, ip=172.31.181.85, actor_id=4283eb1cf991fd24fbe619c801000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1026, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 216, in run\n",
      "    self.advance()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 455, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 151, in run\n",
      "    self.on_advance_end(data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 370, in on_advance_end\n",
      "    self.val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 2) cannot be concatenated\n",
      "\u001b[36m(train_nn_reg pid=3422740)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_a70dbacd_15_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=11,data_transform=None_2025-02-12_10-33-04/checkpoint_000091)\n",
      "\u001b[36m(train_nn_reg pid=3440497)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_0ed9ed1e_24_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0038,data_trans_2025-02-12_10-41-28/checkpoint_000035)\n",
      "2025-02-12 10:46:00,739\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.224 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:46:00,742\tWARNING util.py:201 -- The `process_trial_result` operation took 2.227 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:46:00,742\tWARNING util.py:201 -- Processing trial results took 2.227 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:46:00,742\tWARNING util.py:201 -- The `process_trial_result` operation took 2.228 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:46:01,990\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.245 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:46:01,992\tWARNING util.py:201 -- The `process_trial_result` operation took 1.247 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:46:01,993\tWARNING util.py:201 -- Processing trial results took 1.248 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:46:01,993\tWARNING util.py:201 -- The `process_trial_result` operation took 1.249 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:46:02,990\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 0.995 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:46:02,992\tWARNING util.py:201 -- The `process_trial_result` operation took 0.998 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:46:02,993\tWARNING util.py:201 -- Processing trial results took 0.998 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:46:02,993\tWARNING util.py:201 -- The `process_trial_result` operation took 0.999 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3450819)\u001b[0m Seed set to 12\n",
      "\u001b[36m(train_nn_reg pid=3440497)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_0ed9ed1e_24_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0038,data_trans_2025-02-12_10-41-28/checkpoint_000036)\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3450819)\u001b[0m Train: (156, 42), Test: (65, 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3450819)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3450819)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3450819)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3450819)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_5473c5fe_40_batch_size=64,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0011,data_transf_2025-02-12_10-45-54 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3450819)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3450819)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3450819)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3450819)\u001b[0m 0 | layers | ModuleList | 1.2 K  | train\n",
      "\u001b[36m(train_nn_reg pid=3450819)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3450819)\u001b[0m 1.2 K     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3450819)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3450819)\u001b[0m 1.2 K     Total params\n",
      "\u001b[36m(train_nn_reg pid=3450819)\u001b[0m 0.005     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3450819)\u001b[0m 4         Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3450819)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3450819)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3450819)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-02-12 10:46:05,282\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.199 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:46:05,285\tWARNING util.py:201 -- The `process_trial_result` operation took 1.201 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:46:05,285\tWARNING util.py:201 -- Processing trial results took 1.202 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:46:05,286\tWARNING util.py:201 -- The `process_trial_result` operation took 1.202 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:46:09,291\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 3.972 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:46:09,293\tWARNING util.py:201 -- The `process_trial_result` operation took 3.974 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:46:09,293\tWARNING util.py:201 -- Processing trial results took 3.975 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:46:09,294\tWARNING util.py:201 -- The `process_trial_result` operation took 3.975 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:46:09,335\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_5473c5fe\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3450819, ip=172.31.181.85, actor_id=2275cda0cac8443aa96e678401000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1024, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1053, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 1) cannot be concatenated\n",
      "\u001b[36m(train_nn_reg pid=3422740)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_a70dbacd_15_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=11,data_transform=None_2025-02-12_10-33-04/checkpoint_000093)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "2025-02-12 10:46:10,905\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.365 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:46:10,907\tWARNING util.py:201 -- The `process_trial_result` operation took 1.367 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:46:10,908\tWARNING util.py:201 -- Processing trial results took 1.368 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:46:10,908\tWARNING util.py:201 -- The `process_trial_result` operation took 1.368 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:46:11,884\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 0.974 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:46:11,886\tWARNING util.py:201 -- The `process_trial_result` operation took 0.976 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:46:11,887\tWARNING util.py:201 -- Processing trial results took 0.977 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:46:11,887\tWARNING util.py:201 -- The `process_trial_result` operation took 0.977 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3451213)\u001b[0m Seed set to 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3451213)\u001b[0m Train: (156, 42), Test: (65, 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3451213)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3451213)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3451213)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3451213)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_46be22cb_41_batch_size=64,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0011,data_transf_2025-02-12_10-46-04 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3451213)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3451213)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3451213)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3451213)\u001b[0m 0 | layers | ModuleList | 1.2 K  | train\n",
      "\u001b[36m(train_nn_reg pid=3451213)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3451213)\u001b[0m 1.2 K     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3451213)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3451213)\u001b[0m 1.2 K     Total params\n",
      "\u001b[36m(train_nn_reg pid=3451213)\u001b[0m 0.005     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3451213)\u001b[0m 4         Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3451213)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3451213)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3451213)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-02-12 10:46:14,420\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.284 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:46:14,422\tWARNING util.py:201 -- The `process_trial_result` operation took 1.286 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:46:14,422\tWARNING util.py:201 -- Processing trial results took 1.287 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:46:14,422\tWARNING util.py:201 -- The `process_trial_result` operation took 1.287 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:46:14,456\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_46be22cb\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3451213, ip=172.31.181.85, actor_id=1e6b97bd6c85f1d9c0435e0d01000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1024, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1053, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 1) cannot be concatenated\n",
      "\u001b[36m(train_nn_reg pid=3440497)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_0ed9ed1e_24_batch_size=256,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0038,data_trans_2025-02-12_10-41-28/checkpoint_000039)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "2025-02-12 10:46:18,152\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.780 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:46:18,154\tWARNING util.py:201 -- The `process_trial_result` operation took 2.783 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:46:18,154\tWARNING util.py:201 -- Processing trial results took 2.784 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:46:18,155\tWARNING util.py:201 -- The `process_trial_result` operation took 2.784 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3451388)\u001b[0m Seed set to 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3451388)\u001b[0m Train: (156, 42), Test: (65, 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3451388)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3451388)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3451388)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3451388)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_b0c744e4_42_batch_size=32,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0011,data_transf_2025-02-12_10-46-12 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3451388)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3451388)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3451388)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3451388)\u001b[0m 0 | layers | ModuleList | 1.2 K  | train\n",
      "\u001b[36m(train_nn_reg pid=3451388)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3451388)\u001b[0m 1.2 K     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3451388)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3451388)\u001b[0m 1.2 K     Total params\n",
      "\u001b[36m(train_nn_reg pid=3451388)\u001b[0m 0.005     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3451388)\u001b[0m 4         Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3451388)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3451388)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(train_nn_reg pid=3451388)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3451388)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-02-12 10:46:20,534\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.228 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:46:20,536\tWARNING util.py:201 -- The `process_trial_result` operation took 1.230 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:46:20,537\tWARNING util.py:201 -- Processing trial results took 1.231 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:46:20,537\tWARNING util.py:201 -- The `process_trial_result` operation took 1.232 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:46:20,558\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_b0c744e4\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3451388, ip=172.31.181.85, actor_id=bb51c6445d644a6239790d1a01000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1026, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 216, in run\n",
      "    self.advance()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 455, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 151, in run\n",
      "    self.on_advance_end(data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 370, in on_advance_end\n",
      "    self.val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 2) cannot be concatenated\n",
      "2025-02-12 10:46:22,348\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.607 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:46:22,350\tWARNING util.py:201 -- The `process_trial_result` operation took 1.609 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:46:22,350\tWARNING util.py:201 -- Processing trial results took 1.610 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:46:22,351\tWARNING util.py:201 -- The `process_trial_result` operation took 1.610 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3422740)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_a70dbacd_15_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=11,data_transform=None_2025-02-12_10-33-04/checkpoint_000095)\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_3c8a1025_26_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,_2025-02-12_10-43-17/checkpoint_000019)\n",
      "2025-02-12 10:46:23,752\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.090 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:46:23,754\tWARNING util.py:201 -- The `process_trial_result` operation took 1.092 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:46:23,755\tWARNING util.py:201 -- Processing trial results took 1.093 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:46:23,755\tWARNING util.py:201 -- The `process_trial_result` operation took 1.093 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:46:25,302\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.545 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:46:25,305\tWARNING util.py:201 -- The `process_trial_result` operation took 1.548 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:46:25,305\tWARNING util.py:201 -- Processing trial results took 1.548 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:46:25,306\tWARNING util.py:201 -- The `process_trial_result` operation took 1.549 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3451707)\u001b[0m Seed set to 12\n",
      "\u001b[36m(train_nn_reg pid=3422740)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_a70dbacd_15_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=11,data_transform=None_2025-02-12_10-33-04/checkpoint_000096)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3451707)\u001b[0m Train: (156, 42), Test: (65, 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3451707)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3451707)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3451707)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3451707)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_e750ae98_43_batch_size=32,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0009,data_transf_2025-02-12_10-46-19 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3451707)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3451707)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3451707)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3451707)\u001b[0m 0 | layers | ModuleList | 1.2 K  | train\n",
      "\u001b[36m(train_nn_reg pid=3451707)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3451707)\u001b[0m 1.2 K     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3451707)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3451707)\u001b[0m 1.2 K     Total params\n",
      "\u001b[36m(train_nn_reg pid=3451707)\u001b[0m 0.005     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3451707)\u001b[0m 4         Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3451707)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3451707)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(train_nn_reg pid=3451707)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3451707)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-02-12 10:46:29,624\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.181 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:46:29,626\tWARNING util.py:201 -- The `process_trial_result` operation took 1.183 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:46:29,627\tWARNING util.py:201 -- Processing trial results took 1.184 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:46:29,628\tWARNING util.py:201 -- The `process_trial_result` operation took 1.185 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:46:29,661\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_e750ae98\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3451707, ip=172.31.181.85, actor_id=9e7cac94cad65060a7c65abd01000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1026, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 216, in run\n",
      "    self.advance()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 455, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 151, in run\n",
      "    self.on_advance_end(data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 370, in on_advance_end\n",
      "    self.val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 2) cannot be concatenated\n",
      "\u001b[36m(train_nn_reg pid=3422740)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_a70dbacd_15_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=11,data_transform=None_2025-02-12_10-33-04/checkpoint_000097)\n",
      "2025-02-12 10:46:30,983\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.075 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:46:30,985\tWARNING util.py:201 -- The `process_trial_result` operation took 1.078 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:46:30,986\tWARNING util.py:201 -- Processing trial results took 1.078 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:46:30,986\tWARNING util.py:201 -- The `process_trial_result` operation took 1.079 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:46:32,518\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.530 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:46:32,520\tWARNING util.py:201 -- The `process_trial_result` operation took 1.532 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:46:32,521\tWARNING util.py:201 -- Processing trial results took 1.533 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:46:32,521\tWARNING util.py:201 -- The `process_trial_result` operation took 1.533 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:46:36,025\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.704 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:46:36,027\tWARNING util.py:201 -- The `process_trial_result` operation took 2.706 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:46:36,028\tWARNING util.py:201 -- Processing trial results took 2.707 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:46:36,029\tWARNING util.py:201 -- The `process_trial_result` operation took 2.708 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:46:37,574\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.528 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:46:37,577\tWARNING util.py:201 -- The `process_trial_result` operation took 1.530 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:46:37,577\tWARNING util.py:201 -- Processing trial results took 1.530 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:46:37,577\tWARNING util.py:201 -- The `process_trial_result` operation took 1.531 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3422740)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_a70dbacd_15_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=11,data_transform=None_2025-02-12_10-33-04/checkpoint_000099)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_nn_reg pid=3452120)\u001b[0m Seed set to 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3452120)\u001b[0m Train: (156, 42), Test: (65, 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3452120)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3452120)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3452120)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3452120)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_56629185_44_batch_size=64,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0007,data_transf_2025-02-12_10-46-28 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3452120)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3452120)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3452120)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3452120)\u001b[0m 0 | layers | ModuleList | 1.2 K  | train\n",
      "\u001b[36m(train_nn_reg pid=3452120)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3452120)\u001b[0m 1.2 K     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3452120)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3452120)\u001b[0m 1.2 K     Total params\n",
      "\u001b[36m(train_nn_reg pid=3452120)\u001b[0m 0.005     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3452120)\u001b[0m 4         Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3452120)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3452120)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3452120)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-02-12 10:46:40,308\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.560 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:46:40,311\tWARNING util.py:201 -- The `process_trial_result` operation took 1.562 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:46:40,311\tWARNING util.py:201 -- Processing trial results took 1.563 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:46:40,312\tWARNING util.py:201 -- The `process_trial_result` operation took 1.563 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:46:40,344\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_56629185\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3452120, ip=172.31.181.85, actor_id=6d517d59ccb3897f60756fd201000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1024, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1053, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 1) cannot be concatenated\n",
      "2025-02-12 10:46:41,701\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.156 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:46:41,703\tWARNING util.py:201 -- The `process_trial_result` operation took 1.158 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:46:41,705\tWARNING util.py:201 -- Processing trial results took 1.160 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:46:41,706\tWARNING util.py:201 -- The `process_trial_result` operation took 1.161 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:46:43,284\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.560 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:46:43,286\tWARNING util.py:201 -- The `process_trial_result` operation took 1.562 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:46:43,286\tWARNING util.py:201 -- Processing trial results took 1.562 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:46:43,286\tWARNING util.py:201 -- The `process_trial_result` operation took 1.563 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_3c8a1025_26_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,_2025-02-12_10-43-17/checkpoint_000024)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_nn_reg pid=3452556)\u001b[0m Seed set to 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3452556)\u001b[0m Train: (156, 42), Test: (65, 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3452556)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3452556)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3452556)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3452556)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_14270525_45_batch_size=32,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0007,data_transf_2025-02-12_10-46-38 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3452556)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3452556)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3452556)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3452556)\u001b[0m 0 | layers | ModuleList | 1.2 K  | train\n",
      "\u001b[36m(train_nn_reg pid=3452556)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3452556)\u001b[0m 1.2 K     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3452556)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3452556)\u001b[0m 1.2 K     Total params\n",
      "\u001b[36m(train_nn_reg pid=3452556)\u001b[0m 0.005     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3452556)\u001b[0m 4         Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3452556)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3452556)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(train_nn_reg pid=3452556)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3452556)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-02-12 10:46:48,480\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.602 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:46:48,482\tWARNING util.py:201 -- The `process_trial_result` operation took 1.605 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:46:48,483\tWARNING util.py:201 -- Processing trial results took 1.605 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:46:48,484\tWARNING util.py:201 -- The `process_trial_result` operation took 1.606 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:46:48,490\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_14270525\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3452556, ip=172.31.181.85, actor_id=58ad4b19b2aac426d2fde3e901000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1026, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 216, in run\n",
      "    self.advance()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 455, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 151, in run\n",
      "    self.on_advance_end(data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 370, in on_advance_end\n",
      "    self.val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 2) cannot be concatenated\n",
      "2025-02-12 10:46:50,339\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.524 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:46:50,342\tWARNING util.py:201 -- The `process_trial_result` operation took 1.527 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:46:50,343\tWARNING util.py:201 -- Processing trial results took 1.528 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:46:50,343\tWARNING util.py:201 -- The `process_trial_result` operation took 1.528 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_3c8a1025_26_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,_2025-02-12_10-43-17/checkpoint_000026)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "2025-02-12 10:46:54,272\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 3.370 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:46:54,274\tWARNING util.py:201 -- The `process_trial_result` operation took 3.373 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:46:54,275\tWARNING util.py:201 -- Processing trial results took 3.373 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:46:54,276\tWARNING util.py:201 -- The `process_trial_result` operation took 3.374 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3452820)\u001b[0m Seed set to 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3452820)\u001b[0m Train: (156, 42), Test: (65, 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3452820)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3452820)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3452820)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3452820)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_9290736f_46_batch_size=32,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0007,data_transf_2025-02-12_10-46-46 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3452820)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3452820)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3452820)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3452820)\u001b[0m 0 | layers | ModuleList | 881 K  | train\n",
      "\u001b[36m(train_nn_reg pid=3452820)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3452820)\u001b[0m 881 K     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3452820)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3452820)\u001b[0m 881 K     Total params\n",
      "\u001b[36m(train_nn_reg pid=3452820)\u001b[0m 3.525     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3452820)\u001b[0m 24        Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3452820)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_3c8a1025_26_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,_2025-02-12_10-43-17/checkpoint_000027)\n",
      "\u001b[36m(train_nn_reg pid=3452820)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(train_nn_reg pid=3452820)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3452820)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-02-12 10:46:57,381\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.666 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:46:57,384\tWARNING util.py:201 -- The `process_trial_result` operation took 1.669 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:46:57,384\tWARNING util.py:201 -- Processing trial results took 1.669 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:46:57,384\tWARNING util.py:201 -- The `process_trial_result` operation took 1.669 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:46:57,391\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_9290736f\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3452820, ip=172.31.181.85, actor_id=840dcc351fadab70a9adf30b01000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1026, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 216, in run\n",
      "    self.advance()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 455, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 151, in run\n",
      "    self.on_advance_end(data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 370, in on_advance_end\n",
      "    self.val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 2) cannot be concatenated\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_3c8a1025_26_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,_2025-02-12_10-43-17/checkpoint_000028)\n",
      "2025-02-12 10:46:59,323\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.603 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:46:59,325\tWARNING util.py:201 -- The `process_trial_result` operation took 1.605 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:46:59,326\tWARNING util.py:201 -- Processing trial results took 1.606 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:46:59,326\tWARNING util.py:201 -- The `process_trial_result` operation took 1.607 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_3c8a1025_26_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,_2025-02-12_10-43-17/checkpoint_000029)\n",
      "2025-02-12 10:47:04,000\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 4.119 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:47:04,002\tWARNING util.py:201 -- The `process_trial_result` operation took 4.122 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:47:04,003\tWARNING util.py:201 -- Processing trial results took 4.122 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:47:04,003\tWARNING util.py:201 -- The `process_trial_result` operation took 4.123 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_3c8a1025_26_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,_2025-02-12_10-43-17/checkpoint_000030)\n",
      "\u001b[36m(train_nn_reg pid=3453142)\u001b[0m Seed set to 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3453142)\u001b[0m Train: (156, 42), Test: (65, 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3453142)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3453142)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3453142)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3453142)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_667fe2f0_47_batch_size=64,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0007,data_transf_2025-02-12_10-46-55 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3453142)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3453142)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3453142)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3453142)\u001b[0m 0 | layers | ModuleList | 881 K  | train\n",
      "\u001b[36m(train_nn_reg pid=3453142)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3453142)\u001b[0m 881 K     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3453142)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3453142)\u001b[0m 881 K     Total params\n",
      "\u001b[36m(train_nn_reg pid=3453142)\u001b[0m 3.525     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3453142)\u001b[0m 24        Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3453142)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3453142)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3453142)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-02-12 10:47:07,115\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.598 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:47:07,117\tWARNING util.py:201 -- The `process_trial_result` operation took 1.600 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:47:07,117\tWARNING util.py:201 -- Processing trial results took 1.601 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:47:07,118\tWARNING util.py:201 -- The `process_trial_result` operation took 1.601 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:47:07,124\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_667fe2f0\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3453142, ip=172.31.181.85, actor_id=f006c89329aae88e20e9f7ec01000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1024, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1053, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 1) cannot be concatenated\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_3c8a1025_26_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,_2025-02-12_10-43-17/checkpoint_000031)\n",
      "2025-02-12 10:48:19,211\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 71.770 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:48:19,213\tWARNING util.py:201 -- The `process_trial_result` operation took 71.773 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:48:19,214\tWARNING util.py:201 -- Processing trial results took 71.774 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:48:19,215\tWARNING util.py:201 -- The `process_trial_result` operation took 71.774 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_3c8a1025_26_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,_2025-02-12_10-43-17/checkpoint_000032)\n",
      "\u001b[36m(train_nn_reg pid=3453498)\u001b[0m Seed set to 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3453498)\u001b[0m Train: (156, 42), Test: (65, 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3453498)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3453498)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3453498)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3453498)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_3cacc334_48_batch_size=32,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0007,data_transf_2025-02-12_10-47-05 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3453498)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3453498)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3453498)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3453498)\u001b[0m 0 | layers | ModuleList | 881 K  | train\n",
      "\u001b[36m(train_nn_reg pid=3453498)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3453498)\u001b[0m 881 K     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3453498)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3453498)\u001b[0m 881 K     Total params\n",
      "\u001b[36m(train_nn_reg pid=3453498)\u001b[0m 3.525     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3453498)\u001b[0m 24        Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3453498)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3453498)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(train_nn_reg pid=3453498)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3453498)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-02-12 10:48:24,487\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.717 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:48:24,489\tWARNING util.py:201 -- The `process_trial_result` operation took 1.719 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:48:24,489\tWARNING util.py:201 -- Processing trial results took 1.720 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:48:24,490\tWARNING util.py:201 -- The `process_trial_result` operation took 1.721 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:48:24,525\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_3cacc334\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3453498, ip=172.31.181.85, actor_id=5523c63f9c88cbfa13f3deee01000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1026, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 216, in run\n",
      "    self.advance()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 455, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 151, in run\n",
      "    self.on_advance_end(data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 370, in on_advance_end\n",
      "    self.val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 2) cannot be concatenated\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_3c8a1025_26_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,_2025-02-12_10-43-17/checkpoint_000033)\n",
      "2025-02-12 10:48:26,385\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.531 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:48:26,387\tWARNING util.py:201 -- The `process_trial_result` operation took 1.533 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:48:26,388\tWARNING util.py:201 -- Processing trial results took 1.534 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:48:26,389\tWARNING util.py:201 -- The `process_trial_result` operation took 1.535 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_3c8a1025_26_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,_2025-02-12_10-43-17/checkpoint_000034)\n",
      "2025-02-12 10:48:30,288\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 3.358 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:48:30,290\tWARNING util.py:201 -- The `process_trial_result` operation took 3.360 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:48:30,291\tWARNING util.py:201 -- Processing trial results took 3.361 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:48:30,291\tWARNING util.py:201 -- The `process_trial_result` operation took 3.361 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_3c8a1025_26_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,_2025-02-12_10-43-17/checkpoint_000035)\n",
      "\u001b[36m(train_nn_reg pid=3453925)\u001b[0m Seed set to 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3453925)\u001b[0m Train: (156, 42), Test: (65, 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3453925)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3453925)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3453925)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3453925)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_66004043_49_batch_size=64,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0008,data_transf_2025-02-12_10-48-22 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3453925)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3453925)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3453925)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3453925)\u001b[0m 0 | layers | ModuleList | 881 K  | train\n",
      "\u001b[36m(train_nn_reg pid=3453925)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3453925)\u001b[0m 881 K     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3453925)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3453925)\u001b[0m 881 K     Total params\n",
      "\u001b[36m(train_nn_reg pid=3453925)\u001b[0m 3.525     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3453925)\u001b[0m 24        Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3453925)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3453925)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3453925)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-02-12 10:48:33,255\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.580 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:48:33,257\tWARNING util.py:201 -- The `process_trial_result` operation took 1.582 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:48:33,257\tWARNING util.py:201 -- Processing trial results took 1.583 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:48:33,258\tWARNING util.py:201 -- The `process_trial_result` operation took 1.584 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:48:33,263\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_66004043\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3453925, ip=172.31.181.85, actor_id=5ca7e8eb6ea78601153f355101000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1024, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1053, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 1) cannot be concatenated\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_3c8a1025_26_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,_2025-02-12_10-43-17/checkpoint_000036)\n",
      "2025-02-12 10:48:35,268\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.525 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:48:35,270\tWARNING util.py:201 -- The `process_trial_result` operation took 1.527 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:48:35,270\tWARNING util.py:201 -- Processing trial results took 1.527 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:48:35,271\tWARNING util.py:201 -- The `process_trial_result` operation took 1.528 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_3c8a1025_26_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,_2025-02-12_10-43-17/checkpoint_000037)\n",
      "2025-02-12 10:48:39,385\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 3.490 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:48:39,387\tWARNING util.py:201 -- The `process_trial_result` operation took 3.493 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:48:39,388\tWARNING util.py:201 -- Processing trial results took 3.493 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:48:39,388\tWARNING util.py:201 -- The `process_trial_result` operation took 3.494 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3454188)\u001b[0m Seed set to 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3454188)\u001b[0m Train: (156, 42), Test: (65, 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3454188)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3454188)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3454188)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3454188)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_df106c16_50_batch_size=64,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0007,data_transf_2025-02-12_10-48-31 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3454188)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3454188)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3454188)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3454188)\u001b[0m 0 | layers | ModuleList | 881 K  | train\n",
      "\u001b[36m(train_nn_reg pid=3454188)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3454188)\u001b[0m 881 K     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3454188)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3454188)\u001b[0m 881 K     Total params\n",
      "\u001b[36m(train_nn_reg pid=3454188)\u001b[0m 3.525     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3454188)\u001b[0m 24        Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3454188)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3454188)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3454188)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_3c8a1025_26_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,_2025-02-12_10-43-17/checkpoint_000038)\n",
      "2025-02-12 10:48:42,575\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.542 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:48:42,577\tWARNING util.py:201 -- The `process_trial_result` operation took 1.545 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:48:42,578\tWARNING util.py:201 -- Processing trial results took 1.545 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:48:42,578\tWARNING util.py:201 -- The `process_trial_result` operation took 1.546 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:48:42,583\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_df106c16\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3454188, ip=172.31.181.85, actor_id=f2fefc95cbaf58977c4dbbe601000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1024, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1053, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 1) cannot be concatenated\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_3c8a1025_26_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,_2025-02-12_10-43-17/checkpoint_000039)\n",
      "2025-02-12 10:48:44,620\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.558 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:48:44,622\tWARNING util.py:201 -- The `process_trial_result` operation took 1.560 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:48:44,623\tWARNING util.py:201 -- Processing trial results took 1.561 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:48:44,623\tWARNING util.py:201 -- The `process_trial_result` operation took 1.561 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:48:44,646\tERROR experiment_state.py:173 -- Saving experiment state to storage at '/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg' failed with exception: \n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/execution/experiment_state.py\", line 171, in wait_for_sync\n",
      "    self._storage.syncer.wait()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 474, in wait\n",
      "    raise e\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 472, in wait\n",
      "    self._sync_process.wait(timeout=timeout or self.sync_timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 173, in wait\n",
      "    raise exception\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/syncer.py\", line 136, in entrypoint\n",
      "    result = self._fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/storage.py\", line 218, in _upload_to_fs_path\n",
      "    _pyarrow_fs_copy_files(local_path, fs_path, destination_filesystem=fs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/train/_internal/storage.py\", line 116, in _pyarrow_fs_copy_files\n",
      "    return pyarrow.fs.copy_files(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/pyarrow/fs.py\", line 259, in copy_files\n",
      "    _copy_files_selector(source_fs, source_sel,\n",
      "  File \"pyarrow/_fs.pyx\", line 1624, in pyarrow._fs._copy_files_selector\n",
      "  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\n",
      "FileNotFoundError: [Errno 2] Failed to open local file '/scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_3c8a1025_26_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,_2025-02-12_10-43-17/epoch=29-val_rmse=0.97.ckpt'. Detail: [errno 2] No such file or directory\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_3c8a1025_26_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,_2025-02-12_10-43-17/checkpoint_000040)\n",
      "2025-02-12 10:48:48,554\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 3.367 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:48:48,556\tWARNING util.py:201 -- The `process_trial_result` operation took 3.370 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:48:48,556\tWARNING util.py:201 -- Processing trial results took 3.370 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:48:48,557\tWARNING util.py:201 -- The `process_trial_result` operation took 3.371 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_3c8a1025_26_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,_2025-02-12_10-43-17/checkpoint_000041)\n",
      "\u001b[36m(train_nn_reg pid=3454360)\u001b[0m Seed set to 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3454360)\u001b[0m Train: (156, 42), Test: (65, 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3454360)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3454360)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3454360)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3454360)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_7ffde30b_51_batch_size=64,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0007,data_transf_2025-02-12_10-48-40 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3454360)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3454360)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3454360)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3454360)\u001b[0m 0 | layers | ModuleList | 881 K  | train\n",
      "\u001b[36m(train_nn_reg pid=3454360)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3454360)\u001b[0m 881 K     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3454360)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3454360)\u001b[0m 881 K     Total params\n",
      "\u001b[36m(train_nn_reg pid=3454360)\u001b[0m 3.525     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3454360)\u001b[0m 24        Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3454360)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3454360)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3454360)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-02-12 10:48:51,524\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.572 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:48:51,527\tWARNING util.py:201 -- The `process_trial_result` operation took 1.574 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:48:51,527\tWARNING util.py:201 -- Processing trial results took 1.575 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:48:51,527\tWARNING util.py:201 -- The `process_trial_result` operation took 1.575 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:48:51,533\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_7ffde30b\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3454360, ip=172.31.181.85, actor_id=9ebf78faf54d93762a7dd41601000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1024, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1053, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 1) cannot be concatenated\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_3c8a1025_26_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,_2025-02-12_10-43-17/checkpoint_000042)\n",
      "2025-02-12 10:48:53,572\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.550 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:48:53,574\tWARNING util.py:201 -- The `process_trial_result` operation took 1.552 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:48:53,575\tWARNING util.py:201 -- Processing trial results took 1.553 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:48:53,575\tWARNING util.py:201 -- The `process_trial_result` operation took 1.553 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_3c8a1025_26_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,_2025-02-12_10-43-17/checkpoint_000043)\n",
      "2025-02-12 10:48:57,607\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 3.458 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:48:57,610\tWARNING util.py:201 -- The `process_trial_result` operation took 3.461 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:48:57,610\tWARNING util.py:201 -- Processing trial results took 3.461 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:48:57,611\tWARNING util.py:201 -- The `process_trial_result` operation took 3.462 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3454711)\u001b[0m Seed set to 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3454711)\u001b[0m Train: (156, 42), Test: (65, 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3454711)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3454711)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3454711)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3454711)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_d9a45f0c_52_batch_size=32,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0007,data_transf_2025-02-12_10-48-49 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3454711)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3454711)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3454711)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3454711)\u001b[0m 0 | layers | ModuleList | 881 K  | train\n",
      "\u001b[36m(train_nn_reg pid=3454711)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3454711)\u001b[0m 881 K     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3454711)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3454711)\u001b[0m 881 K     Total params\n",
      "\u001b[36m(train_nn_reg pid=3454711)\u001b[0m 3.525     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3454711)\u001b[0m 24        Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3454711)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_3c8a1025_26_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,_2025-02-12_10-43-17/checkpoint_000044)\n",
      "\u001b[36m(train_nn_reg pid=3454711)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(train_nn_reg pid=3454711)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3454711)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-02-12 10:49:01,209\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.985 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:49:01,211\tWARNING util.py:201 -- The `process_trial_result` operation took 1.987 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:49:01,212\tWARNING util.py:201 -- Processing trial results took 1.988 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:49:01,212\tWARNING util.py:201 -- The `process_trial_result` operation took 1.988 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:49:01,216\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_d9a45f0c\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3454711, ip=172.31.181.85, actor_id=03939ab3654521ffa166450501000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1026, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 216, in run\n",
      "    self.advance()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 455, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 151, in run\n",
      "    self.on_advance_end(data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 370, in on_advance_end\n",
      "    self.val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 2) cannot be concatenated\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_3c8a1025_26_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,_2025-02-12_10-43-17/checkpoint_000045)\n",
      "2025-02-12 10:49:03,259\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.568 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:49:03,261\tWARNING util.py:201 -- The `process_trial_result` operation took 1.570 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:49:03,262\tWARNING util.py:201 -- Processing trial results took 1.571 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:49:03,262\tWARNING util.py:201 -- The `process_trial_result` operation took 1.571 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_3c8a1025_26_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,_2025-02-12_10-43-17/checkpoint_000046)\n",
      "2025-02-12 10:49:07,213\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 3.387 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:49:07,215\tWARNING util.py:201 -- The `process_trial_result` operation took 3.389 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:49:07,216\tWARNING util.py:201 -- Processing trial results took 3.389 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:49:07,216\tWARNING util.py:201 -- The `process_trial_result` operation took 3.390 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3454883)\u001b[0m Seed set to 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3454883)\u001b[0m Train: (156, 42), Test: (65, 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3454883)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3454883)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3454883)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3454883)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_aa8b02c2_53_batch_size=32,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0008,data_transf_2025-02-12_10-48-58 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3454883)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3454883)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3454883)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3454883)\u001b[0m 0 | layers | ModuleList | 881 K  | train\n",
      "\u001b[36m(train_nn_reg pid=3454883)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3454883)\u001b[0m 881 K     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3454883)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3454883)\u001b[0m 881 K     Total params\n",
      "\u001b[36m(train_nn_reg pid=3454883)\u001b[0m 3.525     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3454883)\u001b[0m 24        Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3454883)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_3c8a1025_26_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,_2025-02-12_10-43-17/checkpoint_000047)\n",
      "\u001b[36m(train_nn_reg pid=3454883)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(train_nn_reg pid=3454883)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3454883)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-02-12 10:49:10,550\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.612 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:49:10,552\tWARNING util.py:201 -- The `process_trial_result` operation took 1.614 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:49:10,552\tWARNING util.py:201 -- Processing trial results took 1.615 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:49:10,553\tWARNING util.py:201 -- The `process_trial_result` operation took 1.616 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:49:10,556\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_aa8b02c2\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3454883, ip=172.31.181.85, actor_id=3789363ec611d78b3fb8691701000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1026, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 216, in run\n",
      "    self.advance()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 455, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 151, in run\n",
      "    self.on_advance_end(data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 370, in on_advance_end\n",
      "    self.val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 2) cannot be concatenated\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_3c8a1025_26_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,_2025-02-12_10-43-17/checkpoint_000048)\n",
      "2025-02-12 10:49:12,624\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.594 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:49:12,626\tWARNING util.py:201 -- The `process_trial_result` operation took 1.596 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:49:12,626\tWARNING util.py:201 -- Processing trial results took 1.597 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:49:12,627\tWARNING util.py:201 -- The `process_trial_result` operation took 1.597 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_3c8a1025_26_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,_2025-02-12_10-43-17/checkpoint_000049)\n",
      "2025-02-12 10:49:16,830\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 3.634 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:49:16,833\tWARNING util.py:201 -- The `process_trial_result` operation took 3.636 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:49:16,834\tWARNING util.py:201 -- Processing trial results took 3.637 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:49:16,834\tWARNING util.py:201 -- The `process_trial_result` operation took 3.638 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3455266)\u001b[0m Seed set to 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3455266)\u001b[0m Train: (156, 42), Test: (65, 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3455266)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3455266)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3455266)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3455266)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_af910a96_54_batch_size=32,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0007,data_transf_2025-02-12_10-49-08 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3455266)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3455266)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3455266)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3455266)\u001b[0m 0 | layers | ModuleList | 881 K  | train\n",
      "\u001b[36m(train_nn_reg pid=3455266)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3455266)\u001b[0m 881 K     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3455266)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3455266)\u001b[0m 881 K     Total params\n",
      "\u001b[36m(train_nn_reg pid=3455266)\u001b[0m 3.525     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3455266)\u001b[0m 24        Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3455266)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_3c8a1025_26_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,_2025-02-12_10-43-17/checkpoint_000050)\n",
      "\u001b[36m(train_nn_reg pid=3455266)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(train_nn_reg pid=3455266)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3455266)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-02-12 10:49:20,083\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.597 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:49:20,085\tWARNING util.py:201 -- The `process_trial_result` operation took 1.599 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:49:20,086\tWARNING util.py:201 -- Processing trial results took 1.600 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:49:20,086\tWARNING util.py:201 -- The `process_trial_result` operation took 1.601 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:49:20,091\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_af910a96\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3455266, ip=172.31.181.85, actor_id=2e77a7cf9874f4568c76041201000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1026, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 216, in run\n",
      "    self.advance()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 455, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 151, in run\n",
      "    self.on_advance_end(data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 370, in on_advance_end\n",
      "    self.val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 2) cannot be concatenated\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_3c8a1025_26_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,_2025-02-12_10-43-17/checkpoint_000051)\n",
      "2025-02-12 10:49:22,118\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.571 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:49:22,120\tWARNING util.py:201 -- The `process_trial_result` operation took 1.573 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:49:22,121\tWARNING util.py:201 -- Processing trial results took 1.574 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:49:22,121\tWARNING util.py:201 -- The `process_trial_result` operation took 1.575 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_3c8a1025_26_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,_2025-02-12_10-43-17/checkpoint_000052)\n",
      "2025-02-12 10:49:26,096\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 3.424 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:49:26,099\tWARNING util.py:201 -- The `process_trial_result` operation took 3.426 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:49:26,099\tWARNING util.py:201 -- Processing trial results took 3.427 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:49:26,100\tWARNING util.py:201 -- The `process_trial_result` operation took 3.428 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_3c8a1025_26_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,_2025-02-12_10-43-17/checkpoint_000053)\n",
      "\u001b[36m(train_nn_reg pid=3455594)\u001b[0m Seed set to 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3455594)\u001b[0m Train: (156, 42), Test: (65, 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3455594)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3455594)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3455594)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3455594)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_0d054fd5_55_batch_size=32,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0007,data_transf_2025-02-12_10-49-18 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3455594)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3455594)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3455594)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3455594)\u001b[0m 0 | layers | ModuleList | 881 K  | train\n",
      "\u001b[36m(train_nn_reg pid=3455594)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3455594)\u001b[0m 881 K     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3455594)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3455594)\u001b[0m 881 K     Total params\n",
      "\u001b[36m(train_nn_reg pid=3455594)\u001b[0m 3.525     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3455594)\u001b[0m 24        Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3455594)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3455594)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(train_nn_reg pid=3455594)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3455594)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-02-12 10:49:29,126\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.605 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:49:29,128\tWARNING util.py:201 -- The `process_trial_result` operation took 1.608 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:49:29,129\tWARNING util.py:201 -- Processing trial results took 1.608 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:49:29,130\tWARNING util.py:201 -- The `process_trial_result` operation took 1.609 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:49:29,145\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_0d054fd5\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3455594, ip=172.31.181.85, actor_id=fee303db1fdd1157f45bb3bc01000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1026, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 216, in run\n",
      "    self.advance()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 455, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 151, in run\n",
      "    self.on_advance_end(data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 370, in on_advance_end\n",
      "    self.val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 2) cannot be concatenated\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_3c8a1025_26_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,_2025-02-12_10-43-17/checkpoint_000054)\n",
      "2025-02-12 10:49:31,051\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.580 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:49:31,054\tWARNING util.py:201 -- The `process_trial_result` operation took 1.582 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:49:31,054\tWARNING util.py:201 -- Processing trial results took 1.583 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:49:31,055\tWARNING util.py:201 -- The `process_trial_result` operation took 1.584 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_3c8a1025_26_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,_2025-02-12_10-43-17/checkpoint_000055)\n",
      "2025-02-12 10:49:35,155\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 3.522 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:49:35,158\tWARNING util.py:201 -- The `process_trial_result` operation took 3.526 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:49:35,158\tWARNING util.py:201 -- Processing trial results took 3.526 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:49:35,159\tWARNING util.py:201 -- The `process_trial_result` operation took 3.527 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_3c8a1025_26_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,_2025-02-12_10-43-17/checkpoint_000056)\n",
      "\u001b[36m(train_nn_reg pid=3456068)\u001b[0m Seed set to 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3456068)\u001b[0m Train: (156, 42), Test: (65, 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3456068)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3456068)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3456068)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3456068)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_cd9994a1_56_batch_size=32,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0008,data_transf_2025-02-12_10-49-27 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3456068)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3456068)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3456068)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3456068)\u001b[0m 0 | layers | ModuleList | 881 K  | train\n",
      "\u001b[36m(train_nn_reg pid=3456068)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3456068)\u001b[0m 881 K     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3456068)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3456068)\u001b[0m 881 K     Total params\n",
      "\u001b[36m(train_nn_reg pid=3456068)\u001b[0m 3.525     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3456068)\u001b[0m 24        Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3456068)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3456068)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(train_nn_reg pid=3456068)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3456068)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-02-12 10:49:38,249\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.635 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:49:38,252\tWARNING util.py:201 -- The `process_trial_result` operation took 1.638 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:49:38,253\tWARNING util.py:201 -- Processing trial results took 1.639 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:49:38,253\tWARNING util.py:201 -- The `process_trial_result` operation took 1.640 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:49:38,269\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_cd9994a1\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3456068, ip=172.31.181.85, actor_id=522f5a132df4d1c1ead3854001000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1026, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 216, in run\n",
      "    self.advance()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 455, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 151, in run\n",
      "    self.on_advance_end(data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 370, in on_advance_end\n",
      "    self.val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 2) cannot be concatenated\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_3c8a1025_26_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,_2025-02-12_10-43-17/checkpoint_000057)\n",
      "2025-02-12 10:49:40,203\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.617 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:49:40,205\tWARNING util.py:201 -- The `process_trial_result` operation took 1.620 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:49:40,206\tWARNING util.py:201 -- Processing trial results took 1.620 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:49:40,206\tWARNING util.py:201 -- The `process_trial_result` operation took 1.621 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_3c8a1025_26_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,_2025-02-12_10-43-17/checkpoint_000058)\n",
      "2025-02-12 10:49:44,317\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 3.532 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:49:44,320\tWARNING util.py:201 -- The `process_trial_result` operation took 3.534 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:49:44,320\tWARNING util.py:201 -- Processing trial results took 3.535 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:49:44,321\tWARNING util.py:201 -- The `process_trial_result` operation took 3.536 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3456390)\u001b[0m Seed set to 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3456390)\u001b[0m Train: (156, 42), Test: (65, 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3456390)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3456390)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3456390)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3456390)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_9314c34e_57_batch_size=32,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0008,data_transf_2025-02-12_10-49-36 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3456390)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3456390)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3456390)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3456390)\u001b[0m 0 | layers | ModuleList | 881 K  | train\n",
      "\u001b[36m(train_nn_reg pid=3456390)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3456390)\u001b[0m 881 K     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3456390)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3456390)\u001b[0m 881 K     Total params\n",
      "\u001b[36m(train_nn_reg pid=3456390)\u001b[0m 3.525     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3456390)\u001b[0m 24        Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3456390)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_3c8a1025_26_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,_2025-02-12_10-43-17/checkpoint_000059)\n",
      "\u001b[36m(train_nn_reg pid=3456390)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(train_nn_reg pid=3456390)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3456390)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-02-12 10:49:47,723\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.742 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:49:47,725\tWARNING util.py:201 -- The `process_trial_result` operation took 1.744 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:49:47,726\tWARNING util.py:201 -- Processing trial results took 1.745 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:49:47,727\tWARNING util.py:201 -- The `process_trial_result` operation took 1.746 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:49:47,740\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_9314c34e\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3456390, ip=172.31.181.85, actor_id=3bbb771f15e733e86cbf0afa01000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1026, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 216, in run\n",
      "    self.advance()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 455, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 151, in run\n",
      "    self.on_advance_end(data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 370, in on_advance_end\n",
      "    self.val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 2) cannot be concatenated\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_3c8a1025_26_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,_2025-02-12_10-43-17/checkpoint_000060)\n",
      "2025-02-12 10:49:49,624\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.568 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:49:49,626\tWARNING util.py:201 -- The `process_trial_result` operation took 1.570 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:49:49,627\tWARNING util.py:201 -- Processing trial results took 1.571 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:49:49,628\tWARNING util.py:201 -- The `process_trial_result` operation took 1.571 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_3c8a1025_26_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,_2025-02-12_10-43-17/checkpoint_000061)\n",
      "2025-02-12 10:49:53,772\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 3.567 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:49:53,774\tWARNING util.py:201 -- The `process_trial_result` operation took 3.568 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:49:53,775\tWARNING util.py:201 -- Processing trial results took 3.570 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:49:53,776\tWARNING util.py:201 -- The `process_trial_result` operation took 3.570 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3456561)\u001b[0m Seed set to 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3456561)\u001b[0m Train: (156, 42), Test: (65, 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3456561)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3456561)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3456561)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3456561)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_c0b612be_58_batch_size=32,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0007,data_transf_2025-02-12_10-49-45 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3456561)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3456561)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3456561)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3456561)\u001b[0m 0 | layers | ModuleList | 881 K  | train\n",
      "\u001b[36m(train_nn_reg pid=3456561)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3456561)\u001b[0m 881 K     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3456561)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3456561)\u001b[0m 881 K     Total params\n",
      "\u001b[36m(train_nn_reg pid=3456561)\u001b[0m 3.525     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3456561)\u001b[0m 24        Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3456561)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_3c8a1025_26_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,_2025-02-12_10-43-17/checkpoint_000062)\n",
      "\u001b[36m(train_nn_reg pid=3456561)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(train_nn_reg pid=3456561)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3456561)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-02-12 10:49:56,997\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.603 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:49:56,999\tWARNING util.py:201 -- The `process_trial_result` operation took 1.606 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:49:56,999\tWARNING util.py:201 -- Processing trial results took 1.606 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:49:57,000\tWARNING util.py:201 -- The `process_trial_result` operation took 1.606 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:49:57,016\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_c0b612be\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3456561, ip=172.31.181.85, actor_id=66be942844ec49f65af142aa01000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1026, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 216, in run\n",
      "    self.advance()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 455, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 151, in run\n",
      "    self.on_advance_end(data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 370, in on_advance_end\n",
      "    self.val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 2) cannot be concatenated\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_3c8a1025_26_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,_2025-02-12_10-43-17/checkpoint_000063)\n",
      "2025-02-12 10:49:58,902\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.575 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:49:58,904\tWARNING util.py:201 -- The `process_trial_result` operation took 1.577 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:49:58,905\tWARNING util.py:201 -- Processing trial results took 1.578 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:49:58,905\tWARNING util.py:201 -- The `process_trial_result` operation took 1.578 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_3c8a1025_26_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,_2025-02-12_10-43-17/checkpoint_000064)\n",
      "2025-02-12 10:50:04,293\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 4.796 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:50:04,295\tWARNING util.py:201 -- The `process_trial_result` operation took 4.798 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:50:04,296\tWARNING util.py:201 -- Processing trial results took 4.799 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:50:04,296\tWARNING util.py:201 -- The `process_trial_result` operation took 4.800 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3456883)\u001b[0m Seed set to 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3456883)\u001b[0m Train: (156, 42), Test: (65, 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3456883)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3456883)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3456883)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3456883)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_221e4edc_59_batch_size=32,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0006,data_transf_2025-02-12_10-49-55 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3456883)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3456883)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3456883)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3456883)\u001b[0m 0 | layers | ModuleList | 881 K  | train\n",
      "\u001b[36m(train_nn_reg pid=3456883)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3456883)\u001b[0m 881 K     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3456883)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3456883)\u001b[0m 881 K     Total params\n",
      "\u001b[36m(train_nn_reg pid=3456883)\u001b[0m 3.525     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3456883)\u001b[0m 24        Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3456883)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_3c8a1025_26_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,_2025-02-12_10-43-17/checkpoint_000065)\n",
      "\u001b[36m(train_nn_reg pid=3456883)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(train_nn_reg pid=3456883)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3456883)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-02-12 10:50:07,945\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.650 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:50:07,947\tWARNING util.py:201 -- The `process_trial_result` operation took 1.652 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:50:07,948\tWARNING util.py:201 -- Processing trial results took 1.652 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:50:07,948\tWARNING util.py:201 -- The `process_trial_result` operation took 1.653 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:50:07,967\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_221e4edc\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3456883, ip=172.31.181.85, actor_id=87910479d0e870a7f1c135bf01000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1026, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 216, in run\n",
      "    self.advance()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 455, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 151, in run\n",
      "    self.on_advance_end(data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 370, in on_advance_end\n",
      "    self.val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 2) cannot be concatenated\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_3c8a1025_26_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,_2025-02-12_10-43-17/checkpoint_000066)\n",
      "2025-02-12 10:50:09,853\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.570 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:50:09,855\tWARNING util.py:201 -- The `process_trial_result` operation took 1.572 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:50:09,855\tWARNING util.py:201 -- Processing trial results took 1.572 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:50:09,856\tWARNING util.py:201 -- The `process_trial_result` operation took 1.573 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_3c8a1025_26_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,_2025-02-12_10-43-17/checkpoint_000067)\n",
      "2025-02-12 10:50:14,193\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 3.754 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:50:14,195\tWARNING util.py:201 -- The `process_trial_result` operation took 3.757 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:50:14,197\tWARNING util.py:201 -- Processing trial results took 3.758 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:50:14,197\tWARNING util.py:201 -- The `process_trial_result` operation took 3.759 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_3c8a1025_26_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,_2025-02-12_10-43-17/checkpoint_000068)\n",
      "\u001b[36m(train_nn_reg pid=3457297)\u001b[0m Seed set to 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3457297)\u001b[0m Train: (156, 42), Test: (65, 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3457297)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3457297)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3457297)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3457297)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_7da01b72_60_batch_size=64,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0007,data_transf_2025-02-12_10-50-05 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3457297)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3457297)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3457297)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3457297)\u001b[0m 0 | layers | ModuleList | 881 K  | train\n",
      "\u001b[36m(train_nn_reg pid=3457297)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3457297)\u001b[0m 881 K     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3457297)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3457297)\u001b[0m 881 K     Total params\n",
      "\u001b[36m(train_nn_reg pid=3457297)\u001b[0m 3.525     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3457297)\u001b[0m 24        Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3457297)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3457297)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3457297)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-02-12 10:50:17,263\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.610 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:50:17,265\tWARNING util.py:201 -- The `process_trial_result` operation took 1.613 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:50:17,266\tWARNING util.py:201 -- Processing trial results took 1.613 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:50:17,266\tWARNING util.py:201 -- The `process_trial_result` operation took 1.613 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:50:17,271\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_7da01b72\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3457297, ip=172.31.181.85, actor_id=ea2fb02b0ab310393552b5e801000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1024, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1053, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 1) cannot be concatenated\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_3c8a1025_26_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,_2025-02-12_10-43-17/checkpoint_000069)\n",
      "2025-02-12 10:50:22,014\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 4.232 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:50:22,017\tWARNING util.py:201 -- The `process_trial_result` operation took 4.235 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:50:22,017\tWARNING util.py:201 -- Processing trial results took 4.235 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:50:22,018\tWARNING util.py:201 -- The `process_trial_result` operation took 4.236 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_3c8a1025_26_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,_2025-02-12_10-43-17/checkpoint_000070)\n",
      "\u001b[36m(train_nn_reg pid=3457717)\u001b[0m Seed set to 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3457717)\u001b[0m Train: (156, 42), Test: (65, 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3457717)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3457717)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3457717)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3457717)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_0607541b_61_batch_size=32,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0007,data_transf_2025-02-12_10-50-15 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3457717)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3457717)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3457717)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3457717)\u001b[0m 0 | layers | ModuleList | 881 K  | train\n",
      "\u001b[36m(train_nn_reg pid=3457717)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3457717)\u001b[0m 881 K     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3457717)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3457717)\u001b[0m 881 K     Total params\n",
      "\u001b[36m(train_nn_reg pid=3457717)\u001b[0m 3.525     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3457717)\u001b[0m 24        Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3457717)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3457717)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(train_nn_reg pid=3457717)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3457717)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-02-12 10:50:25,561\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.987 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:50:25,563\tWARNING util.py:201 -- The `process_trial_result` operation took 1.989 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:50:25,563\tWARNING util.py:201 -- Processing trial results took 1.989 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:50:25,564\tWARNING util.py:201 -- The `process_trial_result` operation took 1.990 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:50:25,580\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_0607541b\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3457717, ip=172.31.181.85, actor_id=830dad466ce43ca079fc19bf01000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1026, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 216, in run\n",
      "    self.advance()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 455, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 151, in run\n",
      "    self.on_advance_end(data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 370, in on_advance_end\n",
      "    self.val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 2) cannot be concatenated\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_3c8a1025_26_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,_2025-02-12_10-43-17/checkpoint_000071)\n",
      "2025-02-12 10:50:27,446\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.561 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:50:27,448\tWARNING util.py:201 -- The `process_trial_result` operation took 1.563 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:50:27,449\tWARNING util.py:201 -- Processing trial results took 1.564 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:50:27,449\tWARNING util.py:201 -- The `process_trial_result` operation took 1.565 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_3c8a1025_26_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,_2025-02-12_10-43-17/checkpoint_000072)\n",
      "2025-02-12 10:50:31,898\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 3.858 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:50:31,900\tWARNING util.py:201 -- The `process_trial_result` operation took 3.861 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:50:31,900\tWARNING util.py:201 -- Processing trial results took 3.861 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:50:31,901\tWARNING util.py:201 -- The `process_trial_result` operation took 3.861 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3457982)\u001b[0m Seed set to 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3457982)\u001b[0m Train: (156, 42), Test: (65, 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3457982)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3457982)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3457982)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3457982)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_23753ef6_62_batch_size=64,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0008,data_transf_2025-02-12_10-50-23 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3457982)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3457982)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3457982)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3457982)\u001b[0m 0 | layers | ModuleList | 881 K  | train\n",
      "\u001b[36m(train_nn_reg pid=3457982)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3457982)\u001b[0m 881 K     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3457982)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3457982)\u001b[0m 881 K     Total params\n",
      "\u001b[36m(train_nn_reg pid=3457982)\u001b[0m 3.525     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3457982)\u001b[0m 24        Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3457982)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3457982)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3457982)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_3c8a1025_26_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,_2025-02-12_10-43-17/checkpoint_000073)\n",
      "2025-02-12 10:50:35,140\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.575 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:50:35,142\tWARNING util.py:201 -- The `process_trial_result` operation took 1.577 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:50:35,142\tWARNING util.py:201 -- Processing trial results took 1.578 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:50:35,143\tWARNING util.py:201 -- The `process_trial_result` operation took 1.578 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:50:35,145\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_23753ef6\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3457982, ip=172.31.181.85, actor_id=f726a3906e74357d961202a601000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1024, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1053, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 1) cannot be concatenated\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_3c8a1025_26_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,_2025-02-12_10-43-17/checkpoint_000074)\n",
      "2025-02-12 10:50:39,952\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 4.309 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:50:39,955\tWARNING util.py:201 -- The `process_trial_result` operation took 4.311 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:50:39,955\tWARNING util.py:201 -- Processing trial results took 4.312 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:50:39,956\tWARNING util.py:201 -- The `process_trial_result` operation took 4.312 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3458161)\u001b[0m Seed set to 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3458161)\u001b[0m Train: (156, 42), Test: (65, 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3458161)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3458161)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3458161)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3458161)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_c8b3d228_63_batch_size=32,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0009,data_transf_2025-02-12_10-50-33 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3458161)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3458161)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3458161)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3458161)\u001b[0m 0 | layers | ModuleList | 881 K  | train\n",
      "\u001b[36m(train_nn_reg pid=3458161)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3458161)\u001b[0m 881 K     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3458161)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3458161)\u001b[0m 881 K     Total params\n",
      "\u001b[36m(train_nn_reg pid=3458161)\u001b[0m 3.525     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3458161)\u001b[0m 24        Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3458161)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_3c8a1025_26_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,_2025-02-12_10-43-17/checkpoint_000075)\n",
      "\u001b[36m(train_nn_reg pid=3458161)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(train_nn_reg pid=3458161)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3458161)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-02-12 10:50:43,259\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.628 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:50:43,261\tWARNING util.py:201 -- The `process_trial_result` operation took 1.630 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:50:43,262\tWARNING util.py:201 -- Processing trial results took 1.631 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:50:43,263\tWARNING util.py:201 -- The `process_trial_result` operation took 1.632 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:50:43,278\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_c8b3d228\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3458161, ip=172.31.181.85, actor_id=5f5cb568e8d96f0022c9cb4e01000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1026, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 216, in run\n",
      "    self.advance()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 455, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 151, in run\n",
      "    self.on_advance_end(data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 370, in on_advance_end\n",
      "    self.val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 2) cannot be concatenated\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_3c8a1025_26_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,_2025-02-12_10-43-17/checkpoint_000076)\n",
      "2025-02-12 10:50:45,189\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.589 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:50:45,191\tWARNING util.py:201 -- The `process_trial_result` operation took 1.592 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:50:45,191\tWARNING util.py:201 -- Processing trial results took 1.592 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:50:45,192\tWARNING util.py:201 -- The `process_trial_result` operation took 1.593 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_3c8a1025_26_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,_2025-02-12_10-43-17/checkpoint_000077)\n",
      "2025-02-12 10:50:50,333\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 4.578 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:50:50,336\tWARNING util.py:201 -- The `process_trial_result` operation took 4.581 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:50:50,337\tWARNING util.py:201 -- Processing trial results took 4.582 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:50:50,337\tWARNING util.py:201 -- The `process_trial_result` operation took 4.582 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_3c8a1025_26_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,_2025-02-12_10-43-17/checkpoint_000078)\n",
      "\u001b[36m(train_nn_reg pid=3458366)\u001b[0m Seed set to 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3458366)\u001b[0m Train: (156, 42), Test: (65, 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3458366)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3458366)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3458366)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3458366)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_b7d8f5dd_64_batch_size=64,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0007,data_transf_2025-02-12_10-50-41 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3458366)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3458366)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3458366)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3458366)\u001b[0m 0 | layers | ModuleList | 881 K  | train\n",
      "\u001b[36m(train_nn_reg pid=3458366)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3458366)\u001b[0m 881 K     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3458366)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3458366)\u001b[0m 881 K     Total params\n",
      "\u001b[36m(train_nn_reg pid=3458366)\u001b[0m 3.525     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3458366)\u001b[0m 24        Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3458366)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3458366)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3458366)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-02-12 10:50:53,377\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.586 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:50:53,379\tWARNING util.py:201 -- The `process_trial_result` operation took 1.588 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:50:53,380\tWARNING util.py:201 -- Processing trial results took 1.589 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:50:53,380\tWARNING util.py:201 -- The `process_trial_result` operation took 1.589 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:50:53,399\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_b7d8f5dd\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3458366, ip=172.31.181.85, actor_id=587dce315ccb323ccfa74d3b01000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1024, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1053, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 1) cannot be concatenated\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_3c8a1025_26_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,_2025-02-12_10-43-17/checkpoint_000079)\n",
      "2025-02-12 10:50:55,276\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.564 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:50:55,278\tWARNING util.py:201 -- The `process_trial_result` operation took 1.566 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:50:55,278\tWARNING util.py:201 -- Processing trial results took 1.567 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:50:55,279\tWARNING util.py:201 -- The `process_trial_result` operation took 1.567 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_3c8a1025_26_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,_2025-02-12_10-43-17/checkpoint_000080)\n",
      "2025-02-12 10:50:59,678\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 3.820 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:50:59,681\tWARNING util.py:201 -- The `process_trial_result` operation took 3.823 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:50:59,682\tWARNING util.py:201 -- Processing trial results took 3.824 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:50:59,683\tWARNING util.py:201 -- The `process_trial_result` operation took 3.825 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3458778)\u001b[0m Seed set to 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3458778)\u001b[0m Train: (156, 42), Test: (65, 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3458778)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3458778)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3458778)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3458778)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_17a82890_65_batch_size=64,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0008,data_transf_2025-02-12_10-50-51 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3458778)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3458778)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3458778)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3458778)\u001b[0m 0 | layers | ModuleList | 881 K  | train\n",
      "\u001b[36m(train_nn_reg pid=3458778)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3458778)\u001b[0m 881 K     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3458778)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3458778)\u001b[0m 881 K     Total params\n",
      "\u001b[36m(train_nn_reg pid=3458778)\u001b[0m 3.525     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3458778)\u001b[0m 24        Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3458778)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_3c8a1025_26_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,_2025-02-12_10-43-17/checkpoint_000081)\n",
      "\u001b[36m(train_nn_reg pid=3458778)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3458778)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-02-12 10:51:05,126\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.642 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:51:05,129\tWARNING util.py:201 -- The `process_trial_result` operation took 2.644 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:51:05,129\tWARNING util.py:201 -- Processing trial results took 2.645 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:51:05,131\tWARNING util.py:201 -- The `process_trial_result` operation took 2.646 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:51:05,136\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_17a82890\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3458778, ip=172.31.181.85, actor_id=cc1a8d0b2a2775b05ae400ba01000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1024, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1053, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 1) cannot be concatenated\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_3c8a1025_26_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,_2025-02-12_10-43-17/checkpoint_000082)\n",
      "2025-02-12 10:51:09,398\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 3.604 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:51:09,400\tWARNING util.py:201 -- The `process_trial_result` operation took 3.607 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:51:09,400\tWARNING util.py:201 -- Processing trial results took 3.607 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:51:09,401\tWARNING util.py:201 -- The `process_trial_result` operation took 3.608 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_3c8a1025_26_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,_2025-02-12_10-43-17/checkpoint_000083)\n",
      "\u001b[36m(train_nn_reg pid=3459018)\u001b[0m Seed set to 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3459018)\u001b[0m Train: (156, 42), Test: (65, 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3459018)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3459018)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3459018)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3459018)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_eee9180a_66_batch_size=32,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0007,data_transf_2025-02-12_10-51-02 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3459018)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3459018)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3459018)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3459018)\u001b[0m 0 | layers | ModuleList | 881 K  | train\n",
      "\u001b[36m(train_nn_reg pid=3459018)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3459018)\u001b[0m 881 K     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3459018)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3459018)\u001b[0m 881 K     Total params\n",
      "\u001b[36m(train_nn_reg pid=3459018)\u001b[0m 3.525     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3459018)\u001b[0m 24        Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3459018)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3459018)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(train_nn_reg pid=3459018)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3459018)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-02-12 10:51:12,705\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.635 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:51:12,707\tWARNING util.py:201 -- The `process_trial_result` operation took 1.637 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:51:12,707\tWARNING util.py:201 -- Processing trial results took 1.638 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:51:12,707\tWARNING util.py:201 -- The `process_trial_result` operation took 1.638 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:51:12,736\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_eee9180a\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3459018, ip=172.31.181.85, actor_id=b894d6c153c22b6dceba862101000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1026, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 216, in run\n",
      "    self.advance()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 455, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 151, in run\n",
      "    self.on_advance_end(data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 370, in on_advance_end\n",
      "    self.val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 2) cannot be concatenated\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_3c8a1025_26_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,_2025-02-12_10-43-17/checkpoint_000084)\n",
      "2025-02-12 10:51:17,502\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 3.350 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:51:17,504\tWARNING util.py:201 -- The `process_trial_result` operation took 3.353 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:51:17,505\tWARNING util.py:201 -- Processing trial results took 3.353 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:51:17,505\tWARNING util.py:201 -- The `process_trial_result` operation took 3.354 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_3c8a1025_26_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,_2025-02-12_10-43-17/checkpoint_000085)\n",
      "\u001b[36m(train_nn_reg pid=3459390)\u001b[0m Seed set to 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3459390)\u001b[0m Train: (156, 43), Test: (65, 43)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3459390)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3459390)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3459390)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3459390)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_efd632d9_67_batch_size=64,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0006,data_transf_2025-02-12_10-51-11 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3459390)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3459390)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3459390)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3459390)\u001b[0m 0 | layers | ModuleList | 881 K  | train\n",
      "\u001b[36m(train_nn_reg pid=3459390)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3459390)\u001b[0m 881 K     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3459390)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3459390)\u001b[0m 881 K     Total params\n",
      "\u001b[36m(train_nn_reg pid=3459390)\u001b[0m 3.526     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3459390)\u001b[0m 24        Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3459390)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3459390)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3459390)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-02-12 10:51:20,874\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.654 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:51:20,876\tWARNING util.py:201 -- The `process_trial_result` operation took 1.657 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:51:20,877\tWARNING util.py:201 -- Processing trial results took 1.657 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:51:20,877\tWARNING util.py:201 -- The `process_trial_result` operation took 1.658 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:51:20,931\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_efd632d9\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3459390, ip=172.31.181.85, actor_id=b0a0ba27069c2befee18f02101000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1024, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1053, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 1) cannot be concatenated\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_3c8a1025_26_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,_2025-02-12_10-43-17/checkpoint_000086)\n",
      "2025-02-12 10:51:22,859\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.592 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:51:22,861\tWARNING util.py:201 -- The `process_trial_result` operation took 1.595 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:51:22,862\tWARNING util.py:201 -- Processing trial results took 1.595 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:51:22,862\tWARNING util.py:201 -- The `process_trial_result` operation took 1.596 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_3c8a1025_26_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,_2025-02-12_10-43-17/checkpoint_000087)\n",
      "2025-02-12 10:51:28,385\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 4.955 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:51:28,388\tWARNING util.py:201 -- The `process_trial_result` operation took 4.958 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:51:28,388\tWARNING util.py:201 -- Processing trial results took 4.958 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:51:28,388\tWARNING util.py:201 -- The `process_trial_result` operation took 4.958 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3459594)\u001b[0m Seed set to 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3459594)\u001b[0m Train: (156, 42), Test: (65, 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3459594)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3459594)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3459594)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3459594)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_dc108924_68_batch_size=64,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0007,data_transf_2025-02-12_10-51-19 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3459594)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3459594)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3459594)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3459594)\u001b[0m 0 | layers | ModuleList | 881 K  | train\n",
      "\u001b[36m(train_nn_reg pid=3459594)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3459594)\u001b[0m 881 K     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3459594)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3459594)\u001b[0m 881 K     Total params\n",
      "\u001b[36m(train_nn_reg pid=3459594)\u001b[0m 3.525     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3459594)\u001b[0m 24        Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3459594)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_3c8a1025_26_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,_2025-02-12_10-43-17/checkpoint_000088)\n",
      "\u001b[36m(train_nn_reg pid=3459594)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3459594)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-02-12 10:51:31,607\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.593 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:51:31,609\tWARNING util.py:201 -- The `process_trial_result` operation took 1.595 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:51:31,610\tWARNING util.py:201 -- Processing trial results took 1.596 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:51:31,610\tWARNING util.py:201 -- The `process_trial_result` operation took 1.596 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:51:31,613\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_dc108924\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3459594, ip=172.31.181.85, actor_id=22c4b5a8788f1e3b4a970def01000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1024, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1053, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 1) cannot be concatenated\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_3c8a1025_26_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,_2025-02-12_10-43-17/checkpoint_000089)\n",
      "2025-02-12 10:51:37,372\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 5.066 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:51:37,375\tWARNING util.py:201 -- The `process_trial_result` operation took 5.069 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:51:37,375\tWARNING util.py:201 -- Processing trial results took 5.070 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:51:37,376\tWARNING util.py:201 -- The `process_trial_result` operation took 5.070 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3459766)\u001b[0m Seed set to 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3459766)\u001b[0m Train: (156, 42), Test: (65, 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3459766)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3459766)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3459766)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3459766)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_bcd4bc02_69_batch_size=32,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0008,data_transf_2025-02-12_10-51-29 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3459766)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3459766)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3459766)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3459766)\u001b[0m 0 | layers | ModuleList | 881 K  | train\n",
      "\u001b[36m(train_nn_reg pid=3459766)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3459766)\u001b[0m 881 K     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3459766)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3459766)\u001b[0m 881 K     Total params\n",
      "\u001b[36m(train_nn_reg pid=3459766)\u001b[0m 3.525     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3459766)\u001b[0m 24        Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3459766)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_3c8a1025_26_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,_2025-02-12_10-43-17/checkpoint_000090)\n",
      "\u001b[36m(train_nn_reg pid=3459766)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(train_nn_reg pid=3459766)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3459766)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-02-12 10:51:40,743\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.675 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:51:40,746\tWARNING util.py:201 -- The `process_trial_result` operation took 1.678 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:51:40,746\tWARNING util.py:201 -- Processing trial results took 1.679 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:51:40,747\tWARNING util.py:201 -- The `process_trial_result` operation took 1.679 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:51:40,750\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_bcd4bc02\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3459766, ip=172.31.181.85, actor_id=34d1339c83f0435eba902cfc01000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1026, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 216, in run\n",
      "    self.advance()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 455, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 151, in run\n",
      "    self.on_advance_end(data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 370, in on_advance_end\n",
      "    self.val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 2) cannot be concatenated\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_3c8a1025_26_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,_2025-02-12_10-43-17/checkpoint_000091)\n",
      "2025-02-12 10:51:42,824\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.569 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:51:42,826\tWARNING util.py:201 -- The `process_trial_result` operation took 1.571 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:51:42,827\tWARNING util.py:201 -- Processing trial results took 1.572 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:51:42,828\tWARNING util.py:201 -- The `process_trial_result` operation took 1.573 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_3c8a1025_26_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,_2025-02-12_10-43-17/checkpoint_000092)\n",
      "2025-02-12 10:51:49,365\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 5.939 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:51:49,367\tWARNING util.py:201 -- The `process_trial_result` operation took 5.942 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:51:49,367\tWARNING util.py:201 -- Processing trial results took 5.942 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:51:49,368\tWARNING util.py:201 -- The `process_trial_result` operation took 5.943 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_3c8a1025_26_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,_2025-02-12_10-43-17/checkpoint_000093)\n",
      "\u001b[36m(train_nn_reg pid=3459971)\u001b[0m Seed set to 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3459971)\u001b[0m Train: (156, 42), Test: (65, 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3459971)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3459971)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3459971)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3459971)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_9df06404_70_batch_size=64,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0006,data_transf_2025-02-12_10-51-38 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3459971)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3459971)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3459971)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3459971)\u001b[0m 0 | layers | ModuleList | 881 K  | train\n",
      "\u001b[36m(train_nn_reg pid=3459971)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3459971)\u001b[0m 881 K     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3459971)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3459971)\u001b[0m 881 K     Total params\n",
      "\u001b[36m(train_nn_reg pid=3459971)\u001b[0m 3.525     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3459971)\u001b[0m 24        Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3459971)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3459971)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3459971)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-02-12 10:51:52,557\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.703 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:51:52,559\tWARNING util.py:201 -- The `process_trial_result` operation took 1.706 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:51:52,560\tWARNING util.py:201 -- Processing trial results took 1.706 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:51:52,560\tWARNING util.py:201 -- The `process_trial_result` operation took 1.706 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:51:52,566\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_9df06404\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3459971, ip=172.31.181.85, actor_id=098f5f4f9ef24f374d40fee801000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1024, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1053, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 1) cannot be concatenated\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_3c8a1025_26_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,_2025-02-12_10-43-17/checkpoint_000094)\n",
      "2025-02-12 10:51:57,512\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 4.269 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:51:57,514\tWARNING util.py:201 -- The `process_trial_result` operation took 4.272 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:51:57,514\tWARNING util.py:201 -- Processing trial results took 4.272 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:51:57,515\tWARNING util.py:201 -- The `process_trial_result` operation took 4.273 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_3c8a1025_26_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,_2025-02-12_10-43-17/checkpoint_000095)\n",
      "\u001b[36m(train_nn_reg pid=3460383)\u001b[0m Seed set to 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3460383)\u001b[0m Train: (156, 42), Test: (65, 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3460383)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3460383)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3460383)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3460383)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_d0e4e46e_71_batch_size=32,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0008,data_transf_2025-02-12_10-51-50 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3460383)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3460383)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3460383)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3460383)\u001b[0m 0 | layers | ModuleList | 881 K  | train\n",
      "\u001b[36m(train_nn_reg pid=3460383)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3460383)\u001b[0m 881 K     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3460383)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3460383)\u001b[0m 881 K     Total params\n",
      "\u001b[36m(train_nn_reg pid=3460383)\u001b[0m 3.525     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3460383)\u001b[0m 24        Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3460383)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3460383)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(train_nn_reg pid=3460383)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3460383)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-02-12 10:52:02,182\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.609 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:52:02,184\tWARNING util.py:201 -- The `process_trial_result` operation took 2.611 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:52:02,184\tWARNING util.py:201 -- Processing trial results took 2.611 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:52:02,185\tWARNING util.py:201 -- The `process_trial_result` operation took 2.612 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:52:02,203\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_d0e4e46e\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3460383, ip=172.31.181.85, actor_id=98ddd2a814c73e6f8ab588af01000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1026, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 216, in run\n",
      "    self.advance()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 455, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 151, in run\n",
      "    self.on_advance_end(data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 370, in on_advance_end\n",
      "    self.val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 2) cannot be concatenated\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_3c8a1025_26_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,_2025-02-12_10-43-17/checkpoint_000096)\n",
      "2025-02-12 10:52:04,791\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.188 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:52:04,793\tWARNING util.py:201 -- The `process_trial_result` operation took 2.191 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:52:04,794\tWARNING util.py:201 -- Processing trial results took 2.191 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:52:04,794\tWARNING util.py:201 -- The `process_trial_result` operation took 2.192 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_3c8a1025_26_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,_2025-02-12_10-43-17/checkpoint_000097)\n",
      "2025-02-12 10:52:09,167\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 3.842 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:52:09,170\tWARNING util.py:201 -- The `process_trial_result` operation took 3.845 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:52:09,170\tWARNING util.py:201 -- Processing trial results took 3.845 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:52:09,171\tWARNING util.py:201 -- The `process_trial_result` operation took 3.846 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_3c8a1025_26_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,_2025-02-12_10-43-17/checkpoint_000098)\n",
      "\u001b[36m(train_nn_reg pid=3460719)\u001b[0m Seed set to 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3460719)\u001b[0m Train: (156, 42), Test: (65, 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3460719)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3460719)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3460719)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3460719)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_11891b5f_72_batch_size=64,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0007,data_transf_2025-02-12_10-51-59 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3460719)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3460719)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3460719)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3460719)\u001b[0m 0 | layers | ModuleList | 737 K  | train\n",
      "\u001b[36m(train_nn_reg pid=3460719)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3460719)\u001b[0m 737 K     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3460719)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3460719)\u001b[0m 737 K     Total params\n",
      "\u001b[36m(train_nn_reg pid=3460719)\u001b[0m 2.951     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3460719)\u001b[0m 24        Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3460719)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3460719)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3460719)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-02-12 10:52:13,363\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.613 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:52:13,366\tWARNING util.py:201 -- The `process_trial_result` operation took 1.616 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:52:13,366\tWARNING util.py:201 -- Processing trial results took 1.616 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:52:13,367\tWARNING util.py:201 -- The `process_trial_result` operation took 1.617 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:52:13,403\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_11891b5f\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3460719, ip=172.31.181.85, actor_id=ebabb5017d4f8204114acd7101000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1024, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1053, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 1) cannot be concatenated\n",
      "\u001b[36m(train_nn_reg pid=3444073)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg/train_nn_reg_3c8a1025_26_batch_size=128,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,_2025-02-12_10-43-17/checkpoint_000099)\n",
      "2025-02-12 10:52:18,275\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 3.807 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:52:18,277\tWARNING util.py:201 -- The `process_trial_result` operation took 3.809 s, which may be a performance bottleneck.\n",
      "2025-02-12 10:52:18,278\tWARNING util.py:201 -- Processing trial results took 3.810 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-02-12 10:52:18,279\tWARNING util.py:201 -- The `process_trial_result` operation took 3.811 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_nn_reg pid=3460987)\u001b[0m Seed set to 12\n",
      "\u001b[36m(train_nn_reg pid=3460987)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3460987)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3460987)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3460987)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_8466fb79_73_batch_size=64,data_aggregation=None,data_selection=variance_threshold,data_selection_t=0.0008,data_transf_2025-02-12_10-52-11 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3460987)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3460987)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3460987)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3460987)\u001b[0m 0 | layers | ModuleList | 1.2 K  | train\n",
      "\u001b[36m(train_nn_reg pid=3460987)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3460987)\u001b[0m 1.2 K     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3460987)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3460987)\u001b[0m 1.2 K     Total params\n",
      "\u001b[36m(train_nn_reg pid=3460987)\u001b[0m 0.005     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3460987)\u001b[0m 4         Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3460987)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3460987)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3460987)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-02-12 10:52:19,880\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_8466fb79\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3460987, ip=172.31.181.85, actor_id=aa53f5fbedd43314af41e97901000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1024, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1053, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 1) cannot be concatenated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3460987)\u001b[0m Train: (156, 42), Test: (65, 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3461101)\u001b[0m Seed set to 12\n",
      "\u001b[36m(train_nn_reg pid=3461101)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3461101)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3461101)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3461101)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_395d94f1_74_batch_size=64,data_aggregation=None,data_selection=abundance_ith,data_selection_i=2,data_transform=None,e_2025-02-12_10-52-19 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3461101)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3461101)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3461101)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3461101)\u001b[0m 0 | layers | ModuleList | 1.5 M  | train\n",
      "\u001b[36m(train_nn_reg pid=3461101)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3461101)\u001b[0m 1.5 M     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3461101)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3461101)\u001b[0m 1.5 M     Total params\n",
      "\u001b[36m(train_nn_reg pid=3461101)\u001b[0m 5.891     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3461101)\u001b[0m 54        Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3461101)\u001b[0m 0         Modules in eval mode\n",
      "2025-02-12 10:52:28,315\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_395d94f1\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3461101, ip=172.31.181.85, actor_id=4c5216c64cf1461e43f58a0801000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1024, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1053, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 1) cannot be concatenated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3461101)\u001b[0m Train: (156, 159), Test: (65, 159)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3461101)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3461101)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "\u001b[36m(train_nn_reg pid=3461190)\u001b[0m Seed set to 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3461190)\u001b[0m Train: (156, 537), Test: (65, 537)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3461190)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3461190)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3461190)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3461190)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_a7d0654d_75_batch_size=32,data_aggregation=None,data_selection=abundance_ith,data_selection_i=3,data_transform=None,e_2025-02-12_10-52-27 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3461190)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3461190)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3461190)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3461190)\u001b[0m 0 | layers | ModuleList | 1.3 M  | train\n",
      "\u001b[36m(train_nn_reg pid=3461190)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3461190)\u001b[0m 1.3 M     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3461190)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3461190)\u001b[0m 1.3 M     Total params\n",
      "\u001b[36m(train_nn_reg pid=3461190)\u001b[0m 5.250     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3461190)\u001b[0m 54        Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3461190)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3461190)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(train_nn_reg pid=3461190)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3461190)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-02-12 10:52:35,948\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_a7d0654d\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3461190, ip=172.31.181.85, actor_id=6e3b2be8033705a968cd9a2e01000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1026, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 216, in run\n",
      "    self.advance()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 455, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 151, in run\n",
      "    self.on_advance_end(data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 370, in on_advance_end\n",
      "    self.val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 2) cannot be concatenated\n",
      "\u001b[36m(train_nn_reg pid=3461278)\u001b[0m Seed set to 12\n",
      "\u001b[36m(train_nn_reg pid=3461278)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3461278)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3461278)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3461278)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_595528ac_76_batch_size=64,data_aggregation=None,data_selection=abundance_ith,data_selection_i=2,data_transform=None,e_2025-02-12_10-52-35 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3461278)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3461278)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3461278)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3461278)\u001b[0m 0 | layers | ModuleList | 1.4 M  | train\n",
      "\u001b[36m(train_nn_reg pid=3461278)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3461278)\u001b[0m 1.4 M     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3461278)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3461278)\u001b[0m 1.4 M     Total params\n",
      "\u001b[36m(train_nn_reg pid=3461278)\u001b[0m 5.425     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3461278)\u001b[0m 54        Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3461278)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3461278)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3461278)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-02-12 10:52:42,374\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_595528ac\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3461278, ip=172.31.181.85, actor_id=49840651e352b8a2003cf23301000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1024, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1053, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 1) cannot be concatenated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3461278)\u001b[0m Train: (156, 159), Test: (65, 159)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3461425)\u001b[0m Seed set to 12\n",
      "\u001b[36m(train_nn_reg pid=3461425)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3461425)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3461425)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3461425)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_fbb76e04_77_batch_size=32,data_aggregation=None,data_selection=abundance_ith,data_selection_i=2,data_transform=None,e_2025-02-12_10-52-42 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3461425)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3461425)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3461425)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3461425)\u001b[0m 0 | layers | ModuleList | 772 K  | train\n",
      "\u001b[36m(train_nn_reg pid=3461425)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3461425)\u001b[0m 772 K     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3461425)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3461425)\u001b[0m 772 K     Total params\n",
      "\u001b[36m(train_nn_reg pid=3461425)\u001b[0m 3.091     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3461425)\u001b[0m 54        Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3461425)\u001b[0m 0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3461425)\u001b[0m Train: (156, 159), Test: (65, 159)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3461425)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "2025-02-12 10:52:50,430\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_fbb76e04\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3461425, ip=172.31.181.85, actor_id=02e043de36083f7716dafce101000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1026, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 216, in run\n",
      "    self.advance()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 455, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 151, in run\n",
      "    self.on_advance_end(data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 370, in on_advance_end\n",
      "    self.val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 2) cannot be concatenated\n",
      "\u001b[36m(train_nn_reg pid=3461425)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3461425)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "\u001b[36m(train_nn_reg pid=3461513)\u001b[0m Seed set to 12\n",
      "\u001b[36m(train_nn_reg pid=3461513)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3461513)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3461513)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3461513)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_dcd4a051_78_batch_size=64,data_aggregation=None,data_selection=abundance_ith,data_selection_i=2,data_transform=None,e_2025-02-12_10-52-49 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3461513)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3461513)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3461513)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3461513)\u001b[0m 0 | layers | ModuleList | 1.6 M  | train\n",
      "\u001b[36m(train_nn_reg pid=3461513)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3461513)\u001b[0m 1.6 M     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3461513)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3461513)\u001b[0m 1.6 M     Total params\n",
      "\u001b[36m(train_nn_reg pid=3461513)\u001b[0m 6.547     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3461513)\u001b[0m 54        Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3461513)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3461513)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3461513)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-02-12 10:52:57,049\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_dcd4a051\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3461513, ip=172.31.181.85, actor_id=a1e032741e6328a05763c44801000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1024, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1053, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 1) cannot be concatenated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3461513)\u001b[0m Train: (156, 159), Test: (65, 159)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3461661)\u001b[0m Seed set to 12\n",
      "\u001b[36m(train_nn_reg pid=3461661)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3461661)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3461661)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3461661)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_5781d649_79_batch_size=64,data_aggregation=None,data_selection=abundance_ith,data_selection_i=3,data_transform=None,e_2025-02-12_10-52-56 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3461661)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3461661)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3461661)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3461661)\u001b[0m 0 | layers | ModuleList | 1.2 M  | train\n",
      "\u001b[36m(train_nn_reg pid=3461661)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3461661)\u001b[0m 1.2 M     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3461661)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3461661)\u001b[0m 1.2 M     Total params\n",
      "\u001b[36m(train_nn_reg pid=3461661)\u001b[0m 4.798     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3461661)\u001b[0m 54        Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3461661)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3461661)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3461661)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-02-12 10:53:04,521\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_5781d649\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3461661, ip=172.31.181.85, actor_id=f84900f586f7d8901ee3bbda01000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1024, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1053, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 1) cannot be concatenated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3461661)\u001b[0m Train: (156, 537), Test: (65, 537)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3461820)\u001b[0m Seed set to 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3461820)\u001b[0m Train: (156, 159), Test: (65, 159)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3461820)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3461820)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3461820)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3461820)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_c96b517b_80_batch_size=64,data_aggregation=None,data_selection=abundance_ith,data_selection_i=2,data_transform=None,e_2025-02-12_10-53-04 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3461820)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3461820)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3461820)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3461820)\u001b[0m 0 | layers | ModuleList | 965 K  | train\n",
      "\u001b[36m(train_nn_reg pid=3461820)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3461820)\u001b[0m 965 K     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3461820)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3461820)\u001b[0m 965 K     Total params\n",
      "\u001b[36m(train_nn_reg pid=3461820)\u001b[0m 3.863     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3461820)\u001b[0m 54        Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3461820)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3461820)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3461820)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-02-12 10:53:12,063\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_c96b517b\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3461820, ip=172.31.181.85, actor_id=77e728444841887c7399cb7f01000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1024, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1053, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 1) cannot be concatenated\n",
      "\u001b[36m(train_nn_reg pid=3461907)\u001b[0m Seed set to 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3461907)\u001b[0m Train: (156, 159), Test: (65, 159)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3461907)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3461907)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3461907)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3461907)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_f6efb6b7_81_batch_size=32,data_aggregation=None,data_selection=abundance_ith,data_selection_i=2,data_transform=None,e_2025-02-12_10-53-11 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3461907)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3461907)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3461907)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3461907)\u001b[0m 0 | layers | ModuleList | 1.2 M  | train\n",
      "\u001b[36m(train_nn_reg pid=3461907)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3461907)\u001b[0m 1.2 M     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3461907)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3461907)\u001b[0m 1.2 M     Total params\n",
      "\u001b[36m(train_nn_reg pid=3461907)\u001b[0m 4.865     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3461907)\u001b[0m 54        Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3461907)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3461907)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(train_nn_reg pid=3461907)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3461907)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-02-12 10:53:19,235\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_f6efb6b7\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3461907, ip=172.31.181.85, actor_id=08f4d48a325bb3c4034515b701000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1026, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 216, in run\n",
      "    self.advance()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 455, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 151, in run\n",
      "    self.on_advance_end(data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 370, in on_advance_end\n",
      "    self.val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 2) cannot be concatenated\n",
      "\u001b[36m(train_nn_reg pid=3462000)\u001b[0m Seed set to 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3462000)\u001b[0m Train: (156, 159), Test: (65, 159)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3462000)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3462000)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3462000)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3462000)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_916754ae_82_batch_size=32,data_aggregation=None,data_selection=abundance_ith,data_selection_i=2,data_transform=None,e_2025-02-12_10-53-18 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3462000)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3462000)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3462000)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3462000)\u001b[0m 0 | layers | ModuleList | 1.5 M  | train\n",
      "\u001b[36m(train_nn_reg pid=3462000)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3462000)\u001b[0m 1.5 M     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3462000)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3462000)\u001b[0m 1.5 M     Total params\n",
      "\u001b[36m(train_nn_reg pid=3462000)\u001b[0m 5.993     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3462000)\u001b[0m 54        Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3462000)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3462000)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(train_nn_reg pid=3462000)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3462000)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-02-12 10:53:26,429\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_916754ae\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3462000, ip=172.31.181.85, actor_id=5e76bb4814d792581dd814d701000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1026, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 216, in run\n",
      "    self.advance()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 455, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 151, in run\n",
      "    self.on_advance_end(data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 370, in on_advance_end\n",
      "    self.val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 2) cannot be concatenated\n",
      "\u001b[36m(train_nn_reg pid=3462147)\u001b[0m Seed set to 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3462147)\u001b[0m Train: (156, 537), Test: (65, 537)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3462147)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3462147)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3462147)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3462147)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_a489d7a5_83_batch_size=64,data_aggregation=None,data_selection=abundance_ith,data_selection_i=3,data_transform=None,e_2025-02-12_10-53-25 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3462147)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3462147)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3462147)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3462147)\u001b[0m 0 | layers | ModuleList | 1.3 M  | train\n",
      "\u001b[36m(train_nn_reg pid=3462147)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3462147)\u001b[0m 1.3 M     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3462147)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3462147)\u001b[0m 1.3 M     Total params\n",
      "\u001b[36m(train_nn_reg pid=3462147)\u001b[0m 5.005     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3462147)\u001b[0m 54        Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3462147)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3462147)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3462147)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-02-12 10:53:33,539\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_a489d7a5\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3462147, ip=172.31.181.85, actor_id=9087fe36409da4feddfc62eb01000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1024, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1053, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 1) cannot be concatenated\n",
      "\u001b[36m(train_nn_reg pid=3462295)\u001b[0m Seed set to 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3462295)\u001b[0m Train: (156, 159), Test: (65, 159)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3462295)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3462295)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3462295)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3462295)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_bf250c6e_84_batch_size=32,data_aggregation=None,data_selection=abundance_ith,data_selection_i=2,data_transform=None,e_2025-02-12_10-53-33 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3462295)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3462295)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3462295)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3462295)\u001b[0m 0 | layers | ModuleList | 1.1 M  | train\n",
      "\u001b[36m(train_nn_reg pid=3462295)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3462295)\u001b[0m 1.1 M     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3462295)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3462295)\u001b[0m 1.1 M     Total params\n",
      "\u001b[36m(train_nn_reg pid=3462295)\u001b[0m 4.455     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3462295)\u001b[0m 54        Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3462295)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3462295)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(train_nn_reg pid=3462295)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3462295)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-02-12 10:53:40,511\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_bf250c6e\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3462295, ip=172.31.181.85, actor_id=0a92df868751bbdb5d49fc8c01000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1026, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 216, in run\n",
      "    self.advance()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 455, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 151, in run\n",
      "    self.on_advance_end(data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 370, in on_advance_end\n",
      "    self.val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 2) cannot be concatenated\n",
      "\u001b[36m(train_nn_reg pid=3462383)\u001b[0m Seed set to 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3462383)\u001b[0m Train: (156, 159), Test: (65, 159)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3462383)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3462383)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3462383)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3462383)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_7d1f05b3_85_batch_size=32,data_aggregation=None,data_selection=abundance_ith,data_selection_i=2,data_transform=None,e_2025-02-12_10-53-39 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3462383)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3462383)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3462383)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3462383)\u001b[0m 0 | layers | ModuleList | 1.2 M  | train\n",
      "\u001b[36m(train_nn_reg pid=3462383)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3462383)\u001b[0m 1.2 M     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3462383)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3462383)\u001b[0m 1.2 M     Total params\n",
      "\u001b[36m(train_nn_reg pid=3462383)\u001b[0m 4.995     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3462383)\u001b[0m 54        Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3462383)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3462383)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(train_nn_reg pid=3462383)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3462383)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-02-12 10:53:47,774\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_7d1f05b3\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3462383, ip=172.31.181.85, actor_id=96429e9b459b0bbb62803c8401000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1026, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 216, in run\n",
      "    self.advance()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 455, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 151, in run\n",
      "    self.on_advance_end(data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 370, in on_advance_end\n",
      "    self.val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 2) cannot be concatenated\n",
      "\u001b[36m(train_nn_reg pid=3462529)\u001b[0m Seed set to 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3462529)\u001b[0m Train: (156, 159), Test: (65, 159)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3462529)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3462529)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3462529)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3462529)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_c7e2ac19_86_batch_size=64,data_aggregation=None,data_selection=abundance_ith,data_selection_i=2,data_transform=None,e_2025-02-12_10-53-46 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3462529)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3462529)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3462529)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3462529)\u001b[0m 0 | layers | ModuleList | 1.2 M  | train\n",
      "\u001b[36m(train_nn_reg pid=3462529)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3462529)\u001b[0m 1.2 M     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3462529)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3462529)\u001b[0m 1.2 M     Total params\n",
      "\u001b[36m(train_nn_reg pid=3462529)\u001b[0m 4.893     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3462529)\u001b[0m 54        Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3462529)\u001b[0m 0         Modules in eval mode\n",
      "2025-02-12 10:53:54,963\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_c7e2ac19\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3462529, ip=172.31.181.85, actor_id=ec8ef2e86f50b1ef2741589b01000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1024, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1053, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 1) cannot be concatenated\n",
      "\u001b[36m(train_nn_reg pid=3462529)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3462529)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "\u001b[36m(train_nn_reg pid=3462677)\u001b[0m Seed set to 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3462677)\u001b[0m Train: (156, 159), Test: (65, 159)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3462677)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3462677)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3462677)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3462677)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_9e6167db_87_batch_size=32,data_aggregation=None,data_selection=abundance_ith,data_selection_i=2,data_transform=None,e_2025-02-12_10-53-54 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3462677)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3462677)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3462677)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3462677)\u001b[0m 0 | layers | ModuleList | 1.8 M  | train\n",
      "\u001b[36m(train_nn_reg pid=3462677)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3462677)\u001b[0m 1.8 M     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3462677)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3462677)\u001b[0m 1.8 M     Total params\n",
      "\u001b[36m(train_nn_reg pid=3462677)\u001b[0m 7.165     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3462677)\u001b[0m 54        Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3462677)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3462677)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(train_nn_reg pid=3462677)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3462677)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-02-12 10:54:03,122\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_9e6167db\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3462677, ip=172.31.181.85, actor_id=be2790cdc201398b4000deef01000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1026, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 216, in run\n",
      "    self.advance()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 455, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 151, in run\n",
      "    self.on_advance_end(data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 370, in on_advance_end\n",
      "    self.val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 2) cannot be concatenated\n",
      "\u001b[36m(train_nn_reg pid=3462845)\u001b[0m Seed set to 12\n",
      "\u001b[36m(train_nn_reg pid=3462845)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3462845)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3462845)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3462845)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_7ac2c560_88_batch_size=64,data_aggregation=None,data_selection=abundance_ith,data_selection_i=2,data_transform=None,e_2025-02-12_10-54-02 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3462845)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3462845)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3462845)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3462845)\u001b[0m 0 | layers | ModuleList | 1.0 M  | train\n",
      "\u001b[36m(train_nn_reg pid=3462845)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3462845)\u001b[0m 1.0 M     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3462845)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3462845)\u001b[0m 1.0 M     Total params\n",
      "\u001b[36m(train_nn_reg pid=3462845)\u001b[0m 4.027     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3462845)\u001b[0m 54        Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3462845)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3462845)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3462845)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-02-12 10:54:09,743\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_7ac2c560\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3462845, ip=172.31.181.85, actor_id=d59bcf7fd2e350968ed2dd8301000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1024, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1053, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 1) cannot be concatenated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3462845)\u001b[0m Train: (156, 159), Test: (65, 159)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3462991)\u001b[0m Seed set to 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3462991)\u001b[0m Train: (156, 159), Test: (65, 159)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3462991)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3462991)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3462991)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3462991)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_68016e2d_89_batch_size=64,data_aggregation=None,data_selection=abundance_ith,data_selection_i=2,data_transform=None,e_2025-02-12_10-54-09 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3462991)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3462991)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3462991)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3462991)\u001b[0m 0 | layers | ModuleList | 1.5 M  | train\n",
      "\u001b[36m(train_nn_reg pid=3462991)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3462991)\u001b[0m 1.5 M     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3462991)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3462991)\u001b[0m 1.5 M     Total params\n",
      "\u001b[36m(train_nn_reg pid=3462991)\u001b[0m 5.956     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3462991)\u001b[0m 54        Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3462991)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3462991)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3462991)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-02-12 10:54:17,665\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_68016e2d\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3462991, ip=172.31.181.85, actor_id=cb9c597b55194d9f813d10b601000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1024, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1053, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 1) cannot be concatenated\n",
      "\u001b[36m(train_nn_reg pid=3463097)\u001b[0m Seed set to 12\n",
      "\u001b[36m(train_nn_reg pid=3463097)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3463097)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3463097)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3463097)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_4166b2ca_90_batch_size=32,data_aggregation=None,data_selection=abundance_ith,data_selection_i=2,data_transform=None,e_2025-02-12_10-54-17 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3463097)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3463097)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3463097)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3463097)\u001b[0m 0 | layers | ModuleList | 1.2 M  | train\n",
      "\u001b[36m(train_nn_reg pid=3463097)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3463097)\u001b[0m 1.2 M     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3463097)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3463097)\u001b[0m 1.2 M     Total params\n",
      "\u001b[36m(train_nn_reg pid=3463097)\u001b[0m 4.972     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3463097)\u001b[0m 54        Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3463097)\u001b[0m 0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3463097)\u001b[0m Train: (156, 159), Test: (65, 159)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3463097)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(train_nn_reg pid=3463097)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3463097)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-02-12 10:54:24,633\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_4166b2ca\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3463097, ip=172.31.181.85, actor_id=27968a7199c4137ae0dc6ac601000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1026, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 216, in run\n",
      "    self.advance()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 455, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 151, in run\n",
      "    self.on_advance_end(data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 370, in on_advance_end\n",
      "    self.val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 2) cannot be concatenated\n",
      "\u001b[36m(train_nn_reg pid=3463188)\u001b[0m Seed set to 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3463188)\u001b[0m Train: (156, 159), Test: (65, 159)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3463188)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3463188)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3463188)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3463188)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_595d4bda_91_batch_size=32,data_aggregation=None,data_selection=abundance_ith,data_selection_i=2,data_transform=None,e_2025-02-12_10-54-23 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3463188)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3463188)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3463188)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3463188)\u001b[0m 0 | layers | ModuleList | 1.4 M  | train\n",
      "\u001b[36m(train_nn_reg pid=3463188)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3463188)\u001b[0m 1.4 M     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3463188)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3463188)\u001b[0m 1.4 M     Total params\n",
      "\u001b[36m(train_nn_reg pid=3463188)\u001b[0m 5.463     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3463188)\u001b[0m 54        Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3463188)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3463188)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(train_nn_reg pid=3463188)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3463188)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-02-12 10:54:31,662\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_595d4bda\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3463188, ip=172.31.181.85, actor_id=ecde15b73203d6c72ab7acce01000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1026, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 216, in run\n",
      "    self.advance()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 455, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 151, in run\n",
      "    self.on_advance_end(data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 370, in on_advance_end\n",
      "    self.val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 2) cannot be concatenated\n",
      "\u001b[36m(train_nn_reg pid=3463453)\u001b[0m Seed set to 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3463453)\u001b[0m Train: (156, 159), Test: (65, 159)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3463453)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3463453)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3463453)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3463453)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_17ec6b6d_92_batch_size=64,data_aggregation=None,data_selection=abundance_ith,data_selection_i=2,data_transform=None,e_2025-02-12_10-54-30 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3463453)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3463453)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3463453)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3463453)\u001b[0m 0 | layers | ModuleList | 1.2 M  | train\n",
      "\u001b[36m(train_nn_reg pid=3463453)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3463453)\u001b[0m 1.2 M     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3463453)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3463453)\u001b[0m 1.2 M     Total params\n",
      "\u001b[36m(train_nn_reg pid=3463453)\u001b[0m 4.613     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3463453)\u001b[0m 54        Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3463453)\u001b[0m 0         Modules in eval mode\n",
      "2025-02-12 10:54:39,494\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_17ec6b6d\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3463453, ip=172.31.181.85, actor_id=70aea8b1a86ecb9cb5bbc7f301000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1024, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1053, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 1) cannot be concatenated\n",
      "\u001b[36m(train_nn_reg pid=3463453)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3463453)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "\u001b[36m(train_nn_reg pid=3463957)\u001b[0m Seed set to 12\n",
      "\u001b[36m(train_nn_reg pid=3463957)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3463957)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3463957)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3463957)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_6c34831c_93_batch_size=64,data_aggregation=None,data_selection=abundance_ith,data_selection_i=2,data_transform=None,e_2025-02-12_10-54-39 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3463957)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3463957)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3463957)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3463957)\u001b[0m 0 | layers | ModuleList | 1.5 M  | train\n",
      "\u001b[36m(train_nn_reg pid=3463957)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3463957)\u001b[0m 1.5 M     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3463957)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3463957)\u001b[0m 1.5 M     Total params\n",
      "\u001b[36m(train_nn_reg pid=3463957)\u001b[0m 6.145     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3463957)\u001b[0m 54        Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3463957)\u001b[0m 0         Modules in eval mode\n",
      "2025-02-12 10:54:46,201\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_6c34831c\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3463957, ip=172.31.181.85, actor_id=ab9d4adcf9a06bffc1dbde4f01000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1024, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1053, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 1) cannot be concatenated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3463957)\u001b[0m Train: (156, 159), Test: (65, 159)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3463957)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3463957)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "\u001b[36m(train_nn_reg pid=3464046)\u001b[0m Seed set to 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3464046)\u001b[0m Train: (156, 159), Test: (65, 159)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3464046)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3464046)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3464046)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3464046)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_f6ead3d2_94_batch_size=32,data_aggregation=None,data_selection=abundance_ith,data_selection_i=2,data_transform=None,e_2025-02-12_10-54-45 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3464046)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3464046)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3464046)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3464046)\u001b[0m 0 | layers | ModuleList | 986 K  | train\n",
      "\u001b[36m(train_nn_reg pid=3464046)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3464046)\u001b[0m 986 K     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3464046)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3464046)\u001b[0m 986 K     Total params\n",
      "\u001b[36m(train_nn_reg pid=3464046)\u001b[0m 3.945     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3464046)\u001b[0m 54        Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3464046)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3464046)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "2025-02-12 10:54:53,346\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_f6ead3d2\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3464046, ip=172.31.181.85, actor_id=5ba980b80d0b6d94457144ca01000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1026, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 216, in run\n",
      "    self.advance()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 455, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 151, in run\n",
      "    self.on_advance_end(data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 370, in on_advance_end\n",
      "    self.val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 2) cannot be concatenated\n",
      "\u001b[36m(train_nn_reg pid=3464046)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3464046)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "\u001b[36m(train_nn_reg pid=3464133)\u001b[0m Seed set to 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3464133)\u001b[0m Train: (156, 1032), Test: (65, 1032)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3464133)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3464133)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3464133)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3464133)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_62ddb4ba_95_batch_size=32,data_aggregation=None,data_selection=abundance_ith,data_selection_i=4,data_transform=None,e_2025-02-12_10-54-52 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3464133)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3464133)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3464133)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3464133)\u001b[0m 0 | layers | ModuleList | 1.8 M  | train\n",
      "\u001b[36m(train_nn_reg pid=3464133)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3464133)\u001b[0m 1.8 M     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3464133)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3464133)\u001b[0m 1.8 M     Total params\n",
      "\u001b[36m(train_nn_reg pid=3464133)\u001b[0m 7.391     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3464133)\u001b[0m 54        Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3464133)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3464133)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(train_nn_reg pid=3464133)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3464133)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-02-12 10:55:02,885\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_62ddb4ba\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3464133, ip=172.31.181.85, actor_id=61cd4d7091f306045cd0640d01000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1026, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 216, in run\n",
      "    self.advance()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 455, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 151, in run\n",
      "    self.on_advance_end(data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 370, in on_advance_end\n",
      "    self.val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 2) cannot be concatenated\n",
      "\u001b[36m(train_nn_reg pid=3464365)\u001b[0m Seed set to 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3464365)\u001b[0m Train: (156, 159), Test: (65, 159)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3464365)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3464365)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3464365)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3464365)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_0aeac832_96_batch_size=32,data_aggregation=None,data_selection=abundance_ith,data_selection_i=2,data_transform=None,e_2025-02-12_10-55-02 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3464365)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3464365)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3464365)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3464365)\u001b[0m 0 | layers | ModuleList | 1.5 M  | train\n",
      "\u001b[36m(train_nn_reg pid=3464365)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3464365)\u001b[0m 1.5 M     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3464365)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3464365)\u001b[0m 1.5 M     Total params\n",
      "\u001b[36m(train_nn_reg pid=3464365)\u001b[0m 5.948     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3464365)\u001b[0m 54        Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3464365)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3464365)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(train_nn_reg pid=3464365)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3464365)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-02-12 10:55:12,543\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_0aeac832\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3464365, ip=172.31.181.85, actor_id=ec1397e514c60a9d88190ec801000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1026, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 216, in run\n",
      "    self.advance()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 455, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 151, in run\n",
      "    self.on_advance_end(data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 370, in on_advance_end\n",
      "    self.val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 2) cannot be concatenated\n",
      "\u001b[36m(train_nn_reg pid=3464517)\u001b[0m Seed set to 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3464517)\u001b[0m Train: (156, 159), Test: (65, 159)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3464517)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3464517)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3464517)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3464517)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_0a5872d4_97_batch_size=32,data_aggregation=None,data_selection=abundance_ith,data_selection_i=2,data_transform=None,e_2025-02-12_10-55-11 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3464517)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3464517)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3464517)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3464517)\u001b[0m 0 | layers | ModuleList | 1.5 M  | train\n",
      "\u001b[36m(train_nn_reg pid=3464517)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3464517)\u001b[0m 1.5 M     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3464517)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3464517)\u001b[0m 1.5 M     Total params\n",
      "\u001b[36m(train_nn_reg pid=3464517)\u001b[0m 5.894     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3464517)\u001b[0m 54        Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3464517)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3464517)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(train_nn_reg pid=3464517)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3464517)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-02-12 10:55:21,846\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_0a5872d4\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3464517, ip=172.31.181.85, actor_id=9932e8afdd792971c51f5b5e01000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1026, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 216, in run\n",
      "    self.advance()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 455, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 151, in run\n",
      "    self.on_advance_end(data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 370, in on_advance_end\n",
      "    self.val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 2) cannot be concatenated\n",
      "\u001b[36m(train_nn_reg pid=3464665)\u001b[0m Seed set to 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3464665)\u001b[0m Train: (156, 159), Test: (65, 159)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3464665)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3464665)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3464665)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3464665)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_85fc59e5_98_batch_size=32,data_aggregation=None,data_selection=abundance_ith,data_selection_i=2,data_transform=None,e_2025-02-12_10-55-21 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3464665)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3464665)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3464665)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3464665)\u001b[0m 0 | layers | ModuleList | 1.4 M  | train\n",
      "\u001b[36m(train_nn_reg pid=3464665)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3464665)\u001b[0m 1.4 M     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3464665)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3464665)\u001b[0m 1.4 M     Total params\n",
      "\u001b[36m(train_nn_reg pid=3464665)\u001b[0m 5.455     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3464665)\u001b[0m 54        Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3464665)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3464665)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(train_nn_reg pid=3464665)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3464665)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-02-12 10:55:28,490\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_85fc59e5\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3464665, ip=172.31.181.85, actor_id=514628159f1f41c2afa714ef01000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1026, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 216, in run\n",
      "    self.advance()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 455, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 151, in run\n",
      "    self.on_advance_end(data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 370, in on_advance_end\n",
      "    self.val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 2) cannot be concatenated\n",
      "\u001b[36m(train_nn_reg pid=3464813)\u001b[0m Seed set to 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3464813)\u001b[0m Train: (156, 159), Test: (65, 159)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3464813)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3464813)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3464813)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3464813)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_f9cf00e2_99_batch_size=32,data_aggregation=None,data_selection=abundance_ith,data_selection_i=2,data_transform=None,e_2025-02-12_10-55-27 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3464813)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3464813)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3464813)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3464813)\u001b[0m 0 | layers | ModuleList | 1.7 M  | train\n",
      "\u001b[36m(train_nn_reg pid=3464813)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3464813)\u001b[0m 1.7 M     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3464813)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3464813)\u001b[0m 1.7 M     Total params\n",
      "\u001b[36m(train_nn_reg pid=3464813)\u001b[0m 6.770     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3464813)\u001b[0m 54        Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3464813)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3464813)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(train_nn_reg pid=3464813)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3464813)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-02-12 10:55:35,548\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_f9cf00e2\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3464813, ip=172.31.181.85, actor_id=462fcc73a92362d1fe5ff69c01000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1026, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 216, in run\n",
      "    self.advance()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 455, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 151, in run\n",
      "    self.on_advance_end(data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 370, in on_advance_end\n",
      "    self.val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 2) cannot be concatenated\n",
      "\u001b[36m(train_nn_reg pid=3464960)\u001b[0m Seed set to 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3464960)\u001b[0m Train: (156, 537), Test: (65, 537)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_nn_reg pid=3464960)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_nn_reg pid=3464960)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_nn_reg pid=3464960)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_nn_reg pid=3464960)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/lfloerl/tmpdata/ray/session_2025-02-12_10-05-50_883839_3395825/artifacts/2025-02-12_10-27-29/nn_reg/driver_artifacts/train_nn_reg_de26a40b_100_batch_size=32,data_aggregation=None,data_selection=abundance_ith,data_selection_i=3,data_transform=None,_2025-02-12_10-55-34 exists and is not empty.\n",
      "\u001b[36m(train_nn_reg pid=3464960)\u001b[0m \n",
      "\u001b[36m(train_nn_reg pid=3464960)\u001b[0m   | Name   | Type       | Params | Mode \n",
      "\u001b[36m(train_nn_reg pid=3464960)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3464960)\u001b[0m 0 | layers | ModuleList | 1.2 M  | train\n",
      "\u001b[36m(train_nn_reg pid=3464960)\u001b[0m ----------------------------------------------\n",
      "\u001b[36m(train_nn_reg pid=3464960)\u001b[0m 1.2 M     Trainable params\n",
      "\u001b[36m(train_nn_reg pid=3464960)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_nn_reg pid=3464960)\u001b[0m 1.2 M     Total params\n",
      "\u001b[36m(train_nn_reg pid=3464960)\u001b[0m 4.936     Total estimated model params size (MB)\n",
      "\u001b[36m(train_nn_reg pid=3464960)\u001b[0m 54        Modules in train mode\n",
      "\u001b[36m(train_nn_reg pid=3464960)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_nn_reg pid=3464960)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(train_nn_reg pid=3464960)\u001b[0m /scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(train_nn_reg pid=3464960)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-02-12 10:55:43,786\tERROR tune_controller.py:1331 -- Trial task failed for trial train_nn_reg_de26a40b\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3464960, ip=172.31.181.85, actor_id=624a7fb03bd6ff1653fa1c2401000000, repr=train_nn_reg)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 562, in train_nn_reg\n",
      "    train_nn(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 556, in train_nn\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1026, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 216, in run\n",
      "    self.advance()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 455, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 151, in run\n",
      "    self.on_advance_end(data_fetcher)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 370, in on_advance_end\n",
      "    self.val_loop.run()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 151, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 291, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 371, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/model_space/static_trainables.py\", line 409, in on_validation_epoch_end\n",
      "    all_preds = torch.cat(self.validation_predictions)\n",
      "RuntimeError: zero-dimensional tensor (at position 2) cannot be concatenated\n",
      "2025-02-12 10:55:47,463\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/lfloerl/cloud/lfloerl/Microterroir/artifacts/ITS/lavaux/climate-berries/ritme_all_models/ all_models/nn_reg' in 3.4879s.\n",
      "2025-02-12 10:55:47,525\tERROR tune.py:1037 -- Trials did not complete: [train_nn_reg_fdd33f93, train_nn_reg_b5df96e2, train_nn_reg_126b80e4, train_nn_reg_a8c9e393, train_nn_reg_585afc1b, train_nn_reg_96d75adb, train_nn_reg_fb67bf3c, train_nn_reg_885d4181, train_nn_reg_f82534bf, train_nn_reg_db54a0f9, train_nn_reg_bd4ccfe5, train_nn_reg_f0dd288e, train_nn_reg_7f5aa875, train_nn_reg_b1eab41a, train_nn_reg_52256c85, train_nn_reg_fe0d4d04, train_nn_reg_a68b3ce9, train_nn_reg_6a6e83a2, train_nn_reg_658848a7, train_nn_reg_d7e6c1b5, train_nn_reg_c085a7aa, train_nn_reg_832a9f56, train_nn_reg_eee28542, train_nn_reg_8d531d6a, train_nn_reg_6b109c2e, train_nn_reg_5473c5fe, train_nn_reg_46be22cb, train_nn_reg_b0c744e4, train_nn_reg_e750ae98, train_nn_reg_56629185, train_nn_reg_14270525, train_nn_reg_9290736f, train_nn_reg_667fe2f0, train_nn_reg_3cacc334, train_nn_reg_66004043, train_nn_reg_df106c16, train_nn_reg_7ffde30b, train_nn_reg_d9a45f0c, train_nn_reg_aa8b02c2, train_nn_reg_af910a96, train_nn_reg_0d054fd5, train_nn_reg_cd9994a1, train_nn_reg_9314c34e, train_nn_reg_c0b612be, train_nn_reg_221e4edc, train_nn_reg_7da01b72, train_nn_reg_0607541b, train_nn_reg_23753ef6, train_nn_reg_c8b3d228, train_nn_reg_b7d8f5dd, train_nn_reg_17a82890, train_nn_reg_eee9180a, train_nn_reg_efd632d9, train_nn_reg_dc108924, train_nn_reg_bcd4bc02, train_nn_reg_9df06404, train_nn_reg_d0e4e46e, train_nn_reg_11891b5f, train_nn_reg_8466fb79, train_nn_reg_395d94f1, train_nn_reg_a7d0654d, train_nn_reg_595528ac, train_nn_reg_fbb76e04, train_nn_reg_dcd4a051, train_nn_reg_5781d649, train_nn_reg_c96b517b, train_nn_reg_f6efb6b7, train_nn_reg_916754ae, train_nn_reg_a489d7a5, train_nn_reg_bf250c6e, train_nn_reg_7d1f05b3, train_nn_reg_c7e2ac19, train_nn_reg_9e6167db, train_nn_reg_7ac2c560, train_nn_reg_68016e2d, train_nn_reg_4166b2ca, train_nn_reg_595d4bda, train_nn_reg_17ec6b6d, train_nn_reg_6c34831c, train_nn_reg_f6ead3d2, train_nn_reg_62ddb4ba, train_nn_reg_0aeac832, train_nn_reg_0a5872d4, train_nn_reg_85fc59e5, train_nn_reg_f9cf00e2, train_nn_reg_de26a40b]\n",
      "2025-02-12 10:55:47,526\tINFO tune.py:1041 -- Total run time: 1697.66 seconds (1693.96 seconds for the tuning loop).\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Some trials encountered errors. See above for reported Ray Tune errors.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m tax \u001b[38;5;241m=\u001b[39m _load_taxonomy(path_to_tax)\n\u001b[0;32m----> 3\u001b[0m best_model_dict, path_to_exp \u001b[38;5;241m=\u001b[39m \u001b[43mfind_best_model_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath_store_model_logs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mritme_all_models\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/find_best_model_config.py:175\u001b[0m, in \u001b[0;36mfind_best_model_config\u001b[0;34m(config, train_val, tax, tree_phylo, path_store_model_logs)\u001b[0m\n\u001b[1;32m    172\u001b[0m     tree_phylo \u001b[38;5;241m=\u001b[39m _process_phylogeny(tree_phylo, train_val[ft_col])\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# ! Run all experiments on train_val\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m result_dic \u001b[38;5;241m=\u001b[39m \u001b[43mrun_all_trials\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtarget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstratify_by_column\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed_data\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtree_phylo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_tracker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_exp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# number of trials to run per model type * grid_search parameters in\u001b[39;49;00m\n\u001b[1;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# @_static_searchspace\u001b[39;49;00m\n\u001b[1;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_trials\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_cuncurrent_trials\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mls_model_types\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfully_reproducible\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest_mode\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_hyperparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_hyperparameters\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# ! Get best models of this experiment\u001b[39;00m\n\u001b[1;32m    196\u001b[0m best_model_dic \u001b[38;5;241m=\u001b[39m retrieve_best_models(result_dic)\n",
      "File \u001b[0;32m/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/tune_models.py:347\u001b[0m, in \u001b[0;36mrun_all_trials\u001b[0;34m(train_val, target, host_id, seed_data, seed_model, tax, tree_phylo, mlflow_uri, path_exp, num_trials, max_concurrent_trials, model_types, fully_reproducible, test_mode, model_hyperparameters)\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    345\u001b[0m         model_hparams_type \u001b[38;5;241m=\u001b[39m model_hyperparameters\u001b[38;5;241m.\u001b[39mget(model, {})\n\u001b[0;32m--> 347\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mrun_trials\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmlflow_uri\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m        \u001b[49m\u001b[43mMODEL_TRAINABLES\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhost_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtree_phylo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_exp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_concurrent_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfully_reproducible\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfully_reproducible\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_hyperparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_hparams_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    365\u001b[0m     results_all[model] \u001b[38;5;241m=\u001b[39m result\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results_all\n",
      "File \u001b[0;32m/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/tune_models.py:295\u001b[0m, in \u001b[0;36mrun_trials\u001b[0;34m(tracking_uri, exp_name, trainable, test_mode, train_val, target, host_id, seed_data, seed_model, tax, tree_phylo, path2exp, num_trials, max_concurrent_trials, fully_reproducible, model_hyperparameters, scheduler_grace_period, scheduler_max_t, resources)\u001b[0m\n\u001b[1;32m    292\u001b[0m result \u001b[38;5;241m=\u001b[39m analysis\u001b[38;5;241m.\u001b[39mfit()\n\u001b[1;32m    294\u001b[0m \u001b[38;5;66;03m# Check all trials & check for error status\u001b[39;00m\n\u001b[0;32m--> 295\u001b[0m \u001b[43m_check_for_errors_in_trials\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/scratch/lfloerl/.condaenvs/ritme_model/lib/python3.10/site-packages/ritme/tune_models.py:52\u001b[0m, in \u001b[0;36m_check_for_errors_in_trials\u001b[0;34m(result)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check if any trials encountered errors and raise an exception if so.\"\"\"\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mnum_errors \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSome trials encountered errors. See above for reported Ray Tune errors.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     54\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Some trials encountered errors. See above for reported Ray Tune errors."
     ]
    }
   ],
   "source": [
    "tax = _load_taxonomy(path_to_tax)\n",
    "\n",
    "best_model_dict, path_to_exp = find_best_model_config(\n",
    "    config, train_val, tax, path_store_model_logs=\"ritme_all_models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate best models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_model_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m metrics \u001b[38;5;241m=\u001b[39m evaluate_tuned_models(\u001b[43mbest_model_dict\u001b[49m, config, train_val, test)\n\u001b[1;32m      2\u001b[0m metrics\n",
      "\u001b[0;31mNameError\u001b[0m: name 'best_model_dict' is not defined"
     ]
    }
   ],
   "source": [
    "metrics = evaluate_tuned_models(best_model_dict, config, train_val, test)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ritme_model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
